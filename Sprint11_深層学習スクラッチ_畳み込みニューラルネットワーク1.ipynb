{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1次元の畳み込みニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。  \n",
    ">次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1次元畳み込み層とは  \n",
    ">CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。  \n",
    ">1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    ">\n",
    ">畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの用意  \n",
    ">検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータセット  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平準化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "#y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "#y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "#print(y_train.shape) # (60000,)\n",
    "#print(y_train_one_hot.shape) # (60000, 10)\n",
    "#print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "#print(X_train.shape) # (48000, 784)\n",
    "#print(X_val.shape) # (12000, 784)\n",
    "#print(y_train.shape)\n",
    "#print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成  \n",
    ">チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。  \n",
    ">基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。  \n",
    ">なお、重みの初期化に関するクラスは必要に応じて作り変えてください。  \n",
    ">Xavierの初期値などを使う点は全結合層と同様です。\n",
    ">\n",
    ">ここでは **パディング** は考えず、**ストライド** も1に固定します。  \n",
    ">また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。  >\n",
    ">この部分の拡張はアドバンス課題とします。\n",
    ">\n",
    ">フォワードプロパゲーションの数式は以下のようになります。\n",
    ">\n",
    ">$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$\n",
    ">\n",
    ">\n",
    ">ai : 出力される配列のi番目の値\n",
    ">\n",
    ">F : フィルタのサイズ\n",
    ">\n",
    ">$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    ">\n",
    ">ws : 重みの配列のs番目の値\n",
    ">\n",
    ">b : バイアス項\n",
    ">\n",
    ">全てスカラーです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。  \n",
    ">\n",
    ">$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$\n",
    ">\n",
    ">α  : 学習率\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial w_s}$ : ws に関する損失 L の勾配\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial b}$ : b に関する損失 L の勾配\n",
    ">\n",
    ">勾配 $\\frac{\\partial L}{\\partial b}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    ">\n",
    ">$N_{out}$ : 出力のサイズ\n",
    ">\n",
    ">前の層に流す誤差の数式は以下です。\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial x_j}$  : 前の層に流す誤差の配列のj番目の値\n",
    ">\n",
    ">ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
    ">\n",
    ">全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, F, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.F = F\n",
    "        self.W = initializer.W(self.F,)\n",
    "        self.B = initializer.B()\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        self.n_input=len(self.X)#.shape[1]\n",
    "        #F=3\n",
    "        stride=1\n",
    "        padding=0\n",
    "        self.n_output = self.output_size(self.n_input, self.F, P=padding, S=stride)\n",
    "        \n",
    "        print(\"X.shape: {}\".format(X.shape))\n",
    "        \n",
    "        A = np.zeros(len(X)-(F-1))#self.n_input\n",
    "        for i in range(len(X)-(F-1)):#self.n_input\n",
    "            A[i] = X[i:i+self.F].T@self.W+self.B\n",
    "            \n",
    "        self.A = A\n",
    "            \n",
    "        print(\"self.A.shape: {}\".format(self.A.shape))\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 想定されるラベル\n",
    "        y_conv1d = np.array([45, 70])\n",
    "        \n",
    "        dA = y_conv1d - self.A\n",
    "        \n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, self.X)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def output_size(self, n_input, F, P=0, S=1):\n",
    "        \"\"\"\n",
    "        出力数数える\n",
    "        \"\"\"\n",
    "        n_output = int((n_input + 2*P - F)/S + 1)\n",
    "\n",
    "        return n_output\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer():\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_filter):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_filter : int\n",
    "          フィルタのサイズ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_filter, )\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, ):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        size=1\n",
    "        B = self.sigma * np.random.randn(size,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer():\n",
    "    \"\"\"\n",
    "    ザビエル\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter):#シグマではなくノード数を入れる　n_filter？\n",
    "        sigma = 1/np.sqrt(n_filter)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_filter):\n",
    "        W = self.sigma * np.random.randn(n_filter,)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, ):\n",
    "        \n",
    "        size=1\n",
    "        B = self.sigma * np.random.randn(size,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer():\n",
    "    \"\"\"\n",
    "    フー\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter):#シグマではなくノード数を入れる　n_filter？\n",
    "        sigma = np.sqrt(2/n_filter)\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_filter):\n",
    "        W = self.sigma * np.random.randn(n_filter, )\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, ):\n",
    "        size=1\n",
    "        B = self.sigma * np.random.randn(size, )\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewalFormula():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, X):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # フィルタのサイズ\n",
    "        F = len(layer.W)\n",
    "        \n",
    "        dW = np.zeros(F)\n",
    "        for i in range(layer.n_output):\n",
    "            dW += dA[i]*X[i:i+F]\n",
    "            \n",
    "        print()\n",
    "        print(\"dW: {}\".format(dW))\n",
    "            \n",
    "        dB = np.sum(dA)#, axis=0\n",
    "        print(\"dB: {}\".format(dB))\n",
    "        print()\n",
    "        \n",
    "        dX = np.zeros(len(X))\n",
    "        for j in range(len(dA)):\n",
    "            #for s in range(len(layer.W)):#len(layer.W)-1\n",
    "            #if not(j-s < 0) and not(j-s > len(dA)-1):\n",
    "            dL_dA = dA[j]*layer.W\n",
    "            dX[j:j+F] += dL_dA\n",
    "            \n",
    "        print(\"dX: \\n{}\".format(dX))\n",
    "        print()\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        return (layer, dX)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class AdaGrad():\n",
    "#    \"\"\"\n",
    "#    最適化手法AdaGrad\n",
    "#    Parameters\n",
    "#    ----------\n",
    "#    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "#        訓練データ\n",
    "#    \"\"\"\n",
    "#    def __init__(self, lr, n_nodes):\n",
    "#        self.lr = lr\n",
    "#        # RuntimeWarning: divide by zero encountered を避ける\n",
    "#        self.H = np.zeros(n_nodes)+1e-10\n",
    "#        \n",
    "#    def update(self, layer, dA, Z):\n",
    "#        #dL_dA3 = Z3 - Y\n",
    "#        dB = np.sum(dA, axis=0)#バッチサイズ分の合計\n",
    "#        dW = Z.T@dA\n",
    "#        \n",
    "#        dZ = dA@layer.W.T\n",
    "#        \n",
    "#        self.H = self.H + np.mean(dW, axis = 0)*np.mean(dW, axis = 0)\n",
    "#        \n",
    "#        layer.W = layer.W - self.lr*(1/np.sqrt(self.H))*np.mean(dW, axis = 0)\n",
    "#        layer.B = layer.B - self.lr*(1/np.sqrt(self.H))*np.mean(dW, axis = 0)\n",
    "#        \n",
    "#        return (layer, dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算  \n",
    ">畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。\n",
    ">パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    ">\n",
    ">$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$\n",
    ">\n",
    ">$N_{out}$  : 出力のサイズ（特徴量の数）\n",
    ">\n",
    ">$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    ">\n",
    ">P : ある方向へのパディングの数\n",
    ">\n",
    ">F : フィルタのサイズ\n",
    ">\n",
    ">S : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラス内にoutput_sizeメソッドを作成。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: (3,)\n",
      "B.shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "n_nodes1=4\n",
    "n_nodes2=2\n",
    "initializer=SimpleInitializer(0.01)\n",
    "optimizer=RenewalFormula(1)\n",
    "F=3\n",
    "# インスタンス化\n",
    "CNN_size = SimpleConv1d(n_nodes1, n_nodes2, F, initializer, optimizer)\n",
    "\n",
    "print(\"W.shape: {}\".format(CNN_size.W.shape))\n",
    "print(\"B.shape: {}\".format(CNN_size.B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_size.output_size(n_input=4, F=3, P=0, S=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward関数内にて呼び出してself.n_outputとして値を保管できるように設計。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4,)\n",
      "self.A.shape: (2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.04271782, -0.05690343])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4])\n",
    "CNN_size.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_size.n_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験  \n",
    ">次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 動作検証用のデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">入力x、重みw、バイアスbを次のようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">フォワードプロパゲーションをすると出力は次のようになります。  \n",
    ">a = np.array([35, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">次にバックプロパゲーションを考えます。誤差は次のようであったとします。  \n",
    ">delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">バックプロパゲーションをすると次のような値になります。\n",
    ">\n",
    ">delta_b = np.array([30])  \n",
    ">delta_w = np.array([50, 80, 110])  \n",
    ">delta_x = np.array([30, 110, 170, 140])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用のコピー\n",
    "class SimpleConv1dTest():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, F, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.F = F\n",
    "        #self.W = initializer.W(F,)\n",
    "        #self.B = initializer.B()\n",
    "        \n",
    "        # テスト用\n",
    "        self.W = np.array([3, 5, 7])\n",
    "        self.B = np.array([1])\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        self.n_input=len(self.X)#.shape[1]\n",
    "        #F=3\n",
    "        stride=1\n",
    "        padding=0\n",
    "        self.n_output = self.output_size(self.n_input, self.F, P=padding, S=stride)\n",
    "        \n",
    "        print(\"X.shape: {}\".format(X.shape))\n",
    "        \n",
    "        A = np.zeros(len(X)-(F-1))#self.n_input\n",
    "        for i in range(len(X)-(F-1)):#self.n_input\n",
    "            A[i] = X[i:i+self.F].T@self.W+self.B\n",
    "            \n",
    "        self.A = A\n",
    "            \n",
    "        print(\"self.A.shape: {}\".format(self.A.shape))\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 想定されるラベル\n",
    "        y_conv1d = np.array([45, 70])\n",
    "        \n",
    "        dA = y_conv1d - self.A\n",
    "        \n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, self.X)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def output_size(self, n_input, F, P=0, S=1):\n",
    "        \"\"\"\n",
    "        出力数数える\n",
    "        \"\"\"\n",
    "        n_output = int((n_input + 2*P - F)/S + 1)\n",
    "\n",
    "        return n_output\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: (3,)\n",
      "B.shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "n_nodes1=4\n",
    "n_nodes2=2\n",
    "initializer=SimpleInitializer(0.01)\n",
    "optimizer=RenewalFormula(1)\n",
    "F=3\n",
    "# インスタンス化\n",
    "CNN_check = SimpleConv1dTest(n_nodes1, n_nodes2, F, initializer, optimizer)\n",
    "\n",
    "print(\"W.shape: {}\".format(CNN_check.W.shape))\n",
    "print(\"B.shape: {}\".format(CNN_check.B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4,)\n",
      "self.A.shape: (2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_check.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dW: [ 50.  80. 110.]\n",
      "dB: 30.0\n",
      "\n",
      "dX: \n",
      "[ 30. 110. 170. 140.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_check.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装上の工夫  \n",
    ">畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。  \n",
    ">しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    ">\n",
    ">$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$\n",
    ">\n",
    ">バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    ">\n",
    ">$\n",
    "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
    "$\n",
    ">\n",
    ">これは、xの一部を取り出した配列とwの配列の内積です。  \n",
    ">具体的な状況を考えると、以下のようなコードで計算できます。  \n",
    ">この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。  \n",
    ">これは結果的に内積と同様です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([1, 2, 3, 4])\n",
    "#w = np.array([3, 5, 7])\n",
    "#a = np.empty((2, 3))\n",
    "#indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "#indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "#a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "#a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "#a = a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    ">\n",
    ">また、二次元配列を使えば一次元配列から二次元配列が取り出せます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([1, 2, 3, 4])\n",
    "#indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "#print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    ">\n",
    ">畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    ">\n",
    ">《参考》\n",
    ">\n",
    ">以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    ">\n",
    ">Indexing — NumPy v1.17 Manual\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実施内容\n",
    "更新用クラスRenewalFormula内のupdateメソッド内にて、Xをスライスで取り出すことで対応。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成  \n",
    ">チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    ">\n",
    ">例えば以下のようなx, w, bがあった場合は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_conv1d = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">出力は次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[16, 22], [17, 23], [18, 24]]) \n",
    "# shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">入力が2チャンネル、出力が3チャンネルの例です。  \n",
    ">計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。  \n",
    ">計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 《補足》  \n",
    ">\n",
    ">チャンネル数を加える場合、配列をどういう順番にするかという問題があります。  \n",
    "(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    ">\n",
    ">今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。\n",
    ">上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, n_filter, n_channel, F, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.F = F\n",
    "        # テスト用\n",
    "        self.W = np.array([[[1, 1, 2], [2, 1, 1]], [[2, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1]]])\n",
    "        #self.W = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "        self.B = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "        \n",
    "        #w(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        #b（出力チャンネル数）\n",
    "        #self.W = initializer.W(n_filter, n_channel, self.F)\n",
    "        #self.B = initializer.B(n_filter)\n",
    "        \n",
    "        self.n_filter = n_filter\n",
    "        self.n_channel = n_channel\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #x(入力チャンネル数、特徴量数)\n",
    "        self.X = X\n",
    "        \n",
    "        self.n_input=self.X.shape[1]#.shape[1]\n",
    "        \n",
    "        #F=3\n",
    "        stride=1\n",
    "        padding=0\n",
    "        self.n_output = self.output_size(self.n_input, self.F, P=padding, S=stride)\n",
    "        \n",
    "        A = np.empty((self.n_filter,self.n_output))\n",
    "        for f in range(self.n_filter):\n",
    "            for a_o in range(self.n_output):\n",
    "                A[f, a_o] = np.sum(X[:, a_o:a_o+self.F]*self.W[f])+self.B[f]\n",
    "   \n",
    "        self.A = A\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self,):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 想定されるラベル\n",
    "        y_conv1d = np.array([[30, 40], [50, 60], [70, 80]]) \n",
    "        #y_conv1d = np.array([45, 70])\n",
    "        \n",
    "        dA = y_conv1d - self.A\n",
    "        \n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, self.X)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def output_size(self, n_input, F, P=0, S=1):\n",
    "        \"\"\"\n",
    "        出力数数える\n",
    "        \"\"\"\n",
    "        n_output = int((n_input + 2*P - F)/S + 1)\n",
    "\n",
    "        return n_output\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer_q4():\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_filter, n_channel, F):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_filter : int\n",
    "          フィルタのサイズ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        #【問題2】\n",
    "        W = self.sigma * np.random.randn(n_filter, n_channel, F)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, n_filter):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_filter, )\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer_q4():\n",
    "    \"\"\"\n",
    "    ザビエル\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter):#シグマではなくノード数を入れる　n_filter？\n",
    "        sigma = 1/np.sqrt(n_filter)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_filter, n_channel, F):\n",
    "        W = self.sigma * np.random.randn(n_filter, n_channel, F)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, ):\n",
    "        B = self.sigma * np.random.randn(n_filter,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer_q4():\n",
    "    \"\"\"\n",
    "    フー\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter):#シグマではなくノード数を入れる　n_filter？\n",
    "        sigma = np.sqrt(2/n_filter)\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_filter, n_channel, F):\n",
    "        W = self.sigma * np.random.randn(n_filter, n_channel, F)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, ):\n",
    "        B = self.sigma * np.random.randn(n_filter, )\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewalFormula_q4():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, X):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        dW_temp = []\n",
    "        for f in range(layer.n_filter):\n",
    "            dA_X = []\n",
    "            for i in range(layer.n_output):\n",
    "                dA_X.append(dA[f, i]*X[:, i:i+layer.F])    \n",
    "            dW_temp.append(dA_X)\n",
    "        \n",
    "        dW_temp = np.array(dW_temp)\n",
    "        \n",
    "        dW = []\n",
    "        for f in range(layer.n_filter):\n",
    "            dW_c = []\n",
    "            for c in range(layer.n_channel):\n",
    "                dW_dW = np.zeros(layer.F)\n",
    "                for i in range(layer.n_output):\n",
    "                    dW_dW += dW_temp[f, i, c, :]\n",
    "                dW_c.append(dW_dW)\n",
    "            dW.append(dW_c)\n",
    "        dW = np.array(dW)\n",
    "        \n",
    "        print(\"dW: \\n{}\".format(dW))\n",
    "        print()\n",
    "        \n",
    "        dB = np.sum(dA, axis=1)#, axis=0\n",
    "        print(\"dB: {}\".format(dB))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        # X\n",
    "        dX=np.zeros(X.shape)#layer.n_channel, layer.n_input\n",
    "        for f in range(layer.n_filter):#チャネル\n",
    "            for i in range(layer.n_output):#９、１１\n",
    "                for c in range(layer.n_channel):#フィルタの選択\n",
    "                    dX[c, i:i+layer.F] += dA[f, i]*layer.W[f, c]\n",
    "                \n",
    "        print(\"dX: \\n{}\".format(dX))\n",
    "        print()\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        return (layer, dX)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class AdaGrad_q4():\n",
    "#    \"\"\"\n",
    "#    最適化手法AdaGrad\n",
    "#    Parameters\n",
    "#    ----------\n",
    "#    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "#    訓練データ\n",
    "#    \"\"\"\n",
    "#    def __init__(self, lr, n_nodes):\n",
    "#        self.lr = lr\n",
    "#        # RuntimeWarning: divide by zero encountered を避ける\n",
    "#        self.H = np.zeros(n_nodes)+1e-10\n",
    "#        \n",
    "#    def update(self, layer, dA, Z):\n",
    "#        #dL_dA3 = Z3 - Y\n",
    "#        dB = np.sum(dA, axis=0)#バッチサイズ分の合計\n",
    "#        dW = Z.T@dA\n",
    "#        \n",
    "#        dZ = dA@layer.W.T\n",
    "#        \n",
    "#        self.H = self.H + np.mean(dW, axis = 0)*np.mean(dW, axis = 0)\n",
    "#        \n",
    "#        layer.W = layer.W - self.lr*(1/np.sqrt(self.H))*np.mean(dW, axis = 0)\n",
    "#        layer.B = layer.B - self.lr*(1/np.sqrt(self.H))*np.mean(dW, axis = 0)\n",
    "#        \n",
    "#        return (layer, dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: (3, 2, 3)\n",
      "B.shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "#x(入力チャンネル数、特徴量数)\n",
    "#w(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "#b（出力チャンネル数）\n",
    "\n",
    "n_nodes1=4\n",
    "n_nodes2=2\n",
    "initializer=SimpleInitializer_q4(0.01)\n",
    "optimizer=RenewalFormula_q4(1)\n",
    "n_filter=3\n",
    "n_channel=2\n",
    "F=3\n",
    "# インスタンス化\n",
    "CNN_conv1d = Conv1d(n_nodes1, n_nodes2, n_filter, n_channel, F, initializer, optimizer)\n",
    "\n",
    "print(\"W.shape: {}\".format(CNN_conv1d.W.shape))\n",
    "print(\"B.shape: {}\".format(CNN_conv1d.B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21., 29.],\n",
       "       [18., 25.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_conv1d.forward(x_conv1d)\n",
    "#CNN_conv1d.forward(X_train[0])#バッチサイズ１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: \n",
      "[[[ 31.  51.  71.]\n",
      "  [ 51.  71.  91.]]\n",
      "\n",
      " [[102. 169. 236.]\n",
      "  [169. 236. 303.]]\n",
      "\n",
      " [[164. 272. 380.]\n",
      "  [272. 380. 488.]]]\n",
      "\n",
      "dB: [ 20.  67. 108.]\n",
      "\n",
      "dX: \n",
      "[[125. 230. 204. 113.]\n",
      " [102. 206. 195. 102.]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[125., 230., 204., 113.],\n",
       "       [102., 206., 195., 102.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_conv1d.backward()\n",
    "#CNN_conv1d.backward(y_train[0])#バッチサイズ１"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定  \n",
    ">これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    ">\n",
    ">出力層だけは全結合層をそのまま使ってください。  \n",
    ">ただし、チャンネルが複数ある状態では全結合層への入力は行えません。\n",
    ">その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    ">\n",
    ">画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n",
      "[2 9 7 ... 7 7 5]\n"
     ]
    }
   ],
   "source": [
    "# テスト用にデコード\n",
    "y_test_decode = np.argmax(y_val, axis=1)\n",
    "print(y_test_decode.shape)\n",
    "print(y_test_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワークのクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.n_features \n",
    "    \n",
    "    self.n_nodes1\n",
    "    \n",
    "    self.sigma\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lr, sigma = 0.01):\n",
    "        self.lr = lr\n",
    "        #self.n_features = n_features\n",
    "        #self.n_nodes1 = n_nodes1\n",
    "        #self.n_nodes2 = n_nodes2\n",
    "        #self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self,X_train, y_train,layer_information, epoch=1, batch_size=20):\n",
    "\n",
    "        # 各層のインスタンス、関数のインスタンス\n",
    "        self.FC_list = []\n",
    "        self.activation_list = []\n",
    "        \n",
    "        # レイヤー数を取得\n",
    "        self.layer = len(layer_information)\n",
    "        \n",
    "        # 各層のインスタンス、関数のインスタンスを用意するためのループ\n",
    "        for i in range(self.layer):\n",
    "            n_nodes_a = layer_information[i][2]\n",
    "            n_nodes_b = layer_information[i][3]\n",
    "        \n",
    "            #最適化関数の選択\n",
    "            if layer_information[i][0] == 'SGD':\n",
    "                optimizer = SGD(self.lr)\n",
    "            else:\n",
    "                optimizer = RenewalFormula_q8(self.lr,)\n",
    "\n",
    "        \n",
    "            #使用するモデルの選択\n",
    "            if layer_information[i][5] == 'CNN':\n",
    "                #初期化の選択\n",
    "                if layer_information[i][4] == 'XavierInitializer':\n",
    "                    FC_temp = Conv1d_q8(n_nodes_a, n_nodes_b, 3, XavierInitializer(n_nodes_a), optimizer)\n",
    "                elif layer_information[i][4] == 'HeInitializer':\n",
    "                    FC_temp = Conv1d_q8(n_nodes_a, n_nodes_b, 3, HeInitializer(n_nodes_a), optimizer)\n",
    "                else:\n",
    "                    FC_temp = Conv1d_q8(n_nodes_a, n_nodes_b, 3, SimpleInitializer_q4(self.sigma), optimizer)\n",
    "            else:\n",
    "                #初期化の選択\n",
    "                if layer_information[i][4] == 'XavierInitializer':\n",
    "                    FC_temp = FC(n_nodes_a, n_nodes_b, XavierInitializer(n_nodes_a), optimizer)\n",
    "                elif layer_information[i][4] == 'HeInitializer':\n",
    "                    FC_temp = FC(n_nodes_a, n_nodes_b, HeInitializer(n_nodes_a), optimizer)\n",
    "                else:\n",
    "                    FC_temp = FC(n_nodes_a, n_nodes_b, SimpleInitializerFC(self.sigma), optimizer)\n",
    "\n",
    "            # 各層のインスタンスをリストで保管\n",
    "            self.FC_list.append(FC_temp)\n",
    "            \n",
    "            #活性化関数の選択\n",
    "            if layer_information[i][1]=='ReLU':\n",
    "                activation_temp = ReLU()\n",
    "            elif layer_information[i][1]=='Sigmoid':\n",
    "                activation_temp = Sigmoid()\n",
    "            elif layer_information[i][1]=='Softmax':\n",
    "                # 最終層の関数\n",
    "                activation_temp = Softmax()\n",
    "            else:\n",
    "                activation_temp = Tanh()\n",
    "\n",
    "            # 各層の関数をリストで保管\n",
    "            self.activation_list.append(activation_temp)\n",
    "            \n",
    "        # Loss Curv用のリスト\n",
    "        L_list = []\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X_train, y_train, batch_size)# ミニバッチ\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードのA、Zを保管するリスト\n",
    "                #A_list=[]\n",
    "                #Z_list=[]\n",
    "                \n",
    "                # 最初のZは　mini_X_train　なので代入\n",
    "                #Z = mini_X_train\n",
    "                \n",
    "                # フォワード処理\n",
    "                #for l in range(len(layer_information)):\n",
    "                #    A = self.FC_list[l].forward(Z)\n",
    "                #    Z = self.activation_list[l].forward(A)\n",
    "                #    A_list.append(A)\n",
    "                #    Z_list.append(Z)\n",
    "                    \n",
    "                A1 = self.FC_list[0].forward(mini_X_train)\n",
    "                Z1 = self.activation_list[0].forward(A1)\n",
    "                A2 = self.FC_list[1].forward(Z1)\n",
    "                Z2 = self.activation_list[1].forward(A2)\n",
    "                A3 = self.FC_list[2].forward(Z2)\n",
    "                Z3 = self.activation_list[2].forward(A3)\n",
    "\n",
    "                # バックワードの処理\n",
    "                #dA3, L = self.activation_list[-1].backward(Z_list[-1], mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                #dZ2 = self.FC_list[-1].backward(dA3, Z_list[-2])#引数増やして渡す。Z2\n",
    "\n",
    "                #dZ = dZ2\n",
    "                #for l in reversed(range(1, len(layer_information)-1)):# 最初と最後をのぞいてループ\n",
    "                #    dA = self.activation_list[l].backward(A_list[l],dZ)#引数増やして渡す。A2\n",
    "                #    dZ = self.FC_list[l].backward(dA, Z_list[l-1])#引数増やして渡す。Z1\n",
    "                \n",
    "                # 値を引き渡すための変数\n",
    "                #dZ1 = dZ\n",
    "                #dA1 = self.activation_list[0].backward(A_list[0], dZ1)#引数増やして渡す。A1\n",
    "                #dZ0 = self.FC_list[0].backward(dA1, mini_X_train) # dZ0は使用しない #引数増やして渡す。X\n",
    "\n",
    "                \n",
    "                dA3, L = self.activation_list[2].backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC_list[2].backward(dA3, Z2)#引数増やして渡す。Z2\n",
    "                dA2 = self.activation_list[1].backward(A2,dZ2)#引数増やして渡す。A2\n",
    "                dZ1 = self.FC_list[1].backward(dA2, Z1)#引数増やして渡す。Z1\n",
    "                dA1 = self.activation_list[0].backward(A1, dZ1)#引数増やして渡す。A1\n",
    "                dZ0 = self.FC_list[0].backward(dA1, mini_X_train) # dZ0は使用しない #引数増やして渡す。X\n",
    "                \n",
    "            \"\"\"\n",
    "            Loss Curvを描くための処理\n",
    "            \"\"\"\n",
    "            L_list.append(L)\n",
    "            self.L_list = np.array(L_list) \n",
    "                \n",
    "        \n",
    "    def predict(self,X):\n",
    "\n",
    "        # 最初のZはXなので代入\n",
    "        #Z = X\n",
    "\n",
    "        # フォワード処理\n",
    "        #for l in range(self.layer):\n",
    "        #A = self.FC_list[l].forward(Z)\n",
    "        #    Z = self.activation_list[l].forward(A)\n",
    "            \n",
    "        #y = self.activation_list[-1].forward(A)\n",
    "        #pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        A1 = self.FC_list[0].forward(X)\n",
    "        Z1 = self.activation_list[0].forward(A1)\n",
    "        A2 = self.FC_list[1].forward(Z1)\n",
    "        Z2 = self.activation_list[1].forward(A2)\n",
    "        A3 = self.FC_list[2].forward(Z2)\n",
    "        y = self.activation_list[2].forward(A3)\n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_q8():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter, n_channel, F, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.F = F\n",
    "        \n",
    "        #w(出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        #b（出力チャンネル数）\n",
    "        \n",
    "        self.W = initializer.W(n_filter, n_channel, self.F)\n",
    "        self.B = initializer.B(n_filter)\n",
    "        \n",
    "        self.n_filter = n_filter\n",
    "        self.n_channel = n_channel\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #x(入力チャンネル数、特徴量数)\n",
    "        self.X = X\n",
    "        \n",
    "        self.n_input=self.X.shape[1]#.shape[1]\n",
    "        \n",
    "        #F=3\n",
    "        stride=1\n",
    "        padding=0\n",
    "        self.n_output = self.output_size(self.n_input, self.F, P=padding, S=stride)\n",
    "        \n",
    "        A = np.empty((self.n_filter,self.n_output))\n",
    "        for f in range(self.n_filter):\n",
    "            for a_o in range(self.n_output):\n",
    "                A[f, a_o] = np.sum(X[:, a_o:a_o+self.F]*self.W[f])+self.B[f]\n",
    "   \n",
    "        self.A = A\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA, Z):#dA2, Z1\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 実際の処理においてはラベルは不要\n",
    "        \n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, self.X)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def output_size(self, n_input, F, P=0, S=1):\n",
    "        \"\"\"\n",
    "        出力数数える\n",
    "        \"\"\"\n",
    "        n_output = int((n_input + 2*P - F)/S + 1)\n",
    "\n",
    "        return n_output\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewalFormula_q8():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, X):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        dW_temp = []\n",
    "        for f in range(layer.n_filter):\n",
    "            dA_X = []\n",
    "            for i in range(layer.n_output):\n",
    "                dA_X.append(dA[f, i]*X[:, i:i+layer.F])    \n",
    "            dW_temp.append(dA_X)\n",
    "        \n",
    "        dW_temp = np.array(dW_temp)\n",
    "        \n",
    "        dW = []\n",
    "        for f in range(layer.n_filter):\n",
    "            dW_c = []\n",
    "            for c in range(layer.n_channel):\n",
    "                dW_dW = np.zeros(layer.F)\n",
    "                for i in range(layer.n_output):\n",
    "                    dW_dW += dW_temp[f, i, c, :]\n",
    "                dW_c.append(dW_dW)\n",
    "            dW.append(dW_c)\n",
    "        dW = np.array(dW)\n",
    "        \n",
    "        dB = np.sum(dA, axis=1)#, axis=0\n",
    "        \n",
    "        # X\n",
    "        dX=np.zeros(X.shape)#layer.n_channel, layer.n_input\n",
    "        for f in range(layer.n_filter):#チャネル\n",
    "            for i in range(layer.n_output):#９、１１\n",
    "                for c in range(layer.n_channel):#フィルタの選択\n",
    "                    dX[c, i:i+layer.F] += dA[f, i]*layer.W[f, c]\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        return (layer, dX)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        #【問題１】\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #【問題１】\n",
    "        self.X = X\n",
    "        \n",
    "        #print(\"self.X: \\n{}\".format(self.X.shape))\n",
    "        #print(\"self.W: \\n{}\".format(self.W.shape))\n",
    "        #print(\"self.B.reshape(-1,1).T: \\n{}\".format(self.B.reshape(-1,1).T.shape))\n",
    "        A = self.X@self.W+self.B.reshape(-1,1).T\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, Z)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializerFC:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        #【問題2】\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        #【問題2】\n",
    "        B = self.sigma * np.random.randn(n_nodes2,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, Z):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #dL_dA3 = Z3 - Y\n",
    "        dB = np.sum(dA, axis=0)#バッチサイズ分の合計\n",
    "        dW = Z.T@dA\n",
    "        \n",
    "        dZ = dA@layer.W.T\n",
    "        \n",
    "        #【問題3】\n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        return (layer, dZ)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.exp(X)/(np.sum(np.exp(X), axis=1).reshape(-1,1))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA =X - y\n",
    "        \n",
    "        # 目的関数（損失関数）　交差エントロピー誤差\n",
    "        nb = y.shape[0]#バッチサイズ\n",
    "        L = -(1/nb)*(np.sum(y*np.log(X)))\n",
    "        \n",
    "        return (dA, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA = dZ*(1 - np.tanh(X)**2)\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNの第１層、および第２層をCNNに変更。    \n",
    "- 第１層\n",
    "    - 出力チャンネル（フィルタ数） = ３\n",
    "    - 入力チャンネル = 1（白黒）\n",
    "- 第２層\n",
    "    - 出力チャンネル（フィルタ数） = 1\n",
    "    - 入力チャンネル = 3（第１層のフィルタ数）\n",
    "- バッチ数\n",
    "    - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh, Sigmoid, ReLU\n",
    "# SGD, AdaGrad\n",
    "# SimpleInitializer, XavierInitializer、HeInitializer\n",
    "# CNN, FC\n",
    "# n_filter, n_channel, \n",
    "\n",
    "layer_information = [['RenewalFormula','Tanh',3,1,'SimpleInitializer', 'CNN'], # 第１層\n",
    "                                   ['RenewalFormula','Tanh',1,3,'SimpleInitializer', 'CNN'], # 第2層\n",
    "                                   ['SGD','Softmax',780,10,'SimpleInitializer', 'FC'], # 第3層\n",
    "                                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN = ScratchDeepNeuralNetrowkClassifier(lr=0.01, sigma=0.01)\n",
    "DNN.fit(X_train[0:1000,:], #X_train[0:20,:]\n",
    "                y_train[0:1000], #y_train[0:20,:]\n",
    "                layer_information,\n",
    "                epoch=10, \n",
    "                batch_size=1\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Conv1d_q8 at 0x7f9746f63b90>,\n",
       " <__main__.Conv1d_q8 at 0x7f9746f63c90>,\n",
       " <__main__.FC at 0x7f9746f63d50>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN.FC_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Tanh at 0x7f9746f63bd0>,\n",
       " <__main__.Tanh at 0x7f9746f63c50>,\n",
       " <__main__.Softmax at 0x7f9747046410>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN.activation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推定結果： [2 9 7 0 0 7 0 8 3 0]\n",
      "accuracy_score： 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "for i in range(10):\n",
    "    y_pred.append(DNN.predict(X_val[i:i+1,:]))\n",
    "print(\"推定結果： {}\".format(np.array(y_pred).reshape(-1,)))\n",
    "\n",
    "acc = accuracy_score(y_test_decode[0:i+1], y_pred)\n",
    "print(\"accuracy_score： {}\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数グラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f97470d6410>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_function_epoch = DNN.L_list\n",
    "#loss_function_epoch_val = DNN.L_list_val\n",
    "plt.title('Loss function - Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss_function_epoch, label='train_loss')#, color='b'\n",
    "#plt.plot(loss_function_epoch_val, label='val_loss')# color='b'\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
