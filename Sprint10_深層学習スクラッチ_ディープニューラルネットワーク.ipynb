{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ディープニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    ">\n",
    ">このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    ">\n",
    ">名前は新しくScratchDeepNeuralNetworkClassifierクラスとしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 層などのクラス化  \n",
    ">クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    ">\n",
    ">手を加える箇所\n",
    ">\n",
    ">- 層の数\n",
    ">- 層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    ">- 活性化関数の種類\n",
    ">- 重みやバイアスの初期化方法\n",
    ">- 最適化手法\n",
    ">\n",
    ">そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    ">\n",
    ">実装方法は自由ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《サンプルコード1》  \n",
    ">ScratchDeepNeuralNetworkClassifierのfitメソッド内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.sigma : ガウス分布の標準偏差\n",
    "# self.lr : 学習率\n",
    "# self.n_nodes1 : 1層目のノード数\n",
    "# self.n_nodes2 : 2層目のノード数\n",
    "# self.n_output : 出力層のノード数\n",
    "#optimizer = SGD(self.lr)\n",
    "#self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "#self.activation1 = Tanh()\n",
    "#self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "#self.activation2 = Tanh()\n",
    "#self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "#self.activation3 = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《サンプルコード2》  \n",
    ">イテレーションごとのフォワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1 = self.FC1.forward(X)\n",
    "#Z1 = self.activation1.forward(A1)\n",
    "#A2 = self.FC2.forward(Z1)\n",
    "#Z2 = self.activation2.forward(A2)\n",
    "#A3 = self.FC3.forward(Z2)\n",
    "#Z3 = self.activation3.forward(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《サンプルコード3》  \n",
    ">イテレーションごとのバックワード\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "#dZ2 = self.FC3.backward(dA3)\n",
    "#dA2 = self.activation2.backward(dZ2)\n",
    "#dZ1 = self.FC2.backward(dA2)\n",
    "#dA1 = self.activation1.backward(dZ1)\n",
    "#dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】全結合層のクラス化  \n",
    ">全結合層のクラス化を行なってください。\n",
    ">\n",
    ">以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    ">\n",
    ">なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    ">\n",
    ">また、引数として自身のインスタンス**`self`**を渡すこともできます。これを利用して**`self.optimizer.update(self)`**という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    ">\n",
    ">初期化方法と最適化手法のクラスについては後述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        #【問題１】\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #【問題１】\n",
    "        self.X = X\n",
    "        A = self.X@self.W+self.B.reshape(-1,1).T\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, Z)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】初期化方法のクラス化  \n",
    ">初期化を行うコードをクラス化してください。\n",
    ">\n",
    ">前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    ">\n",
    ">これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        #【問題2】\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        #【問題2】\n",
    "        B = self.sigma * np.random.randn(n_nodes2,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最適化手法のクラス化  \n",
    ">最適化手法のクラス化を行なってください。\n",
    ">\n",
    ">最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    ">\n",
    ">これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, Z):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #dL_dA3 = Z3 - Y\n",
    "        dB = np.sum(dA, axis=0)#バッチサイズ分の合計\n",
    "        dW = Z.T@dA\n",
    "        \n",
    "        dZ = dA@layer.W.T\n",
    "        \n",
    "        #【問題3】\n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        return (layer, dZ)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】活性化関数のクラス化  \n",
    ">活性化関数のクラス化を行なってください。\n",
    ">\n",
    ">ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
    "### 発展的要素\n",
    ">活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = 1/(1+np.exp(-X))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        sigmoid_A = 1/(1+np.exp(-X))\n",
    "        dA = dZ*(1 - sigmoid_A)*sigmoid_A\n",
    "        \n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA = dZ*(1 - np.tanh(X)**2)\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.exp(X)/(np.sum(np.exp(X), axis=1).reshape(-1,1))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA =X - y\n",
    "        \n",
    "        # 目的関数（損失関数）　交差エントロピー誤差\n",
    "        nb = y.shape[0]#バッチサイズ\n",
    "        L = -(1/nb)*(np.sum(y*np.log(X)))\n",
    "        \n",
    "        return (dA, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】ReLUクラスの作成  \n",
    ">現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    ">\n",
    ">ReLUは以下の数式です。\n",
    ">\n",
    ">$\n",
    "f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}\n",
    "$\n",
    ">\n",
    ">x : ある特徴量。スカラー\n",
    ">\n",
    ">実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    ">\n",
    ">numpy.maximum — NumPy v1.15 Manual\n",
    ">\n",
    ">一方、バックプロパゲーションのための x に関する f(x) の微分は以下のようになります。\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases}\n",
    "$\n",
    ">\n",
    ">数学的には微分可能ではないですが、 x=0 のとき 0 とすることで対応しています。\n",
    ">\n",
    ">フォワード時の x の正負により、勾配を逆伝播するかどうかが決まるということになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数（ReLU）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        #Z = np.where(X <= 0, 0, X)\n",
    "        Z = np.maximum(X, 0)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, X, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        # Xで判定して勾配を渡す\n",
    "        dA = np.where(X <= 0, 0, dZ)\n",
    "        \n",
    "        return dA\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】重みの初期値  \n",
    ">ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。  \n",
    ">しかし、どのような値にすると良いかが知られています。  \n",
    ">シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    ">\n",
    ">XavierInitializerクラスと、HeInitializerクラスを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavierの初期値  \n",
    ">Xavierの初期値における標準偏差 σ は次の式で求められます。\n",
    ">\n",
    ">$\n",
    "\\sigma = \\frac{1}{\\sqrt{n}}\n",
    "$\n",
    ">\n",
    ">n  : 前の層のノード数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    ザビエル\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):#シグマではなくノード数を入れる\n",
    "        sigma = 1/np.sqrt(n_nodes1)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heの初期値  \n",
    ">Heの初期値における標準偏差 σ は次の式で求められます。\n",
    ">\n",
    ">$\n",
    "\\sigma = \\sqrt{\\frac{2}{n}}\n",
    "$\n",
    ">\n",
    ">n  : 前の層のノード数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    フー\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):#シグマではなくノード数を入れる\n",
    "        sigma = np.sqrt(2/n_nodes1)\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2, )\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】最適化手法  \n",
    ">学習率は学習過程で変化させていく方法が一般的です。  \n",
    ">基本的な手法である AdaGrad のクラスを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">まず、これまで使ってきたSGDを確認します。\n",
    ">\n",
    ">$\n",
    "W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})\n",
    "$\n",
    ">\n",
    ">α  : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial L}{\\partial W_i}  : W_iに関する損失 L の勾配\n",
    "$\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial L}{\\partial B_i}  : B_iに関する損失 L の勾配\n",
    "$\n",
    ">\n",
    ">E()  : ミニバッチ方向にベクトルの平均を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    ">\n",
    ">更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 H を保存しておき、その分だけ学習率を小さくします。\n",
    ">\n",
    ">学習率は重み一つひとつに対して異なることになります。\n",
    ">\n",
    ">$\n",
    "H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "$\n",
    ">\n",
    ">Hi  : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    ">\n",
    ">H′i : 更新した Hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    最適化手法AdaGrad\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, n_nodes):\n",
    "        self.lr = lr\n",
    "        # RuntimeWarning: divide by zero encountered を避ける\n",
    "        self.H = np.zeros(n_nodes)+1e-10\n",
    "        \n",
    "    def update(self, layer, dA, Z):\n",
    "        #dL_dA3 = Z3 - Y\n",
    "        dB = np.sum(dA, axis=0)#バッチサイズ分の合計\n",
    "        dW = Z.T@dA\n",
    "        \n",
    "        dZ = dA@layer.W.T\n",
    "        \n",
    "        self.H = self.H + np.mean(dW, axis = 0)*np.mean(dW, axis = 0)\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(1/np.sqrt(self.H))*np.mean(dW, axis = 0)\n",
    "        layer.B = layer.B - self.lr*(1/np.sqrt(self.H))*np.mean(dW, axis = 0)\n",
    "        \n",
    "        return (layer, dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】クラスの完成  \n",
    ">任意の構成で学習と推定が行えるScratchDeepNeuralNetworkClassifierクラスを完成させてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScratchDeepNeuralNetworkClassifier() クラス  \n",
    "３層以上のレイヤーも扱えるようにするため、layer_informationという２次元配列に  \n",
    "層ごとに「'最適化処理','活性化関数',入力ノード数,出力先ノード数,'初期化関数'」を入れて  \n",
    "引数として渡すと、任意の階層の処理が行えるよう実装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワークのクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.n_features \n",
    "    \n",
    "    self.n_nodes1\n",
    "    \n",
    "    self.sigma\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lr, sigma = 0.01):\n",
    "        self.lr = lr\n",
    "        #self.n_features = n_features\n",
    "        #self.n_nodes1 = n_nodes1\n",
    "        #self.n_nodes2 = n_nodes2\n",
    "        #self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self,X,y,layer_information, epoch=1, batch_size=20):\n",
    "\n",
    "        # 各層のインスタンス、関数のインスタンス\n",
    "        self.FC_list = []\n",
    "        self.activation_list = []\n",
    "        \n",
    "        # レイヤー数を取得\n",
    "        self.layer = len(layer_information)\n",
    "        \n",
    "        # 各層のインスタンス、関数のインスタンスを用意するためのループ\n",
    "        for i in range(self.layer):\n",
    "            n_nodes_a = layer_information[i][2]\n",
    "            n_nodes_b = layer_information[i][3]\n",
    "        \n",
    "            #最適化関数の選択\n",
    "            if layer_information[i][0] == 'AdaGrad':\n",
    "                optimizer = AdaGrad(self.lr, n_nodes_b)\n",
    "            else:\n",
    "                optimizer = SGD(self.lr)\n",
    "\n",
    "            #初期化の選択\n",
    "            if layer_information[i][4] == 'XavierInitializer':\n",
    "                FC_temp = FC(n_nodes_a, n_nodes_b, XavierInitializer(n_nodes_a), optimizer)\n",
    "            elif layer_information[i][4] == 'HeInitializer':\n",
    "                FC_temp = FC(n_nodes_a, n_nodes_b, HeInitializer(n_nodes_a), optimizer)\n",
    "            else:\n",
    "                FC_temp = FC(n_nodes_a, n_nodes_b, SimpleInitializer(self.sigma), optimizer)\n",
    "\n",
    "            # 各層のインスタンスをリストで保管\n",
    "            self.FC_list.append(FC_temp)\n",
    "            \n",
    "            #活性化関数の選択\n",
    "            if layer_information[i][1]=='ReLU':\n",
    "                activation_temp = ReLU()\n",
    "            elif layer_information[i][1]=='Sigmoid':\n",
    "                activation_temp = Sigmoid()\n",
    "            elif layer_information[i][1]=='Softmax':\n",
    "                # 最終層の関数\n",
    "                activation_temp = Softmax()\n",
    "            else:\n",
    "                activation_temp = Tanh()\n",
    "\n",
    "            # 各層の関数をリストで保管\n",
    "            self.activation_list.append(activation_temp)\n",
    "            \n",
    "        # Loss Curv用のリスト\n",
    "        L_list = []\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            get_mini_batch = GetMiniBatch(X_train, y_train, batch_size)# ミニバッチ\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # フォワードのA、Zを保管するリスト\n",
    "                A_list=[]\n",
    "                Z_list=[]\n",
    "                \n",
    "                # 最初のZは　mini_X_train　なので代入\n",
    "                Z = mini_X_train\n",
    "                \n",
    "                # フォワード処理\n",
    "                for l in range(len(layer_information)):\n",
    "                    A = self.FC_list[l].forward(Z)\n",
    "                    Z = self.activation_list[l].forward(A)\n",
    "                    A_list.append(A)\n",
    "                    Z_list.append(Z)\n",
    "                    \n",
    "                #A1 = self.FC_list[0].forward(mini_X_train)\n",
    "                #Z1 = self.activation_list[0].forward(A1)\n",
    "                #A2 = self.FC_list[1].forward(Z1)\n",
    "                #Z2 = self.activation_list[1].forward(A2)\n",
    "                #A3 = self.FC_list[2].forward(Z2)\n",
    "                #Z3 = self.activation_list[2].forward(A3)\n",
    "                \n",
    "\n",
    "                # バックワードの処理\n",
    "                dA3, L = self.activation_list[-1].backward(Z_list[-1], mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC_list[-1].backward(dA3, Z_list[-2])#引数増やして渡す。Z2\n",
    "\n",
    "                dZ = dZ2\n",
    "                for l in reversed(range(1, len(layer_information)-1)):# 最初と最後をのぞいてループ\n",
    "                    dA = self.activation_list[l].backward(A_list[l],dZ)#引数増やして渡す。A2\n",
    "                    dZ = self.FC_list[l].backward(dA, Z_list[l-1])#引数増やして渡す。Z1\n",
    "                \n",
    "                # 値を引き渡すための変数\n",
    "                dZ1 = dZ\n",
    "                dA1 = self.activation_list[0].backward(A_list[0], dZ1)#引数増やして渡す。A1\n",
    "                dZ0 = self.FC_list[0].backward(dA1, mini_X_train) # dZ0は使用しない #引数増やして渡す。X\n",
    "\n",
    "                \n",
    "                #dA3, L = self.activation_list[2].backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                #dZ2 = self.FC_list[2].backward(dA3, Z2)#引数増やして渡す。Z2\n",
    "                #dA2 = self.activation_list[1].backward(A2,dZ2)#引数増やして渡す。A2\n",
    "                #dZ1 = self.FC_list[1].backward(dA2, Z1)#引数増やして渡す。Z1\n",
    "                #dA1 = self.activation_list[0].backward(A1, dZ1)#引数増やして渡す。A1\n",
    "                #dZ0 = self.FC_list[0].backward(dA1, mini_X_train) # dZ0は使用しない #引数増やして渡す。X\n",
    "\n",
    "            \"\"\"\n",
    "            Loss Curvを描くための処理\n",
    "            \"\"\"\n",
    "            L_list.append(L)\n",
    "            self.L_list = np.array(L_list) \n",
    "                \n",
    "        \n",
    "    def predict(self,X):\n",
    "\n",
    "        # 最初のZはXなので代入\n",
    "        Z = X\n",
    "\n",
    "        # フォワード処理\n",
    "        for l in range(self.layer):\n",
    "            A = self.FC_list[l].forward(Z)\n",
    "            Z = self.activation_list[l].forward(A)\n",
    "            \n",
    "        y = self.activation_list[-1].forward(A)\n",
    "        pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        #A1 = self.FC_list[0].forward(X)\n",
    "        #Z1 = self.activation_list[0].forward(A1)\n",
    "        #A2 = self.FC_list[1].forward(Z1)\n",
    "        #Z2 = self.activation_list[1].forward(A2)\n",
    "        #A3 = self.FC_list[2].forward(Z2)\n",
    "        #y = self.activation_list[2].forward(A3)\n",
    "        #pred = np.argmax(y, axis=1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】学習と推定  \n",
    ">層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータセット  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平準化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n",
      "[9 7 8 ... 9 1 0]\n"
     ]
    }
   ],
   "source": [
    "# テスト用にデコード\n",
    "y_test_decode = np.argmax(y_val, axis=1)\n",
    "print(y_test_decode.shape)\n",
    "print(y_test_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">pdf記載の良い組み合わせ \n",
    ">- Xavier と sigmoid/tanh\n",
    ">- He と ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のレイヤー数のDNN各層の情報を layer_information リストに入れて　fit 関数に引数として渡すと、  \n",
    "自動的にレイヤーの数を計算して各層のインスタンスがされるよう作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh, Sigmoid, ReLU\n",
    "# SGD, AdaGrad\n",
    "# SimpleInitializer, XavierInitializer、HeInitializer\n",
    "\n",
    "layer_information = [['SGD','ReLU',784,600,'HeInitializer'], # 第１層\n",
    "                                   ['SGD','ReLU',600,400,'HeInitializer'], # 第2層\n",
    "                                   ['SGD','ReLU',400,200,'HeInitializer'], # 第3層\n",
    "                                   ['SGD','ReLU',200,100,'HeInitializer'], # 第4層\n",
    "                                   ['AdaGrad','Softmax',100,10,'SimpleInitializer'], # 第5層\n",
    "                                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### インスタンス化と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN = ScratchDeepNeuralNetrowkClassifier(lr=0.001, sigma=0.01)\n",
    "DNN.fit(X_train, \n",
    "                y_train, \n",
    "                layer_information,\n",
    "                epoch=100, \n",
    "                batch_size=20\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インスタンスが正しく作成されているか確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.FC at 0x7f839d2b5990>,\n",
       " <__main__.FC at 0x7f842bead990>,\n",
       " <__main__.FC at 0x7f842bead150>,\n",
       " <__main__.FC at 0x7f839d2b5a90>,\n",
       " <__main__.FC at 0x7f839d2b5b50>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN.FC_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.ReLU at 0x7f850d46af90>,\n",
       " <__main__.ReLU at 0x7f841cc36990>,\n",
       " <__main__.ReLU at 0x7f842bead9d0>,\n",
       " <__main__.ReLU at 0x7f839d2b5a10>,\n",
       " <__main__.Softmax at 0x7f839d2b5b10>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN.activation_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推定とAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781666666666666"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = DNN.predict(X_val)\n",
    "accuracy_score(y_test_decode, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37400654, 0.19222669, 0.13686016, 0.10845396, 0.08869528,\n",
       "       0.0731665 , 0.06253138, 0.0548513 , 0.04845879, 0.04397902,\n",
       "       0.03941795, 0.03684496, 0.03412166, 0.03307281, 0.03124972,\n",
       "       0.02913789, 0.02808435, 0.02670658, 0.02497211, 0.02332544,\n",
       "       0.02167598, 0.02015955, 0.01759825, 0.0160689 , 0.01468794,\n",
       "       0.01338027, 0.0122737 , 0.01126884, 0.01068503, 0.0096104 ,\n",
       "       0.00894641, 0.00808627, 0.00726337, 0.00676016, 0.00598637,\n",
       "       0.00559843, 0.0050365 , 0.00467732, 0.00425011, 0.00398449,\n",
       "       0.0036643 , 0.00338256, 0.00307555, 0.00294177, 0.00272793,\n",
       "       0.00257284, 0.00239412, 0.00222274, 0.00210332, 0.00198736,\n",
       "       0.00185763, 0.00175588, 0.00166887, 0.00157116, 0.00153598,\n",
       "       0.00147096, 0.00141163, 0.00134696, 0.00130105, 0.00125447,\n",
       "       0.00121429, 0.00117605, 0.00113976, 0.00111294, 0.00107865,\n",
       "       0.0010454 , 0.00102808, 0.00100171, 0.00098216, 0.00095982,\n",
       "       0.00093271, 0.00091369, 0.00089823, 0.00087912, 0.00085651,\n",
       "       0.00084049, 0.00081523, 0.00080061, 0.00078775, 0.00077618,\n",
       "       0.00075503, 0.00074177, 0.00072527, 0.00071196, 0.00069888,\n",
       "       0.00068395, 0.00066896, 0.00065943, 0.00064833, 0.00064004,\n",
       "       0.00062798, 0.00061331, 0.00060641, 0.00060081, 0.00058721,\n",
       "       0.00057833, 0.00057103, 0.00056253, 0.00055392, 0.00054706])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN.L_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数グラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f839d015f90>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5xV5X33/c939p7Zw2k4jiADCAqIGBUjgo2Jpmosaio2Jmpa76hNbm/76KO90+Spee42aUz61KZ5NakN0ZhEk9oYb6MxIZbW22C0NUYFFQ+gCCqHEZTzmRnm8Hv+2GuGzbCHmYFZbJj9fb9e+7XXuta11vqt2bB/+7qudVBEYGZm1lFFqQMwM7MjkxOEmZkV5QRhZmZFOUGYmVlRThBmZlaUE4SZmRXlBGFlQdLXJW2Q9N5h3u9dkv76cO6zVCRdK+npUsdhvccJwg4bSSskXVCC/Y4F/gKYGhGjUtzPfl+QEXFDRHwtrX0eIJaPSmqVtKPD6/cOdyx29MqWOgCzw+A4YGNErCt1IIfZmogYU+og7OjlFoQdEST9d0nLJW2SNFfS6KRckr4laZ2krZJekfSBZNnFkpZI2i7pXUlfKLLdC4DHgdHJL+gfJb+u6zvUa2/dSPobSQ9K+pdk24slTS+oO1bSzyWtl7RR0ncknQTcBfxesp8tSd0fSfp6V8eZLAtJN0haJmmzpDmS1Jt/54J9PSnp7yQ9n/xdfylpWMHyS5Pj3pLUPelAx99h299M4n9H0kVpxG+HhxOElZyk84C/A64AjgVWAg8kiy8EzgEmA0OAK4GNybIfAv8jIgYBHwCe6LjtiPg1cBH5X9MDI+LaboZ1aRLDEGAu8J0k1gzwaBLjeKAOeCAiXgduAH6X7GdID4+zzceBM4HTknp/0M14D8ZngD8FRgPNwB1JnJOBnwJ/DtQC84BfSarq7PgLtjkTWAqMAL4B/DCtJGfpc4KwI8GfAPdExIsR0Qh8ifwv8fFAEzAImAIoIl6PiLXJek3AVEk1EbE5Il7sxZiejoh5EdEC3Ef+CxtgBvkv1C9GxM6IaIiI7g7MHug429weEVsiYhXwG2DaIRzD6KQFUPgaULD8voh4LSJ2An8NXJEkgCuBf4uIxyOiCfgm0A/4EF0f/8qI+H7yd/sx+UQ48hCOwUrICcKOBKPJ/yIFICJ2kG8l1EXEE+R/vc8B3pd0t6SapOrlwMXASklP9fIAbOHZTruAaklZYCz5L8Hmg9hmp8d5gP0OLLahDgPP4zrZ35qIGNLhtbNg+eqC6ZVAJflf/h3jbE3q1tH18b9XsN6uZLLoMdiRzwnCjgRryA8kA5D8yh0OvAsQEXdExBnAyeS7mr6YlC+IiNnAMcAvgAe7ub+dQP+C/WXId6V0x2pgXJIsOurq1sgHPM6eSLqx2l6rerp+YmzB9DjyLbINReJUUvddDnz81sc4QdjhVimpuuCVBe4HrpM0TVIO+P+A5yJihaQzJc2UVEn+i70BaEn6w/9E0uCkG2Qb0NLNGN4k3yK4JNnuXwG5bq77PLAWuF3SgOQYzk6WvQ+MkVTVybqdHmc3993brpY0VVJ/4DbgoaRr6EHgEknnJ3+fvwAagWc48PFbH+MEYYfbPGB3wetvImI++T7wh8l/+ZwAXJXUrwG+D2wm3+2xkXyfOMB/A1ZI2kZ+gPjq7gQQEVuB/wv4AflfxTuB+gOutHfdFuAPgYnAqmS9K5PFTwCLgfckbSiy7oGOMw1tZ24Vvi4vWH4f8CPy3ULVwM1JnEvJ/y3/mXyL4g+BP4yIPV0cv/Ux8gODzMqPpCeBf42IH5Q6FjtyuQVhZmZFOUGYmVlR7mIyM7Oi3IIwM7Oi+sy5zCNGjIjx48eXOgwzs6PKCy+8sCEiil4H1GcSxPjx41m4cGGpwzAzO6pIWtnZMncxmZlZUU4QZmZWlBOEmZkV1WfGIMysb2pqaqK+vp6GhoZSh3JUq66uZsyYMVRWVnZ7HScIMzui1dfXM2jQIMaPH4+fPXRwIoKNGzdSX1/PhAkTur2eu5jM7IjW0NDA8OHDnRwOgSSGDx/e41aYE4SZHfGcHA7dwfwNyz5B7Ghs5h8ff5OXVm0udShmZkeUsk8Qe5pbuWP+Ml5evaXUoZiZHVHKPkFUV+b/BI3NrSWOxMyORFu2bOG73/1uj9e7+OKL2bKl5z88r732Wh566KEer5eGsk8QVRknCDPrXGcJoqXlwE+4nTdvHkOGDEkrrMOi7E9zzWYqyFaIhqbuPs7YzErlq79azJI123p1m1NH1/CVPzy50+W33norb731FtOmTaOyspKBAwdy7LHHsmjRIpYsWcJll13G6tWraWho4JZbbuH6668H9t4fbseOHVx00UV8+MMf5plnnqGuro5f/vKX9OvXr8vY5s+fzxe+8AWam5s588wzufPOO8nlctx6663MnTuXbDbLhRdeyDe/+U1+9rOf8dWvfpVMJsPgwYP5z//8z0P+25R9ggDIZSvcgjCzom6//XZee+01Fi1axJNPPskll1zCa6+91n49wT333MOwYcPYvXs3Z555JpdffjnDhw/fZxvLli3jpz/9Kd///ve54oorePjhh7n66gM/Qr2hoYFrr72W+fPnM3nyZD7zmc9w55138pnPfIZHHnmEN954A0nt3Vi33XYbjz32GHV1dQfVtVWMEwSQq8zQ2OwWhNmR7kC/9A+XGTNm7HOx2R133MEjjzwCwOrVq1m2bNl+CWLChAlMmzYNgDPOOIMVK1Z0uZ+lS5cyYcIEJk+eDMA111zDnDlzuOmmm6iuruZzn/scl1xyCR//+McBOPvss7n22mu54oor+MQnPtEbh+oxCEhaEE1uQZhZ1wYMGNA+/eSTT/LrX/+a3/3ud7z88sucfvrpRS9Gy+Vy7dOZTIbm5uYu99PZ0z6z2SzPP/88l19+Ob/4xS+YNWsWAHfddRdf//rXWb16NdOmTWPjxo09PbT993XIW+gDqisz7mIys6IGDRrE9u3biy7bunUrQ4cOpX///rzxxhs8++yzvbbfKVOmsGLFCpYvX87EiRO57777OPfcc9mxYwe7du3i4osv5qyzzmLixIkAvPXWW8ycOZOZM2fyq1/9itWrV+/XkukpJwjaxiDcxWRm+xs+fDhnn302H/jAB+jXrx8jR45sXzZr1izuuusuTj31VE488UTOOuusXttvdXU19957L5/61KfaB6lvuOEGNm3axOzZs2loaCAi+Na3vgXAF7/4RZYtW0ZEcP7553PaaacdcgzqrBlztJk+fXoc7BPlZn/naYb0r+LHfzqjl6Mys0P1+uuvc9JJJ5U6jD6h2N9S0gsRMb1Y/VTHICTNkrRU0nJJtxZZfoOkVyUtkvS0pKlJ+XhJu5PyRZLuSjPOXNaD1GZmHaXWxSQpA8wBPgbUAwskzY2IJQXV7o+Iu5L6lwL/CMxKlr0VEdPSiq9QrrKCHY1dDxqZmfWWG2+8kd/+9rf7lN1yyy1cd911JYpof2mOQcwAlkfE2wCSHgBmA+0JIiIKr3gZAJSkvyuXzbBxx55S7NrMuiEi+twdXefMmXNY93cwwwlpdjHVAasL5uuTsn1IulHSW8A3gJsLFk2Q9JKkpyR9pNgOJF0vaaGkhevXrz/oQHOVHqQ2O1JVV1ezcePGg/qCs7y2BwZVV1f3aL00WxDF0v1+n3BEzAHmSPpj4K+Aa4C1wLiI2CjpDOAXkk7u0OIgIu4G7ob8IPXBBprLVtDg6yDMjkhjxoyhvr6eQ/kRaHsfOdoTaSaIemBswfwYYM0B6j8A3AkQEY1AYzL9QtLCmAwc3GlKXcgPUjtBmB2JKisre/SYTOs9aXYxLQAmSZogqQq4CphbWEHSpILZS4BlSXltMsiNpOOBScDbaQXq6yDMzPaXWgsiIpol3QQ8BmSAeyJisaTbgIURMRe4SdIFQBOwmXz3EsA5wG2SmoEW4IaI2JRWrPkxCLcgzMwKpXoldUTMA+Z1KPtywfQtnaz3MPBwmrEVqs5m2NPc2ifPlDAzO1i+WR/5FgT4oUFmZoWcIMgPUgO+o6uZWQEnCPKD1IAHqs3MCjhBUJgg3IIwM2vjBEH+iXLgFoSZWSEnCKA6aUH4amozs72cIHALwsysGCcICsYg3IIwM2vnBIEHqc3MinGCoOA6CHcxmZm1c4IAqn0ltZnZfpwgKBik9hiEmVk7Jwj2jkE0uIvJzKydEwQ+i8nMrBgnCDxIbWZWjBMEUJkRkgepzcwKOUEAkqj2c6nNzPaRaoKQNEvSUknLJd1aZPkNkl6VtEjS05KmFiz7UrLeUkl/kGackH9oUEOTu5jMzNqkliAkZYA5wEXAVODThQkgcX9EnBIR04BvAP+YrDsVuAo4GZgFfDfZXmpy2QoPUpuZFUizBTEDWB4Rb0fEHuABYHZhhYjYVjA7AIhkejbwQEQ0RsQ7wPJke6nJZTMepDYzK5BNcdt1wOqC+XpgZsdKkm4EPg9UAecVrPtsh3Xriqx7PXA9wLhx4w4p2Fy2wmMQZmYF0mxBqEhZ7FcQMSciTgD+EvirHq57d0RMj4jptbW1hxRsrtIJwsysUJoJoh4YWzA/BlhzgPoPAJcd5LqHrNpdTGZm+0gzQSwAJkmaIKmK/KDz3MIKkiYVzF4CLEum5wJXScpJmgBMAp5PMdbkLCa3IMzM2qQ2BhERzZJuAh4DMsA9EbFY0m3AwoiYC9wk6QKgCdgMXJOsu1jSg8ASoBm4MSJS/Xmfy2bYurspzV2YmR1V0hykJiLmAfM6lH25YPqWA6z7t8Dfphfdvnyaq5nZvnwldcJnMZmZ7csJIlFd6UFqM7NCThAJtyDMzPblBJHIVWZ8LyYzswJOEIm2FkTEftfjmZmVJSeIRC5bQQQ0tThBmJmBE0Q7P1XOzGxfThCJ6srkudQeqDYzA5wg2rW1IDxQbWaW5wSRyLkFYWa2DyeIRC6bJAjfbsPMDHCCaOdBajOzfTlBJNpbEO5iMjMDnCDa5SrbWhBOEGZm4ATRrq0F4bOYzMzynCASvg7CzGxfThCJ9kFqtyDMzAAniHYepDYz21eqCULSLElLJS2XdGuR5Z+XtETSK5LmSzquYFmLpEXJa26acYIHqc3MOkrtmdSSMsAc4GNAPbBA0tyIWFJQ7SVgekTskvRnwDeAK5NluyNiWlrxdeRBajOzfaXZgpgBLI+ItyNiD/AAMLuwQkT8JiJ2JbPPAmNSjOeA3MVkZravNBNEHbC6YL4+KevMZ4F/L5ivlrRQ0rOSLiu2gqTrkzoL169ff0jBSqIqW+Erqc3MEql1MQEqUlb0aTySrgamA+cWFI+LiDWSjgeekPRqRLy1z8Yi7gbuBpg+ffohP+knl63wvZjMzBJptiDqgbEF82OANR0rSboA+F/ApRHR2FYeEWuS97eBJ4HTU4wVyJ/q6i4mM7O8NBPEAmCSpAmSqoCrgH3ORpJ0OvA98slhXUH5UEm5ZHoEcDZQOLidiupKdzGZmbVJrYspIpol3QQ8BmSAeyJisaTbgIURMRf4B2Ag8DNJAKsi4lLgJOB7klrJJ7HbO5z9lAp3MZmZ7ZXmGAQRMQ+Y16HsywXTF3Sy3jPAKWnGVky+i8ktCDMz8JXU+8hVVngMwsws4QRRwF1MZmZ7OUEUcBeTmdleThAFqisraHALwswMcILYh1sQZmZ7OUEUyGU9SG1m1sYJooDPYjIz28sJokAum/ET5czMEk4QBardgjAza+cEUSCXzdDcGjS3OEmYmTlBFPBDg8zM9nKCKOAEYWa2lxNEgVxlBsDXQpiZ4QSxj/YWhK+mNjNzgihUnbQgGtyCMDNzgijkFoSZ2V5OEAVy2bYxCCcIM7NUE4SkWZKWSlou6dYiyz8vaYmkVyTNl3RcwbJrJC1LXtekGWebXGXbWUzuYjIzSy1BSMoAc4CLgKnApyVN7VDtJWB6RJwKPAR8I1l3GPAVYCYwA/iKpKFpxdrGXUxmZnt1K0FIOkFSLpn+qKSbJQ3pYrUZwPKIeDsi9gAPALMLK0TEbyJiVzL7LDAmmf4D4PGI2BQRm4HHgVndO6SD5y4mM7O9utuCeBhokTQR+CEwAbi/i3XqgNUF8/VJWWc+C/x7T9aVdL2khZIWrl+/votwuladdDE1+IZ9ZmbdThCtEdEM/BHw7Yj4n8CxXayjImVRtKJ0NTAd+IeerBsRd0fE9IiYXltb20U4XXMLwsxsr+4miCZJnwauAR5Nyiq7WKceGFswPwZY07GSpAuA/wVcGhGNPVm3t+291YZbEGZm3U0Q1wG/B/xtRLwjaQLwr12sswCYJGmCpCrgKmBuYQVJpwPfI58c1hUsegy4UNLQZHD6wqQsVXvPYnILwsws251KEbEEuBkg+cIeFBG3d7FOs6SbyH+xZ4B7ImKxpNuAhRExl3yX0kDgZ5IAVkXEpRGxSdLXyCcZgNsiYtNBHF+P5LIZJNjV2Jz2rszMjnjdShCSngQuTeovAtZLeioiPn+g9SJiHjCvQ9mXC6YvOMC69wD3dCe+3pKpEMMH5Fi3vbHrymZmfVx3u5gGR8Q24BPAvRFxBtDpl/vRbGRNjve2NZQ6DDOzkutugshKOha4gr2D1H3SqJpq3tvqBGFm1t0EcRv5sYS3ImKBpOOBZemFVTojB1fzvlsQZmbdHqT+GfCzgvm3gcvTCqqURtVUs3lXEw1NLe23/zYzK0fdvdXGGEmPSFon6X1JD0sa0/WaR59RNdUArPdAtZmVue52Md1L/hqG0eRvefGrpKzPGTk4nyA8UG1m5a67CaI2Iu6NiObk9SPg0O9tcQRqa0F4oNrMyl13E8QGSVdLyiSvq4GNaQZWKm0JwgPVZlbuupsg/pT8Ka7vAWuBT5K//UafU9MvSy5b4RaEmZW9biWIiGi7BUZtRBwTEZeRv2iuz5HEqMHVHoMws7J3KE+UO+BtNo5mI2uqWbfNZzGZWXk7lARR7JkNfcKoGrcgzMwOJUEUffhPX9DWxRTRZw/RzKxLB7ySWtJ2iicCAf1SiegIMLKmmj3NrWzZ1cTQAVWlDsfMrCQOmCAiYtDhCuRI0n4txLYGJwgzK1uH0sXUZ42syQG+mtrMypsTRBEj2y6W87UQZlbGnCCKaE8QPtXVzMpYqglC0ixJSyUtl3RrkeXnSHpRUrOkT3ZY1iJpUfKam2acHVVlKxg+oMpdTGZW1rr1PIiDISkDzAE+BtQDCyTNjYglBdVWAdcCXyiyid0RMS2t+LoyssYPDjKz8pZaggBmAMuThwsh6QFgNtCeICJiRbKsNcU4DsqowX70qJmVtzS7mOqA1QXz9UlZd1VLWijpWUmXFasg6fqkzsL169cfSqz7cQvCzMpdmgmi2K04enJp8riImA78MfBtSSfst7GIuyNiekRMr63t3cdTjKqpZuPOPTQ2t/Tqds3MjhZpJoh6YGzB/BhgTXdXjog1yfvbwJPA6b0ZXFfaroXwo0fNrFylmSAWAJMkTZBUBVxF/rGlXZI0VFIumR4BnE3B2MXh0PboUXczmVm5Si1BREQzcBPwGPA68GBELJZ0m6RLASSdKake+BTwPUmLk9VPAhZKehn4DXB7h7OfUrf30aNuQZhZeUrzLCYiYh4wr0PZlwumF5Dveuq43jPAKWnG1pXC+zGZmZUjX0ndiSH9K8llK1i7ZXepQzEzKwkniE5IYtLIgbzx3vZSh2JmVhJOEAdwSt0QXqnf4gcHmVlZcoI4gNPGDGZbQzMrN+4qdShmZoedE8QBnDJmMACvvLu1xJGYmR1+ThAHMHnkIHLZCl5ZvaXUoZiZHXZOEAdQmalg6ugatyDMrCw5QXTh1LrBLH53Ky2tHqg2s/LiBNGFU8cMYeeeFt5ev6PUoZiZHVZOEF04tW2gut7dTGZWXpwgunB87UD6V2V41eMQZlZmnCC6kKkQH6gbzMv1PpPJzMqLE0Q3nFo3mCVrttHUcsQ9GdXMLDVOEN1wypjBNDa38ub7vi+TmZUPJ4huOG3MEABe9UC1mZURJ4huOG54f2qqsyzyFdVmVkacILpBEjOPH85/LdvgO7uaWdlINUFImiVpqaTlkm4tsvwcSS9Kapb0yQ7LrpG0LHldk2ac3XHelGN4d8tu3nzfF8yZWXlILUFIygBzgIuAqcCnJU3tUG0VcC1wf4d1hwFfAWYCM4CvSBqaVqzd8fsnHgPAE2+sK2UYZmaHTZotiBnA8oh4OyL2AA8AswsrRMSKiHgF6Hj+6B8Aj0fEpojYDDwOzEox1i6NGlzNyaNreOKN90sZhpnZYZNmgqgDVhfM1ydlvbaupOslLZS0cP369QcdaHedN+UYXli5mS279qS+LzOzUkszQahIWXdHeLu1bkTcHRHTI2J6bW1tj4I7GL8/5RhaA556M/1kZGZWamkmiHpgbMH8GGDNYVg3NaeNGcLwAVX8xuMQZlYG0kwQC4BJkiZIqgKuAuZ2c93HgAslDU0Gpy9MykoqUyHOPbGWp95c7+dDmFmfl1qCiIhm4CbyX+yvAw9GxGJJt0m6FEDSmZLqgU8B35O0OFl3E/A18klmAXBbUlZy5005hs27mli0enOpQzEzS1U2zY1HxDxgXoeyLxdMLyDffVRs3XuAe9KM72B8ZFItmQox//V1nHHcsFKHY2aWGl9J3UOD+1Uyc8Iw/v2193xVtZn1aU4QB+GPTq/jnQ07eXGVu5nMrO9ygjgIF59yLP2rMjz0Qn2pQzEzS40TxEEYkMty0QeO5dGX19LQ1FLqcMzMUuEEcZAuP6OO7Y3NPLb4vVKHYmaWCieIg3TWhOHUDennbiYz67OcIA5SRYW4/IwxPL18A2u37i51OGZmvc4J4hBc/sE6IuDnL75b6lDMzHqdE8QhOG74AGZMGMb9z63yYLWZ9TlOEIfoz8+fxLtbdnPf71aWOhQzs17lBHGIPjRxBB89sZZ/fmKZnxNhZn2KE0Qv+MtZU9je2Mx3n3yr1KGYmfUaJ4hecNKxNVz+wTH86LcrWL1pV6nDMTPrFU4QveTzH5uMBP/w2NJSh2Jm1iucIHrJ6CH9+B/nnsDcl9fwy0U+7dXMjn5OEL3o5vMmcub4oXzp56+yfN2OUodjZnZInCB6UTZTwT9/+oNUV2a48ScvsnuPr40ws6OXE0QvGzW4mm9fOY03123nr37xmh8qZGZHrVQThKRZkpZKWi7p1iLLc5L+d7L8OUnjk/LxknZLWpS87kozzt52zuRabjl/Eg+/WM8/zV9W6nDMzA5Kas+klpQB5gAfA+qBBZLmRsSSgmqfBTZHxERJVwF/D1yZLHsrIqalFV/abjl/Eu9u3s23f72MEQNzXH3WcaUOycysR9JsQcwAlkfE2xGxB3gAmN2hzmzgx8n0Q8D5kpRiTIeNJP7uE6dw/pRj+Otfvsa8V9eWOiQzsx5JM0HUAasL5uuTsqJ1IqIZ2AoMT5ZNkPSSpKckfaTYDiRdL2mhpIXr16/v3eh7QTZTwXf++IN8cNxQbv7pS/zw6Xc8JmFmR400E0SxlkDHb8fO6qwFxkXE6cDngfsl1exXMeLuiJgeEdNra2sPOeA09KvKcO91Z/L7U47ha48u4ab7X2JHY3OpwzIz61KaCaIeGFswPwZY01kdSVlgMLApIhojYiNARLwAvAVMTjHWVNVUV3L3fzuDWy+awn8sfo9Lv/M0y97fXuqwzMwOKM0EsQCYJGmCpCrgKmBuhzpzgWuS6U8CT0RESKpNBrmRdDwwCXg7xVhTJ4kbzj2Bn3xuJtt2NzN7zm/5t1c8LmFmR67UEkQypnAT8BjwOvBgRCyWdJukS5NqPwSGS1pOviup7VTYc4BXJL1MfvD6hojYlFash9NZxw/n0f/7w0wZNYgb73+Rrz+6hO0NTaUOy8xsP+org6bTp0+PhQsXljqMbtvT3MrXHl3Cfc+uZGAuy5VnjuXaD41n7LD+pQ7NzMqIpBciYnrRZU4QpfVK/RZ++PQ7/Nsra2mN4JJTR3PDucdz8ujBpQ7NzMqAE8RRYO3W3dz72xX85NmV7NzTwjmTa/nIxBFMOXYQU0bVUDsoV+oQzawPcoI4imzd1cS/PreS+59bxbtbdreXTxk1iAtPHsWFU0dy8uga+sj1hGZWYk4QR6lNO/fwxnvbeLV+K/NfX8fClZtoDTh+xAAuO72OPzq9zmMWZnZInCD6iA07Gnl8yfv84qV3ee6d/Eld44f356Rja5gyqobTxg7mjOOGMqi6ssSRmtnRwgmiD6rfvItHX1nLy6u38PrabazctIsIqBBMGVXD2ROHc/5JI5l+3FCyGd/V3cyKc4IoAzsam3l59Raef2cTz7+ziRdWbmZPSys11VnOPfEYzp1cyzmTR3DMoOpSh2pmR5ADJYjUbvdth9fAXJazJ47g7IkjgHzCeHrZen79+jqeenM9v3o5f5eTk0fX8NETazl38jF8cNwQty7MrFNuQZSB1tZgydptPPXmep5aup4XVm2mpTUYlMsy8/hhfOiEEZx7Yi0n1A4sdahmdpi5i8n2sXV3E79dvoH/WraBZ97awMqNuwD4yKQR/OnZEzh3ci0VFT6N1qwcOEHYAdVv3sUvF63hx8+sYN32Ro4b3p+PTq7l7IkjmHn8cAb381lRZn2VE4R1y57mVv7t1TU88tIann9nIw1Nre1nRc2YMIwzxw/jjOOGMmqwB7rN+gonCOuxxuYWXlq1hd+9tZEFKzbx4qrNNDS1AlA3pB+njxvCKXWDOXn0YE4eXcPQAVUljtjMDobPYrIey2UznHX8cM46Pv8E2KaWVl57dysvrtrCi6s28+LKzTxa8DyLYwblOHHUICYdM4gxQ/sxsqaakTU5jq8dyDAnD7OjklsQdtA27dzDkjXbWLJ2K0vf28Gb729n2brt7S2NNiNrcpx0bA0TRgxg9OB+jB7Sj3HD+jN+RH9f9W1WYm5BWCqGDajiw5NG8OFJI9rLIoLNu5pYt72BtVsbWP7+Dpas3cbra7ex4J1N7NzTss82RgzMMWFEf8YPH8D4EQOoG9KPEQNzDB9Yxciaaob2r/SNCc1KxC0IO2wigm0Nzby7eTerNu3inQ07eWfDDlZs2MWKjTtZt71xv3UGVGUYO2AGqrIAAAmsSURBVKw/o4f0Y0i/Sgb3r2Ro/ypqB+U4ZlCOEQNz1PSrpKY6S02/Sip94Z9Zj5SsBSFpFvBPQAb4QUTc3mF5DvgX4AxgI3BlRKxIln0J+CzQAtwcEY+lGaulTxKD+1UyuF8lU0fX7Ld8Z2Mz721rYOOOPWzY0cjarQ2s3rSL1Zt2sWZrA0vf287W3U3saGzudB8Dc1kG96tk6IBKBlRl6V+VoX8uy6BcloG5LAOr8+/9q7IMyGXyZbksA3JZqisrqMzkX7lsBdWVGaorM2R8TYiVqdQShKQMMAf4GFAPLJA0NyKWFFT7LLA5IiZKugr4e+BKSVOBq4CTgdHAryVNjoh9+yesTxmQy3JC7UBOqD1wvcbmFjbs2MP67Y1s2N7ItoYmtjc0s3V3E1t2NbFl1x627G5iZ2MzG3bsYefGXexobGZHYzO79vT8n1BlRlRlKqjM5hNHLpshl62gqmC+KltBZUZkKyrIZkRlpoJshchmKqhqm8/sWydTIbIVe9+zmQoyFaJCQkBFBVQoP58v7zBfITLSPvUqlE/EhXUlUDLf/k6+XuF8Yd22bYikrO2PkZS17Se/nXwZbeuy/7K2XsL292SLReu7S/GIkWYLYgawPCLeBpD0ADAbKEwQs4G/SaYfAr6j/L+O2cADEdEIvCNpebK936UYrx0lctkMdUP6UTekX4/XbWkNdu5pZldjCzsam9mZvLY3NtPY3EpTcytNLa00NrfS0NTC7qYWGptb2VP4ammlsbmFxqZkuqmVLbubaG7Jr9vcEjS1Ju8trTS1BM0t+brNrUEf6dVN3d5kUiQZJQu0X13tt17bNMXK99mGOmxPBdMFSbJDHIX5rHDb+6y7T539E+A+SbjjNvapWHw/Jx1bwz9/+vT9tnuo0kwQdcDqgvl6YGZndSKiWdJWYHhS/myHdes67kDS9cD1AOPGjeu1wK3vylSImupKakp49lRraz6BtLQGza1BS0vy3ppPKACtEbRG8t4atETQ2tpWnq+bf99b1toKwb7rRezdFgXLIiDIjwsV1omIDuX5bQLt65DUaW1tq7t3W3vrFe5j73yhiOJ12vaRj7jIsoKy9koUi2H/uoUxtB1rsTqF5fvspG15+z47lB8gvg6TRePYL8YO9drLOmxo7NCe/1jqjjQTRLF2Yse/T2d1urMuEXE3cDfkB6l7GqBZKVRUiFxFptRhmHUpzVM+6oGxBfNjgDWd1ZGUBQYDm7q5rpmZpSjNBLEAmCRpgqQq8oPOczvUmQtck0x/Engi8u2oucBVknKSJgCTgOdTjNXMzDpIrYspGVO4CXiM/Gmu90TEYkm3AQsjYi7wQ+C+ZBB6E/kkQlLvQfID2s3AjT6Dyczs8PKFcmZmZexAF8r5slMzMyvKCcLMzIpygjAzs6KcIMzMrKg+M0gtaT2w8hA2MQLY0EvhHC3K8ZihPI+7HI8ZyvO4e3rMx0VE0Tug9ZkEcagkLexsJL+vKsdjhvI87nI8ZijP4+7NY3YXk5mZFeUEYWZmRTlB7HV3qQMogXI8ZijP4y7HY4byPO5eO2aPQZiZWVFuQZiZWVFOEGZmVlTZJwhJsyQtlbRc0q2ljictksZK+o2k1yUtlnRLUj5M0uOSliXvQ0sda2+TlJH0kqRHk/kJkp5Ljvl/J7ej71MkDZH0kKQ3ks/89/r6Zy3pfyb/tl+T9FNJ1X3xs5Z0j6R1kl4rKCv62SrvjuT77RVJH+zJvso6QUjKAHOAi4CpwKclTS1tVKlpBv4iIk4CzgJuTI71VmB+REwC5ifzfc0twOsF838PfCs55s3AZ0sSVbr+CfiPiJgCnEb++PvsZy2pDrgZmB4RHyD/iIGr6Juf9Y+AWR3KOvtsLyL/PJ1J5B/PfGdPdlTWCQKYASyPiLcjYg/wADC7xDGlIiLWRsSLyfR28l8YdeSP98dJtR8Dl5UmwnRIGgNcAvwgmRdwHvBQUqUvHnMNcA75560QEXsiYgt9/LMm/3ybfsnTKfsDa+mDn3VE/Cf55+cU6uyznQ38S+Q9CwyRdGx391XuCaIOWF0wX5+U9WmSxgOnA88BIyNiLeSTCHBM6SJLxbeB/wdoTeaHA1siojmZ74uf+fHAeuDepGvtB5IG0Ic/64h4F/gmsIp8YtgKvEDf/6zbdPbZHtJ3XLknCBUp69Pn/UoaCDwM/HlEbCt1PGmS9HFgXUS8UFhcpGpf+8yzwAeBOyPidGAnfag7qZikz302MAEYDQwg373SUV/7rLtySP/eyz1B1ANjC+bHAGtKFEvqJFWSTw4/iYifJ8XvtzU5k/d1pYovBWcDl0paQb778DzyLYohSTcE9M3PvB6oj4jnkvmHyCeMvvxZXwC8ExHrI6IJ+DnwIfr+Z92ms8/2kL7jyj1BLAAmJWc6VJEf1Jpb4phSkfS9/xB4PSL+sWDRXOCaZPoa4JeHO7a0RMSXImJMRIwn/9k+ERF/AvwG+GRSrU8dM0BEvAeslnRiUnQ++ee799nPmnzX0lmS+if/1tuOuU9/1gU6+2znAp9JzmY6C9ja1hXVHWV/JbWki8n/qswA90TE35Y4pFRI+jDwX8Cr7O2P/3/Jj0M8CIwj/5/sUxHRcQDsqCfpo8AXIuLjko4n36IYBrwEXB0RjaWMr7dJmkZ+YL4KeBu4jvwPwj77WUv6KnAl+TP2XgI+R76/vU991pJ+CnyU/G293we+AvyCIp9tkiy/Q/6sp13AdRGxsNv7KvcEYWZmxZV7F5OZmXXCCcLMzIpygjAzs6KcIMzMrCgnCDMzK8oJwqwHJLVIWlTw6rUrlCWNL7xDp1mpZbuuYmYFdkfEtFIHYXY4uAVh1gskrZD095KeT14Tk/LjJM1P7sU/X9K4pHykpEckvZy8PpRsKiPp+8lzDf6PpH4lOygre04QZj3Tr0MX05UFy7ZFxAzyV65+Oyn7DvnbLZ8K/AS4Iym/A3gqIk4jf5+kxUn5JGBORJwMbAEuT/l4zDrlK6nNekDSjogYWKR8BXBeRLyd3BTxvYgYLmkDcGxENCXlayNihKT1wJjC2z4kt2F/PHnoC5L+EqiMiK+nf2Rm+3MLwqz3RCfTndUppvA+QS14nNBKyAnCrPdcWfD+u2T6GfJ3kgX4E+DpZHo+8GfQ/szsmsMVpFl3+deJWc/0k7SoYP4/IqLtVNecpOfI//D6dFJ2M3CPpC+Sf8rbdUn5LcDdkj5LvqXwZ+SfhGZ2xPAYhFkvSMYgpkfEhlLHYtZb3MVkZmZFuQVhZmZFuQVhZmZFOUGYmVlRThBmZlaUE4SZmRXlBGFmZkX9/8wxnSIqceTaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_function_epoch = DNN.L_list\n",
    "#loss_function_epoch_val = DNN.L_list_val\n",
    "plt.title('Loss function - Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss_function_epoch, label='train_loss')#, color='b'\n",
    "#plt.plot(loss_function_epoch_val, label='val_loss')# color='b'\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数にReLU、初期化にHe、最適化関数にSGDを用いて、学習率0.001、エポック100の  \n",
    "設定で実行したところ、精度が最も高くなった。  \n",
    "ただし、上記設定では学習曲線の形が若干よくなかったため、最終層の最適化にAdaGradを用いたところ、  \n",
    "精度は0.05％程度下がったが学習曲線がガタついたりせず綺麗な形になった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
