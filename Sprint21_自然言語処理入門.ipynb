{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprintの目的\n",
    ">- 自然言語処理の一連の流れを学ぶ\n",
    ">- 自然言語のベクトル化の方法を学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語のベクトル化\n",
    ">**自然言語処理**（**NLP**, **Natural Language Processing**） とは人間が普段使っている 自然言語 をコンピュータに処理させる技術のことです。ここではその中でも、機械学習の入力として自然言語を用いることを考えていきます。\n",
    ">\n",
    ">多くの機械学習手法は **数値データ**（**量的変数**） の入力を前提にしていますので、`自然言語の **テキストデータ** を数値データに変換する必要があります。`これを **自然言語のベクトル化** と呼びます。ベクトル化の際にテキストデータの特徴をうまく捉えられるよう、様々な手法が考えられてきていますので、このSprintではそれらを学びます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非構造化データ\n",
    ">データの分類として、表に数値がまとめられたような`コンピュータが扱いやすい形`を **構造化データ** 、人間が扱いやすい画像・動画・テキスト・音声などを **非構造化データ** と呼ぶことがあります。  \n",
    ">自然言語のベクトル化は、`非構造化データを構造化データに変換する工程`と言えます。同じ非構造化データでも、画像に対してはディープラーニングを用いる場合この変換作業はあまり必要がありませんでしたが、`テキストにおいてはこれをどう行うかが重要`です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然言語処理により何ができるか\n",
    ">機械学習の入力や出力に自然言語のテキストを用いることで様々なことができます。入力も出力もテキストである例としては **機械翻訳** があげられ、実用化されています。入力は画像で出力がテキストである **画像キャプション生成** やその逆の文章からの画像生成も研究が進んでいます。\n",
    ">\n",
    ">しかし、出力をテキストや画像のような非構造化データとすることは`難易度が高い`です。`比較的簡単にできることとしては、入力をテキスト、出力をカテゴリーとする **テキスト分類** です`。\n",
    ">\n",
    ">アヤメやタイタニック、手書き数字のような定番の存在として、**IMDB映画レビューデータセット** の感情分析があります。レビューの文書が映画に対して`肯定的か否定的かを2値分類`します。文書ごとの肯定・否定はラベルが与えられています。このSprintではこれを使っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB映画レビューデータセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダウンロード\n",
    ">次のwgetコマンドによってダウンロードします。\n",
    ">\n",
    ">以下のサイトで公開されているデータセットです。\n",
    ">Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-25 00:57:12--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "ai.stanford.edu (ai.stanford.edu) をDNSに問いあわせています... 171.64.68.10\n",
      "ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 84125825 (80M) [application/x-gzip]\n",
      "`aclImdb_v1.tar.gz.11' に保存中\n",
      "\n",
      "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  16.4MB/s 時間 6.8s       \n",
      "\n",
      "2020-12-25 00:57:20 (11.8 MB/s) - `aclImdb_v1.tar.gz.11' へ保存完了 [84125825/84125825]\n",
      "\n",
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 読み込み  \n",
    ">scikit-learnのload_filesを用いて読み込みます。\n",
    ">\n",
    ">sklearn.datasets.load_files — scikit-learn 0.21.3 documentation\n",
    ">\n",
    ">《読み込むコード》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x_train).shape)\n",
    "print(np.array(y_train).shape)\n",
    "print(np.array(x_test).shape)\n",
    "print(np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## このデータセットについて\n",
    ">中身を見てみると、英語の文章が入っていることが分かります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">IMDBはInternet Movie Databaseの略で、映画のデータベースサイトです。\n",
    ">\n",
    ">Ratings and Reviews for New Movies and TV Shows - IMDb\n",
    ">\n",
    ">このサイトではユーザーが映画に対して`1から10点の評価とコメント`を投稿することができます。そのデータベースから訓練データは25000件、テストデータは25000件のデータセットを作成しています。\n",
    ">\n",
    ">`4点以下を否定的、7点以下を肯定的なレビューとして2値のラベル付け`しており、これにより感情の分類を行います。5,6点の中立的なレビューはデータセットに含んでいません。また、ラベルは訓練用・テスト用それぞれで均一に入っています。詳細はダウンロードしたREADMEを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 古典的な手法\n",
    ">古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW\n",
    "\n",
    ">単純ながら効果的な方法として **BoW (Bag of Words)** があります。\n",
    ">これは、サンプルごとに単語などの **登場回数** を数えたものをベクトルとする方法です。\n",
    ">単語をカテゴリとして捉え **one-hot表現** していることになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例\n",
    ">例として、IMDBデータセットからある3文の最初の5単語を抜き出したものを用意しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n",
    ">\n",
    ">sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">例にあげた`3文の中で登場する8種類の単語が列名`になり、`0,1,2番目のサンプルでそれらが何回登場しているか`を示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ **語彙** と呼びます。\n",
    ">\n",
    ">テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。`この時使用したテキスト全体`のことを **コーパス** と呼びます。`語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習`を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理  \n",
    ">CountVectorizerクラスでは大文字は小文字に揃えるという **前処理** が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（**テキストクリーニング**）や表記揺れの統一といったことを別途行うことが一般的です。\n",
    ">\n",
    ">語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える **ステミング** と呼ばれる処理を行うこともあります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン\n",
    ">`BoWは厳密には単語を数えているのではなく、 **トークン（token）** として定めた固まりを数えます`。\n",
    ">\n",
    ">何をトークンとするかは**CountVectorizer**では引数`token_pattern`で **正規表現** の記法により指定されます。デフォルトは`r'(?u)\\b\\w\\w+\\b'`ですが、上の例では`r'(?u)\\b\\w+\\b'`としています。\n",
    ">\n",
    ">デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。  \n",
    ">英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 《正規表現》\n",
    "\n",
    ">正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n",
    ">\n",
    ">re — 正規表現操作\n",
    ">\n",
    ">正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n",
    ">\n",
    ">Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析\n",
    ">英語などの多くの言語では`空白`という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n",
    ">\n",
    ">`日本語では名詞や助詞、動詞のように異なる **品詞** で分けられる単位で **分かち書き** することになります`。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n",
    ">\n",
    ">`これには **MeCab** や **Janome** のような形態素解析ツールを用います`。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できる**Web茶まめ**というサービスも国立国語研究所が提供しています。\n",
    ">\n",
    ">自然言語では`新しい言葉`も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として **mecab-ipadic-NEologd** がオープンソースで存在しています。\n",
    ">\n",
    ">mecab-ipadic-neologd/README.ja.md at master · neologd/mecab-ipadic-neologd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram\n",
    ">上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは`語順`は全く考慮されていません。\n",
    ">\n",
    ">考慮するために、`隣あう単語同士をまとめて扱う **n-gram** `という考え方を適用することがあります。2つの単語をまとめる場合は **2-gram (bigram)** と呼び、次のようになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">2-gramにより「very good」と「very bad」が区別して数えられています。\n",
    ">\n",
    ">単語をまとめない場合は **1-gram (unigram) **と呼びます。3つまとめる3-gram(trigram)など任意の数を考えることができます。1-gramと2-gramを組み合わせてBoWを行うといったこともあります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】BoWのスクラッチ実装  \n",
    ">以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "\n",
    ">This movie is SOOOO funny!!!  \n",
    ">What a movie! I never  \n",
    ">best movie ever!!!!! this movie  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>movie</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>movie</th>\n",
       "      <th>ever</th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  movie  is  soooo  funny  what  a  movie  i  never  best  movie  ever  \\\n",
       "0     1      1   1      1      1     0  0      1  0      0     0      1     0   \n",
       "1     0      1   0      0      0     1  1      1  1      1     0      1     0   \n",
       "2     1      2   0      0      0     0  0      2  0      0     1      2     1   \n",
       "\n",
       "   this  movie  \n",
       "0     1      1  \n",
       "1     0      1  \n",
       "2     1      2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is soooo</th>\n",
       "      <th>soooo funny</th>\n",
       "      <th>what a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie i</th>\n",
       "      <th>i never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this movie  movie is  is soooo  soooo funny  what a  a movie  movie i  \\\n",
       "0           1         1         1            1       0        0        0   \n",
       "1           0         0         0            0       1        1        1   \n",
       "2           1         0         0            0       0        0        0   \n",
       "\n",
       "   i never  best movie  movie ever  ever this  this movie  \n",
       "0        0           0           0          0           1  \n",
       "1        1           0           0          0           0  \n",
       "2        0           1           1          1           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scratch_Bow(sentence, n):\n",
    "    \"\"\"\n",
    "    Bowのスクラッチ\n",
    "    \n",
    "    Parameters\n",
    "    ----------------\n",
    "    sentence: list（string）\n",
    "      Bowの対象となる文章\n",
    "    n: int\n",
    "      n-gramのn\n",
    "\n",
    "    Returns\n",
    "    ----------------\n",
    "    \n",
    "    \"\"\"\n",
    "    # 正規表現の準備\n",
    "    kigo = string.punctuation# >>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    table = str.maketrans( \"\", \"\", kigo)\n",
    "\n",
    "    # 全部小文字にする\n",
    "    sentence = list(map(str.lower,sentence))\n",
    "    \n",
    "    #文ごとに単語単位に分ける\n",
    "    sentence_words = []\n",
    "    for s in sentence:\n",
    "        result = s.translate(table)#正規表現\n",
    "        sentence_words.append(result.split(' '))\n",
    "        \n",
    "    ############ n-gram ############\n",
    "    sentence_words_n_gram = []\n",
    "    for s in sentence_words:\n",
    "        n_gram_temp = []\n",
    "        for i in range(len(s)-n+1):\n",
    "            n_gram_temp.append(' '.join(s[i:i+n]))\n",
    "        sentence_words_n_gram.append(n_gram_temp)\n",
    "\n",
    "    # 特徴量用に平坦化する\n",
    "    feature_list = sum(sentence_words_n_gram, [])\n",
    "    \n",
    "    # BoWの表の作成\n",
    "    n_appearances = []\n",
    "    for s in sentence_words_n_gram:\n",
    "        n_app_sentence = []\n",
    "        for f in feature_list:\n",
    "            n_app_sentence.append(s.count(f))\n",
    "        n_appearances.append(n_app_sentence)\n",
    "\n",
    "    df = pd.DataFrame(n_appearances, columns=feature_list)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "# 分類したい文字列\n",
    "sentence = ['This movie is SOOOO funny!!!',\n",
    "                        'What a movie! I never',\n",
    "                        'best movie ever!!!!! this movie'\n",
    "                       ]\n",
    "\n",
    "n=1\n",
    "scratch_Bow(sentence, n)\n",
    "\n",
    "n=2\n",
    "scratch_Bow(sentence, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">BoWの発展的手法として **TF-IDF** もよく使われます。これは **Term Frequency (TF)** と **Inverse Document Frequency (IDF)** という2つの指標の組み合わせです。\n",
    ">\n",
    ">《標準的なTF-IDFの式》\n",
    ">\n",
    ">\n",
    ">Term Frequency:\n",
    ">$\n",
    "tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}\n",
    "$\n",
    ">\n",
    ">\n",
    ">$n_{t,d}$ : サンプルd内のトークンtの出現回数（BoWと同じ）\n",
    ">\n",
    ">$\\sum_{s \\in d}n_{s,d}$ : サンプルdの全トークンの出現回数の和\n",
    ">\n",
    ">\n",
    ">Inverse Document Frequency:\n",
    ">$\n",
    "idf(t) = \\log{\\frac{N}{df(t)}}\n",
    "$\n",
    ">\n",
    ">$N$ : サンプル数\n",
    ">\n",
    ">$df(t)$ : トークンtが出現するサンプル数\n",
    ">\n",
    ">＊logの底は任意の値\n",
    ">\n",
    ">\n",
    ">TF-IDF:\n",
    ">\n",
    ">$\n",
    "tfidf(t, d) = tf(t, d) \\times idf(t)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF\n",
    ">IDFは`そのトークンがデータセット内で珍しい`ほど値が大きくなる指標です。\n",
    ">\n",
    ">サンプル数 $N$ をIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 $df(t)$ を変化させたグラフを確認してみると、次のようになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1,n_samples))\n",
    "plt.title(\"IDF\")\n",
    "plt.xlabel(\"df(t)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.plot(idf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TF-IDFではこの数を`出現回数に掛け合わせる`ので、`珍しいトークンの登場に重み付け`を行なっていることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ストップワード\n",
    ">`あまりにも頻繁に登場するトークン`は、値を小さくするだけでなく、`取り除くという前処理`を加えることもあります。  \n",
    ">取り除くもののことを **ストップワード** と呼びます。  \n",
    ">既存の**ストップワード一覧**を利用したり、しきい値によって求めたりします。  \n",
    ">\n",
    ">scikit-learnのCountVectorizerでは引数`stop_words`に**リストで指定**することで処理を行なってくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  movie  this  very\n",
       "0  0    0     0     1      1     1     1\n",
       "1  1    0     1     1      0     1     0\n",
       "2  0    2     0     0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">代表的な既存のストップワード一覧としては、NLTK という自然言語処理のライブラリのものがあげられます。\n",
    ">あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ishiitomoaki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">逆に、`登場回数が特に少ないトークンも取り除くことが多い`です。全てのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n",
    ">\n",
    ">scikit-learnのCountVectorizerでは引数`max_featuresに最大の語彙数を指定`することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  is  this  very\n",
       "0    0     1   1     1     1\n",
       "1    0     1   1     1     0\n",
       "2    2     0   0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】TF-IDFの計算\n",
    ">IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。  \n",
    ">NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。  \n",
    ">テキストクリーニングやステミングなどの前処理はこの問題では要求しません。  \n",
    ">\n",
    ">TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。  \n",
    ">\n",
    ">sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation  \n",
    ">sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation  \n",
    ">\n",
    ">なお、scikit-learnでは標準的な式とは異なる式が採用されています。  \n",
    ">\n",
    ">また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。  \n",
    ">\n",
    ">\n",
    ">Term Frequency:  \n",
    ">\n",
    ">$\n",
    "tf(t,d) = n_{t,d}\n",
    "$\n",
    ">\n",
    ">$n_{t,d}$ : サンプルd内のトークンtの出現回数\n",
    ">\n",
    ">scikit-learnのTFは分母がなくなりBoWと同じ計算になります。\n",
    ">\n",
    ">\n",
    ">Inverse Document Frequency:\n",
    ">\n",
    ">$\n",
    "idf(t) = \\log{\\frac{1+N}{1+df(t)}}+1\n",
    "$\n",
    ">\n",
    ">$N$ : サンプル数\n",
    ">\n",
    ">$df(t)$ : トークンtが出現するサンプル数\n",
    ">\n",
    ">＊logの底はネイピア数e\n",
    ">\n",
    ">詳細は以下のドキュメントを確認してください。\n",
    ">\n",
    ">5.2.3.4. Tf–idf term weighting — scikit-learn 0.21.3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '00', '000', '1', '10', '100', '11', '12', '13', '13th', '14', '15', '16', '17', '18', '1930', '1930s', '1933', '1940', '1950', '1950s', '1960', '1960s', '1968', '1970', '1970s', '1972', '1973', '1980', '1980s', '1983', '1984', '1990', '1996', '1999', '1st', '2', '20', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '20th', '24', '25', '2nd', '3', '30', '35', '3d', '3rd', '4', '40', '45', '5', '50', '50s', '6', '60', '60s', '7', '70', '70s', '8', '80', '80s', '9', '90', '90s', '99', 'a', 'abandoned', 'abc', 'abilities', 'ability', 'able', 'about', 'above', 'abraham', 'absence', 'absolute', 'absolutely', 'absurd', 'abuse', 'abusive', 'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable', 'accepted', 'access', 'accident', 'accidentally', 'accompanied', 'accomplished', 'according', 'account', 'accurate', 'accused', 'achieve', 'achieved', 'achievement', 'acid', 'across', 'act', 'acted', 'acting', 'action', 'actions', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adams', 'adaptation', 'adapted', 'add', 'added', 'adding', 'addition', 'adds', 'adequate', 'admire', 'admit', 'admittedly', 'adorable', 'adult', 'adults', 'advance', 'advanced', 'advantage', 'adventure', 'adventures', 'advertising', 'advice', 'advise', 'affair', 'affect', 'affected', 'afford', 'aforementioned', 'afraid', 'africa', 'african', 'after', 'afternoon', 'afterwards', 'again', 'against', 'age', 'aged', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'agreed', 'agrees', 'ah', 'ahead', 'aid', 'aids', 'aimed', 'ain', 'air', 'aired', 'airplane', 'airport', 'aka', 'akshay', 'al', 'alan', 'alas', 'albeit', 'albert', 'album', 'alcohol', 'alcoholic', 'alert', 'alex', 'alexander', 'alfred', 'alice', 'alien', 'aliens', 'alike', 'alison', 'alive', 'all', 'allen', 'allow', 'allowed', 'allowing', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'alright', 'also', 'alternate', 'although', 'altman', 'altogether', 'always', 'am', 'amanda', 'amateur', 'amateurish', 'amazed', 'amazing', 'amazingly', 'ambitious', 'america', 'american', 'americans', 'amitabh', 'among', 'amongst', 'amount', 'amounts', 'amusing', 'amy', 'an', 'analysis', 'ancient', 'and', 'anderson', 'andrew', 'andrews', 'andy', 'angel', 'angela', 'angeles', 'angels', 'anger', 'angle', 'angles', 'angry', 'animal', 'animals', 'animated', 'animation', 'anime', 'ann', 'anna', 'anne', 'annie', 'annoyed', 'annoying', 'another', 'answer', 'answers', 'anthony', 'anti', 'antics', 'antwone', 'any', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'apartment', 'ape', 'apes', 'appalling', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appearing', 'appears', 'appreciate', 'appreciated', 'appreciation', 'approach', 'appropriate', 'april', 'are', 'area', 'areas', 'aren', 'arguably', 'argue', 'argument', 'arm', 'armed', 'arms', 'army', 'arnold', 'around', 'arrested', 'arrival', 'arrive', 'arrived', 'arrives', 'arrogant', 'art', 'arthur', 'artificial', 'artist', 'artistic', 'artists', 'arts', 'as', 'ashamed', 'ashley', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asleep', 'aspect', 'aspects', 'ass', 'assassin', 'assault', 'assigned', 'assistant', 'associated', 'assume', 'astaire', 'at', 'atlantis', 'atmosphere', 'atmospheric', 'atrocious', 'attached', 'attack', 'attacked', 'attacks', 'attempt', 'attempted', 'attempting', 'attempts', 'attention', 'attitude', 'attitudes', 'attorney', 'attracted', 'attraction', 'attractive', 'audience', 'audiences', 'audio', 'aunt', 'austen', 'australia', 'australian', 'authentic', 'author', 'authority', 'available', 'average', 'avoid', 'avoided', 'awake', 'award', 'awards', 'aware', 'away', 'awe', 'awesome', 'awful', 'awfully', 'awkward', 'b', 'babe', 'baby', 'bacall', 'back', 'backdrop', 'background', 'backgrounds', 'bad', 'badly', 'bag', 'baker', 'bakshi', 'balance', 'ball', 'ballet', 'balls', 'band', 'bands', 'bang', 'bank', 'banned', 'bar', 'barbara', 'bare', 'barely', 'bargain', 'barry', 'barrymore', 'base', 'baseball', 'based', 'basement', 'basic', 'basically', 'basis', 'basketball', 'bat', 'bath', 'bathroom', 'batman', 'battle', 'battles', 'bay', 'bbc', 'be', 'beach', 'bear', 'bears', 'beast', 'beat', 'beaten', 'beating', 'beats', 'beatty', 'beautiful', 'beautifully', 'beauty', 'became', 'because', 'become', 'becomes', 'becoming', 'bed', 'bedroom', 'been', 'beer', 'before', 'began', 'begin', 'beginning', 'begins', 'behave', 'behavior', 'behind', 'being', 'beings', 'bela', 'belief', 'beliefs', 'believable', 'believe', 'believed', 'believes', 'believing', 'bell', 'belong', 'belongs', 'beloved', 'below', 'belushi', 'ben', 'beneath', 'benefit', 'bergman', 'berlin', 'besides', 'best', 'bet', 'bette', 'better', 'bettie', 'betty', 'between', 'beyond', 'bible', 'big', 'bigger', 'biggest', 'biko', 'bill', 'billy', 'bin', 'bird', 'birds', 'birth', 'birthday', 'bit', 'bite', 'bits', 'bitter', 'bizarre', 'black', 'blade', 'blah', 'blair', 'blake', 'blame', 'bland', 'blank', 'blatant', 'bleak', 'blend', 'blew', 'blind', 'blob', 'block', 'blockbuster', 'blond', 'blonde', 'blood', 'bloody', 'blow', 'blowing', 'blown', 'blows', 'blue', 'blues', 'blunt', 'bo', 'board', 'boat', 'bob', 'bobby', 'bodies', 'body', 'bold', 'boll', 'bollywood', 'bomb', 'bond', 'bone', 'bonus', 'book', 'books', 'boom', 'boot', 'border', 'bore', 'bored', 'boredom', 'boring', 'born', 'borrowed', 'boss', 'both', 'bother', 'bothered', 'bottle', 'bottom', 'bought', 'bound', 'bourne', 'box', 'boxing', 'boy', 'boyfriend', 'boys', 'br', 'brad', 'brady', 'brain', 'brains', 'branagh', 'brand', 'brando', 'brave', 'brazil', 'break', 'breaking', 'breaks', 'breasts', 'breath', 'breathtaking', 'brenda', 'brian', 'bride', 'bridge', 'brief', 'briefly', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'bringing', 'brings', 'britain', 'british', 'broad', 'broadcast', 'broadway', 'broke', 'broken', 'brooklyn', 'brooks', 'brosnan', 'brother', 'brothers', 'brought', 'brown', 'bruce', 'brutal', 'brutally', 'buck', 'bucks', 'buddies', 'buddy', 'budget', 'buff', 'buffs', 'bug', 'bugs', 'build', 'building', 'buildings', 'builds', 'built', 'bull', 'bullet', 'bullets', 'bumbling', 'bunch', 'buried', 'burn', 'burned', 'burning', 'burns', 'burt', 'burton', 'bus', 'bush', 'business', 'businessman', 'busy', 'but', 'butler', 'butt', 'button', 'buy', 'buying', 'by', 'c', 'cabin', 'cable', 'cage', 'cagney', 'caine', 'cake', 'caliber', 'california', 'call', 'called', 'calling', 'calls', 'calm', 'came', 'cameo', 'cameos', 'camera', 'cameras', 'cameron', 'camp', 'campbell', 'campy', 'can', 'canada', 'canadian', 'candy', 'cannibal', 'cannot', 'cant', 'capable', 'capital', 'captain', 'captivating', 'capture', 'captured', 'captures', 'capturing', 'car', 'card', 'cardboard', 'cards', 'care', 'cared', 'career', 'careers', 'careful', 'carefully', 'carell', 'cares', 'caring', 'carl', 'carla', 'carol', 'carpenter', 'carradine', 'carrey', 'carrie', 'carried', 'carries', 'carry', 'carrying', 'cars', 'carter', 'cartoon', 'cartoons', 'cary', 'case', 'cases', 'cash', 'cassidy', 'cast', 'casting', 'castle', 'cat', 'catch', 'catches', 'catching', 'catchy', 'category', 'catherine', 'catholic', 'cats', 'caught', 'cause', 'caused', 'causes', 'causing', 'cave', 'cd', 'celebrity', 'cell', 'celluloid', 'center', 'centered', 'centers', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chain', 'chair', 'challenge', 'challenging', 'chan', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chaos', 'chaplin', 'chapter', 'character', 'characterization', 'characters', 'charge', 'charisma', 'charismatic', 'charles', 'charlie', 'charlotte', 'charm', 'charming', 'chase', 'chased', 'chases', 'chasing', 'che', 'cheap', 'cheated', 'cheating', 'check', 'checking', 'cheek', 'cheese', 'cheesy', 'chemistry', 'chess', 'chest', 'chicago', 'chick', 'chicks', 'chief', 'child', 'childhood', 'childish', 'children', 'chilling', 'china', 'chinese', 'choice', 'choices', 'choose', 'chooses', 'choreographed', 'choreography', 'chorus', 'chose', 'chosen', 'chris', 'christ', 'christian', 'christians', 'christmas', 'christopher', 'christy', 'chuck', 'church', 'cia', 'cinderella', 'cinema', 'cinematic', 'cinematographer', 'cinematography', 'circle', 'circumstances', 'cities', 'citizen', 'city', 'civil', 'civilization', 'claim', 'claimed', 'claims', 'claire', 'clark', 'class', 'classes', 'classic', 'classical', 'classics', 'clean', 'clear', 'clearly', 'clever', 'cleverly', 'cliche', 'cliché', 'clichéd', 'clichés', 'cliff', 'climactic', 'climax', 'clint', 'clips', 'clock', 'close', 'closed', 'closely', 'closer', 'closest', 'closet', 'closing', 'clothes', 'clothing', 'clown', 'club', 'clue', 'clues', 'clumsy', 'co', 'coach', 'code', 'coffee', 'coherent', 'cold', 'cole', 'collection', 'college', 'colonel', 'color', 'colorful', 'colors', 'colour', 'columbo', 'com', 'combat', 'combination', 'combine', 'combined', 'come', 'comedian', 'comedic', 'comedies', 'comedy', 'comes', 'comfortable', 'comic', 'comical', 'comics', 'coming', 'command', 'comment', 'commentary', 'commented', 'comments', 'commercial', 'commercials', 'commit', 'committed', 'common', 'communist', 'community', 'companion', 'company', 'compare', 'compared', 'comparing', 'comparison', 'compelled', 'compelling', 'competent', 'competition', 'complain', 'complaint', 'complete', 'completely', 'complex', 'complexity', 'complicated', 'composed', 'computer', 'con', 'conceived', 'concept', 'concern', 'concerned', 'concerning', 'concerns', 'concert', 'conclusion', 'condition', 'confidence', 'conflict', 'conflicts', 'confused', 'confusing', 'confusion', 'connect', 'connected', 'connection', 'connery', 'consequences', 'conservative', 'consider', 'considerable', 'considered', 'considering', 'consistent', 'consistently', 'consists', 'conspiracy', 'constant', 'constantly', 'constructed', 'construction', 'contact', 'contain', 'contained', 'contains', 'contemporary', 'content', 'contest', 'context', 'continue', 'continued', 'continues', 'continuity', 'contract', 'contrary', 'contrast', 'contrived', 'control', 'controversial', 'conventional', 'conversation', 'conversations', 'convey', 'convince', 'convinced', 'convincing', 'convincingly', 'convoluted', 'cook', 'cool', 'cooper', 'cop', 'copies', 'cops', 'copy', 'core', 'corner', 'corny', 'corporate', 'corpse', 'correct', 'corrupt', 'corruption', 'cost', 'costs', 'costume', 'costumes', 'could', 'couldn', 'count', 'counter', 'countless', 'countries', 'country', 'countryside', 'couple', 'couples', 'courage', 'course', 'court', 'cousin', 'cover', 'covered', 'covers', 'cowboy', 'cox', 'crack', 'craft', 'crafted', 'craig', 'crap', 'crappy', 'crash', 'craven', 'crazy', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credibility', 'credible', 'credit', 'credits', 'creep', 'creepy', 'crew', 'cried', 'crime', 'crimes', 'criminal', 'criminals', 'cringe', 'crisis', 'critic', 'critical', 'criticism', 'critics', 'crocodile', 'cross', 'crowd', 'crucial', 'crude', 'cruel', 'cruise', 'crush', 'cry', 'crying', 'crystal', 'cuba', 'cube', 'cult', 'cultural', 'culture', 'cup', 'cure', 'curiosity', 'curious', 'current', 'currently', 'curse', 'curtis', 'cusack', 'cut', 'cute', 'cuts', 'cutting', 'cynical', 'd', 'dad', 'daddy', 'daily', 'dalton', 'damage', 'damme', 'damn', 'damon', 'dan', 'dana', 'dance', 'dancer', 'dancers', 'dances', 'dancing', 'danes', 'danger', 'dangerous', 'daniel', 'danny', 'dare', 'daring', 'dark', 'darker', 'darkness', 'date', 'dated', 'dating', 'daughter', 'daughters', 'dave', 'david', 'davies', 'davis', 'dawn', 'dawson', 'day', 'days', 'de', 'dead', 'deadly', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dear', 'death', 'deaths', 'debut', 'decade', 'decades', 'decent', 'decide', 'decided', 'decides', 'decision', 'decisions', 'dedicated', 'deep', 'deeper', 'deeply', 'defeat', 'defend', 'defense', 'defined', 'definite', 'definitely', 'degree', 'del', 'deliberately', 'delight', 'delightful', 'deliver', 'delivered', 'delivering', 'delivers', 'delivery', 'demand', 'demands', 'demented', 'demon', 'demons', 'dennis', 'dentist', 'denzel', 'department', 'depicted', 'depiction', 'depicts', 'depressed', 'depressing', 'depression', 'depth', 'der', 'derek', 'descent', 'describe', 'described', 'describes', 'description', 'desert', 'deserve', 'deserved', 'deserves', 'design', 'designed', 'designs', 'desire', 'desired', 'despair', 'desperate', 'desperately', 'desperation', 'despite', 'destiny', 'destroy', 'destroyed', 'destroying', 'destruction', 'detail', 'detailed', 'details', 'detective', 'determined', 'develop', 'developed', 'developing', 'development', 'develops', 'device', 'devil', 'devoid', 'devoted', 'dialog', 'dialogs', 'dialogue', 'dialogues', 'diamond', 'diana', 'diane', 'dick', 'dickens', 'did', 'didn', 'die', 'died', 'dies', 'difference', 'differences', 'different', 'difficult', 'dig', 'digital', 'dignity', 'dimensional', 'dinner', 'dinosaur', 'dinosaurs', 'dire', 'direct', 'directed', 'directing', 'direction', 'directly', 'director', 'directorial', 'directors', 'directs', 'dirty', 'disagree', 'disappear', 'disappeared', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'disbelief', 'disc', 'discover', 'discovered', 'discovers', 'discovery', 'discuss', 'discussion', 'disease', 'disgusting', 'disjointed', 'dislike', 'disney', 'display', 'displayed', 'displays', 'distance', 'distant', 'distinct', 'distracting', 'disturbed', 'disturbing', 'divorce', 'dixon', 'do', 'doc', 'doctor', 'documentaries', 'documentary', 'does', 'doesn', 'dog', 'dogs', 'doing', 'doll', 'dollar', 'dollars', 'dolls', 'dolph', 'domestic', 'domino', 'don', 'donald', 'done', 'donna', 'doo', 'doom', 'doomed', 'door', 'doors', 'dorothy', 'double', 'doubt', 'douglas', 'down', 'downhill', 'downright', 'dozen', 'dozens', 'dr', 'dracula', 'drag', 'dragged', 'dragon', 'drags', 'drake', 'drama', 'dramas', 'dramatic', 'draw', 'drawing', 'drawn', 'draws', 'dreadful', 'dream', 'dreams', 'dreary', 'dress', 'dressed', 'dressing', 'drew', 'drink', 'drinking', 'drive', 'drivel', 'driven', 'driver', 'drives', 'driving', 'drop', 'dropped', 'dropping', 'drops', 'drug', 'drugs', 'drunk', 'drunken', 'dry', 'dub', 'dubbed', 'dubbing', 'dud', 'dude', 'due', 'duke', 'dull', 'dumb', 'duo', 'during', 'dust', 'dutch', 'duty', 'dvd', 'dying', 'dynamic', 'e', 'each', 'eager', 'ear', 'earl', 'earlier', 'early', 'earned', 'ears', 'earth', 'ease', 'easier', 'easily', 'east', 'eastern', 'eastwood', 'easy', 'eat', 'eaten', 'eating', 'eccentric', 'ed', 'eddie', 'edgar', 'edge', 'edie', 'edited', 'editing', 'edition', 'editor', 'education', 'edward', 'eerie', 'effect', 'effective', 'effectively', 'effects', 'effort', 'efforts', 'ego', 'eight', 'eighties', 'either', 'elaborate', 'elderly', 'elegant', 'element', 'elements', 'elephant', 'elizabeth', 'ellen', 'elm', 'else', 'elsewhere', 'elvira', 'elvis', 'em', 'embarrassed', 'embarrassing', 'embarrassment', 'emily', 'emma', 'emotion', 'emotional', 'emotionally', 'emotions', 'empathy', 'emperor', 'emphasis', 'empire', 'empty', 'encounter', 'encounters', 'end', 'endearing', 'ended', 'ending', 'endings', 'endless', 'ends', 'endure', 'enemies', 'enemy', 'energy', 'engage', 'engaged', 'engaging', 'england', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enjoying', 'enjoyment', 'enjoys', 'enormous', 'enough', 'ensemble', 'ensues', 'enter', 'enterprise', 'enters', 'entertain', 'entertained', 'entertaining', 'entertainment', 'enthusiasm', 'entire', 'entirely', 'entry', 'environment', 'epic', 'episode', 'episodes', 'equal', 'equally', 'equipment', 'equivalent', 'era', 'eric', 'erotic', 'errors', 'escape', 'escaped', 'escapes', 'especially', 'essence', 'essential', 'essentially', 'established', 'estate', 'et', 'etc', 'ethan', 'eugene', 'europe', 'european', 'eva', 'eve', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'evidence', 'evident', 'evil', 'ex', 'exact', 'exactly', 'exaggerated', 'example', 'examples', 'excellent', 'except', 'exception', 'exceptional', 'exceptionally', 'excessive', 'excited', 'excitement', 'exciting', 'excuse', 'executed', 'execution', 'executive', 'exercise', 'exist', 'existed', 'existence', 'existent', 'exists', 'exotic', 'expect', 'expectations', 'expected', 'expecting', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'exploitation', 'exploration', 'explore', 'explored', 'explosion', 'explosions', 'exposed', 'express', 'expressed', 'expression', 'expressions', 'extended', 'extent', 'extra', 'extraordinary', 'extras', 'extreme', 'extremely', 'eye', 'eyed', 'eyes', 'eyre', 'f', 'fabulous', 'face', 'faced', 'faces', 'facial', 'facing', 'fact', 'factor', 'factory', 'facts', 'fail', 'failed', 'failing', 'fails', 'failure', 'fair', 'fairly', 'fairy', 'faith', 'faithful', 'fake', 'falk', 'fall', 'fallen', 'falling', 'falls', 'false', 'fame', 'familiar', 'families', 'family', 'famous', 'fan', 'fancy', 'fans', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'farm', 'farrell', 'fascinated', 'fascinating', 'fashion', 'fashioned', 'fast', 'faster', 'fat', 'fatal', 'fate', 'father', 'fault', 'faults', 'favor', 'favorite', 'favorites', 'favourite', 'fay', 'fbi', 'fear', 'fears', 'feature', 'featured', 'features', 'featuring', 'feed', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'felix', 'fell', 'fellow', 'felt', 'female', 'feminist', 'fest', 'festival', 'fetched', 'fever', 'few', 'fi', 'fiancé', 'fiction', 'fictional', 'fido', 'field', 'fifteen', 'fight', 'fighter', 'fighting', 'fights', 'figure', 'figured', 'figures', 'fill', 'filled', 'film', 'filmed', 'filming', 'filmmaker', 'filmmakers', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finding', 'finds', 'fine', 'finest', 'finger', 'finish', 'finished', 'fire', 'fired', 'first', 'firstly', 'fish', 'fisher', 'fit', 'fits', 'fitting', 'five', 'fix', 'flash', 'flashback', 'flashbacks', 'flat', 'flaw', 'flawed', 'flawless', 'flaws', 'flesh', 'flick', 'flicks', 'flies', 'flight', 'floating', 'floor', 'flop', 'florida', 'flow', 'fly', 'flying', 'flynn', 'focus', 'focused', 'focuses', 'focusing', 'folk', 'folks', 'follow', 'followed', 'following', 'follows', 'fond', 'fonda', 'food', 'fool', 'fooled', 'foot', 'footage', 'football', 'for', 'forbidden', 'force', 'forced', 'forces', 'ford', 'foreign', 'forest', 'forever', 'forget', 'forgettable', 'forgive', 'forgot', 'forgotten', 'form', 'format', 'former', 'forms', 'formula', 'formulaic', 'forth', 'fortunately', 'fortune', 'forty', 'forward', 'foster', 'foul', 'found', 'four', 'fourth', 'fox', 'frame', 'france', 'franchise', 'francis', 'francisco', 'franco', 'frank', 'frankenstein', 'frankie', 'frankly', 'freak', 'fred', 'freddy', 'free', 'freedom', 'freeman', 'french', 'frequent', 'frequently', 'fresh', 'friday', 'friend', 'friendly', 'friends', 'friendship', 'frightening', 'from', 'front', 'frustrated', 'frustration', 'fu', 'fulci', 'full', 'fully', 'fun', 'funeral', 'funnier', 'funniest', 'funny', 'further', 'furthermore', 'fury', 'future', 'futuristic', 'fx', 'g', 'gabriel', 'gadget', 'gag', 'gags', 'gain', 'game', 'games', 'gandhi', 'gang', 'gangster', 'gangsters', 'garbage', 'garbo', 'garden', 'gary', 'gas', 'gave', 'gay', 'gem', 'gender', 'gene', 'general', 'generally', 'generated', 'generation', 'generic', 'generous', 'genius', 'genre', 'genres', 'gentle', 'gentleman', 'genuine', 'genuinely', 'george', 'gerard', 'german', 'germans', 'germany', 'get', 'gets', 'getting', 'ghost', 'ghosts', 'giallo', 'giant', 'gift', 'gifted', 'ginger', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'glad', 'glass', 'glenn', 'glimpse', 'global', 'glorious', 'glory', 'glover', 'go', 'goal', 'god', 'godfather', 'godzilla', 'goes', 'going', 'gold', 'goldberg', 'golden', 'gone', 'gonna', 'good', 'goodness', 'goofy', 'gordon', 'gore', 'gorgeous', 'gory', 'got', 'gothic', 'gotta', 'gotten', 'government', 'grab', 'grace', 'grade', 'gradually', 'graham', 'grand', 'grandfather', 'grandmother', 'grant', 'granted', 'graphic', 'graphics', 'grasp', 'gratuitous', 'grave', 'gray', 'grayson', 'great', 'greater', 'greatest', 'greatly', 'greed', 'greedy', 'greek', 'green', 'grew', 'grey', 'griffith', 'grim', 'grinch', 'gripping', 'gritty', 'gross', 'ground', 'group', 'groups', 'grow', 'growing', 'grown', 'grows', 'gruesome', 'guarantee', 'guard', 'guess', 'guessed', 'guessing', 'guest', 'guide', 'guilt', 'guilty', 'gun', 'gundam', 'guns', 'guts', 'guy', 'guys', 'h', 'ha', 'had', 'hadn', 'hair', 'hal', 'half', 'halfway', 'hall', 'halloween', 'ham', 'hamilton', 'hamlet', 'hammer', 'hand', 'handed', 'handful', 'handle', 'handled', 'hands', 'handsome', 'hang', 'hanging', 'hank', 'hanks', 'happen', 'happened', 'happening', 'happens', 'happily', 'happiness', 'happy', 'hard', 'hardcore', 'harder', 'hardly', 'hardy', 'harris', 'harry', 'harsh', 'hart', 'hartley', 'harvey', 'has', 'hasn', 'hat', 'hate', 'hated', 'hates', 'hatred', 'haunted', 'haunting', 'have', 'haven', 'having', 'hbo', 'he', 'head', 'headed', 'heads', 'health', 'hear', 'heard', 'hearing', 'heart', 'hearted', 'hearts', 'heat', 'heaven', 'heavily', 'heavy', 'heck', 'heights', 'held', 'helen', 'helicopter', 'hell', 'hello', 'help', 'helped', 'helping', 'helps', 'hence', 'henry', 'her', 'here', 'hero', 'heroes', 'heroic', 'heroine', 'herself', 'heston', 'hey', 'hidden', 'hide', 'hideous', 'hiding', 'high', 'higher', 'highest', 'highlight', 'highlights', 'highly', 'hilarious', 'hilariously', 'hill', 'hills', 'him', 'himself', 'hint', 'hints', 'hip', 'hippie', 'hire', 'hired', 'his', 'historical', 'historically', 'history', 'hit', 'hitchcock', 'hitler', 'hits', 'hitting', 'ho', 'hoffman', 'hold', 'holding', 'holds', 'hole', 'holes', 'holiday', 'hollow', 'holly', 'hollywood', 'holmes', 'holy', 'homage', 'home', 'homeless', 'homer', 'homosexual', 'honest', 'honestly', 'honesty', 'hong', 'honor', 'hood', 'hook', 'hooked', 'hop', 'hope', 'hoped', 'hopefully', 'hopes', 'hoping', 'hopper', 'horrendous', 'horrible', 'horribly', 'horrid', 'horrific', 'horrifying', 'horror', 'horrors', 'horse', 'horses', 'hospital', 'host', 'hot', 'hotel', 'hour', 'hours', 'house', 'household', 'houses', 'how', 'howard', 'however', 'hudson', 'huge', 'hugh', 'huh', 'human', 'humanity', 'humans', 'humble', 'humor', 'humorous', 'humour', 'hundred', 'hundreds', 'hung', 'hunt', 'hunter', 'hunters', 'hunting', 'hurt', 'hurts', 'husband', 'hyde', 'hype', 'hysterical', 'i', 'ian', 'ice', 'icon', 'idea', 'ideal', 'ideas', 'identify', 'identity', 'idiot', 'idiotic', 'idiots', 'if', 'ignorant', 'ignore', 'ignored', 'ii', 'iii', 'ill', 'illegal', 'illness', 'illogical', 'im', 'image', 'imagery', 'images', 'imagination', 'imaginative', 'imagine', 'imagined', 'imdb', 'imitation', 'immediately', 'immensely', 'impact', 'implausible', 'importance', 'important', 'importantly', 'impossible', 'impress', 'impressed', 'impression', 'impressive', 'improve', 'improved', 'improvement', 'in', 'inane', 'inappropriate', 'incident', 'include', 'included', 'includes', 'including', 'incoherent', 'incompetent', 'incomprehensible', 'increasingly', 'incredible', 'incredibly', 'indeed', 'independent', 'india', 'indian', 'indians', 'indie', 'individual', 'individuals', 'industry', 'inept', 'inevitable', 'infamous', 'inferior', 'influence', 'influenced', 'information', 'ingredients', 'initial', 'initially', 'inner', 'innocence', 'innocent', 'innovative', 'insane', 'inside', 'insight', 'inspector', 'inspiration', 'inspired', 'inspiring', 'installment', 'instance', 'instant', 'instantly', 'instead', 'instinct', 'insult', 'insulting', 'intellectual', 'intelligence', 'intelligent', 'intended', 'intense', 'intensity', 'intent', 'intention', 'intentionally', 'intentions', 'interaction', 'interest', 'interested', 'interesting', 'international', 'internet', 'interpretation', 'interview', 'interviews', 'intimate', 'into', 'intrigue', 'intrigued', 'intriguing', 'introduce', 'introduced', 'introduces', 'introduction', 'invasion', 'inventive', 'investigate', 'investigation', 'invisible', 'involve', 'involved', 'involvement', 'involves', 'involving', 'iran', 'iraq', 'ireland', 'irish', 'iron', 'ironic', 'ironically', 'irony', 'irrelevant', 'irritating', 'island', 'isn', 'isolated', 'israel', 'issue', 'issues', 'it', 'italian', 'italy', 'its', 'itself', 'j', 'jack', 'jackie', 'jackson', 'jail', 'jake', 'james', 'jamie', 'jane', 'japan', 'japanese', 'jason', 'jaw', 'jaws', 'jay', 'jazz', 'jealous', 'jean', 'jeff', 'jeffrey', 'jennifer', 'jenny', 'jeremy', 'jerk', 'jerry', 'jesse', 'jessica', 'jesus', 'jet', 'jewish', 'jim', 'jimmy', 'joan', 'job', 'jobs', 'joe', 'joey', 'john', 'johnny', 'johnson', 'join', 'joined', 'joke', 'jokes', 'jon', 'jonathan', 'jones', 'joseph', 'journalist', 'journey', 'joy', 'jr', 'judge', 'judging', 'judy', 'julia', 'julian', 'julie', 'jump', 'jumped', 'jumping', 'jumps', 'june', 'jungle', 'junior', 'junk', 'just', 'justice', 'justify', 'justin', 'juvenile', 'k', 'kane', 'kapoor', 'karen', 'karloff', 'kate', 'kay', 'keaton', 'keep', 'keeping', 'keeps', 'keith', 'kelly', 'ken', 'kennedy', 'kenneth', 'kept', 'kevin', 'key', 'khan', 'kick', 'kicked', 'kicking', 'kicks', 'kid', 'kidding', 'kidnapped', 'kids', 'kill', 'killed', 'killer', 'killers', 'killing', 'killings', 'kills', 'kim', 'kind', 'kinda', 'kinds', 'king', 'kingdom', 'kirk', 'kiss', 'kitchen', 'knew', 'knife', 'knock', 'know', 'knowing', 'knowledge', 'known', 'knows', 'kong', 'korean', 'kubrick', 'kudos', 'kung', 'kurt', 'kyle', 'l', 'la', 'lab', 'lack', 'lacked', 'lacking', 'lacks', 'ladies', 'lady', 'laid', 'lake', 'lame', 'land', 'landscape', 'landscapes', 'lane', 'language', 'large', 'largely', 'larger', 'larry', 'last', 'lasted', 'late', 'lately', 'later', 'latest', 'latin', 'latter', 'laugh', 'laughable', 'laughed', 'laughing', 'laughs', 'laughter', 'laura', 'laurel', 'law', 'lawrence', 'laws', 'lawyer', 'lay', 'lazy', 'le', 'lead', 'leader', 'leading', 'leads', 'league', 'learn', 'learned', 'learning', 'learns', 'least', 'leave', 'leaves', 'leaving', 'led', 'lee', 'left', 'leg', 'legal', 'legend', 'legendary', 'legs', 'lemmon', 'lena', 'length', 'lengthy', 'leo', 'leonard', 'lesbian', 'leslie', 'less', 'lesser', 'lesson', 'lessons', 'let', 'lets', 'letter', 'letters', 'letting', 'level', 'levels', 'lewis', 'li', 'liberal', 'library', 'lie', 'lies', 'life', 'lifestyle', 'lifetime', 'light', 'lighting', 'lights', 'likable', 'like', 'liked', 'likely', 'likes', 'likewise', 'liking', 'lily', 'limited', 'limits', 'lincoln', 'linda', 'line', 'liners', 'lines', 'link', 'lion', 'lips', 'lisa', 'list', 'listed', 'listen', 'listening', 'lit', 'literally', 'literature', 'little', 'live', 'lived', 'lively', 'lives', 'living', 'll', 'lloyd', 'load', 'loaded', 'loads', 'local', 'location', 'locations', 'locked', 'logic', 'logical', 'lol', 'london', 'lone', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loose', 'loosely', 'lord', 'los', 'lose', 'loser', 'losers', 'loses', 'losing', 'loss', 'lost', 'lot', 'lots', 'lou', 'loud', 'louis', 'louise', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lower', 'lowest', 'loyal', 'lucas', 'luck', 'luckily', 'lucky', 'lucy', 'ludicrous', 'lugosi', 'luke', 'lumet', 'lundgren', 'lust', 'lying', 'lynch', 'lyrics', 'm', 'macarthur', 'machine', 'machines', 'macy', 'mad', 'made', 'madness', 'madonna', 'mafia', 'magazine', 'maggie', 'magic', 'magical', 'magnificent', 'maid', 'mail', 'main', 'mainly', 'mainstream', 'maintain', 'major', 'majority', 'make', 'maker', 'makers', 'makes', 'makeup', 'making', 'male', 'mall', 'man', 'manage', 'managed', 'manager', 'manages', 'manhattan', 'maniac', 'manipulative', 'mankind', 'mann', 'manner', 'mansion', 'many', 'map', 'march', 'margaret', 'maria', 'marie', 'mario', 'marion', 'mark', 'market', 'marketing', 'marks', 'marriage', 'married', 'marry', 'mars', 'martial', 'martin', 'marty', 'marvelous', 'mary', 'mask', 'mass', 'massacre', 'massive', 'master', 'masterful', 'masterpiece', 'masterpieces', 'masters', 'match', 'matched', 'matches', 'mate', 'material', 'matrix', 'matt', 'matter', 'matters', 'matthau', 'matthew', 'mature', 'max', 'may', 'maybe', 'mayor', 'me', 'mean', 'meaning', 'meaningful', 'meaningless', 'means', 'meant', 'meanwhile', 'measure', 'meat', 'mechanical', 'media', 'medical', 'mediocre', 'medium', 'meet', 'meeting', 'meets', 'mel', 'melodrama', 'melodramatic', 'melting', 'member', 'members', 'memorable', 'memories', 'memory', 'men', 'menace', 'menacing', 'mental', 'mentally', 'mention', 'mentioned', 'mentioning', 'mentions', 'mere', 'merely', 'merit', 'meryl', 'mess', 'message', 'messages', 'messed', 'met', 'metal', 'method', 'methods', 'mexican', 'mexico', 'mgm', 'michael', 'michelle', 'mickey', 'mid', 'middle', 'midnight', 'might', 'mighty', 'miike', 'mike', 'mild', 'mildly', 'mildred', 'mile', 'miles', 'military', 'mill', 'miller', 'million', 'millions', 'mind', 'minded', 'mindless', 'minds', 'mine', 'mini', 'minimal', 'minimum', 'minor', 'minute', 'minutes', 'miracle', 'mirror', 'miscast', 'miserable', 'miserably', 'misery', 'miss', 'missed', 'misses', 'missing', 'mission', 'mistake', 'mistaken', 'mistakes', 'mistress', 'mitchell', 'mix', 'mixed', 'mixture', 'miyazaki', 'mob', 'model', 'models', 'modern', 'modesty', 'molly', 'mom', 'moment', 'moments', 'money', 'monk', 'monkey', 'monkeys', 'monster', 'monsters', 'montage', 'montana', 'month', 'months', 'mood', 'moody', 'moon', 'moore', 'moral', 'morality', 'more', 'morgan', 'morning', 'moronic', 'morris', 'most', 'mostly', 'mother', 'motion', 'motivation', 'motivations', 'motives', 'mountain', 'mountains', 'mouse', 'mouth', 'move', 'moved', 'movement', 'movements', 'moves', 'movie', 'movies', 'moving', 'mr', 'mrs', 'ms', 'mst3k', 'mtv', 'much', 'multi', 'multiple', 'mummy', 'mundane', 'murder', 'murdered', 'murderer', 'murderous', 'murders', 'murphy', 'museum', 'music', 'musical', 'musicals', 'muslim', 'must', 'my', 'myers', 'myself', 'mysteries', 'mysterious', 'mystery', 'n', 'nail', 'naive', 'naked', 'name', 'named', 'namely', 'names', 'nancy', 'narration', 'narrative', 'narrator', 'nasty', 'nation', 'national', 'native', 'natural', 'naturally', 'nature', 'navy', 'nazi', 'nazis', 'near', 'nearby', 'nearly', 'neat', 'necessarily', 'necessary', 'neck', 'ned', 'need', 'needed', 'needless', 'needs', 'negative', 'neighbor', 'neighborhood', 'neighbors', 'neil', 'neither', 'nelson', 'neo', 'nephew', 'nerd', 'nervous', 'network', 'never', 'nevertheless', 'new', 'newly', 'news', 'newspaper', 'next', 'nice', 'nicely', 'nicholas', 'nicholson', 'nick', 'night', 'nightmare', 'nightmares', 'nights', 'nine', 'ninja', 'niro', 'no', 'noble', 'nobody', 'noir', 'noise', 'nominated', 'nomination', 'non', 'none', 'nonetheless', 'nonsense', 'nor', 'normal', 'normally', 'norman', 'north', 'nose', 'nostalgia', 'nostalgic', 'not', 'notable', 'notably', 'notch', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'notion', 'notorious', 'novak', 'novel', 'novels', 'now', 'nowadays', 'nowhere', 'nuclear', 'nude', 'nudity', 'number', 'numbers', 'numerous', 'nurse', 'nuts', 'nyc', 'o', 'object', 'obnoxious', 'obscure', 'obsessed', 'obsession', 'obvious', 'obviously', 'occasion', 'occasional', 'occasionally', 'occur', 'occurred', 'occurs', 'ocean', 'odd', 'oddly', 'odds', 'of', 'off', 'offended', 'offensive', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'officers', 'official', 'often', 'oh', 'oil', 'ok', 'okay', 'old', 'older', 'oliver', 'olivier', 'ollie', 'omen', 'on', 'once', 'one', 'ones', 'online', 'only', 'onto', 'open', 'opened', 'opening', 'opens', 'opera', 'operation', 'opinion', 'opinions', 'opportunities', 'opportunity', 'opposed', 'opposite', 'or', 'orange', 'order', 'orders', 'ordinary', 'original', 'originality', 'originally', 'orleans', 'orson', 'oscar', 'oscars', 'othello', 'other', 'others', 'otherwise', 'ought', 'our', 'ourselves', 'out', 'outcome', 'outer', 'outfit', 'outrageous', 'outside', 'outstanding', 'over', 'overacting', 'overall', 'overcome', 'overdone', 'overlook', 'overlooked', 'overly', 'overrated', 'overwhelming', 'owen', 'own', 'owner', 'oz', 'p', 'pace', 'paced', 'pacing', 'pacino', 'pack', 'packed', 'page', 'paid', 'pain', 'painful', 'painfully', 'paint', 'painted', 'painting', 'pair', 'pal', 'palace', 'palma', 'paltrow', 'pamela', 'pan', 'panic', 'pants', 'paper', 'par', 'parallel', 'paranoia', 'parent', 'parents', 'paris', 'park', 'parker', 'parody', 'part', 'particular', 'particularly', 'parties', 'partly', 'partner', 'parts', 'party', 'pass', 'passed', 'passes', 'passing', 'passion', 'passionate', 'past', 'pat', 'path', 'pathetic', 'patient', 'patients', 'patrick', 'paul', 'paulie', 'pay', 'paying', 'pays', 'peace', 'pearl', 'people', 'peoples', 'per', 'perfect', 'perfection', 'perfectly', 'perform', 'performance', 'performances', 'performed', 'performer', 'performers', 'performing', 'performs', 'perhaps', 'period', 'perry', 'person', 'persona', 'personal', 'personalities', 'personality', 'personally', 'persons', 'perspective', 'pet', 'peter', 'pg', 'phantom', 'philip', 'philosophical', 'philosophy', 'phone', 'phony', 'photo', 'photographed', 'photographer', 'photography', 'photos', 'physical', 'physically', 'piano', 'pick', 'picked', 'picking', 'picks', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pig', 'pile', 'pilot', 'pink', 'pit', 'pitch', 'pitt', 'pity', 'place', 'placed', 'places', 'plague', 'plain', 'plan', 'plane', 'planet', 'planned', 'planning', 'plans', 'plant', 'plastic', 'plausible', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasant', 'pleasantly', 'please', 'pleased', 'pleasure', 'plenty', 'plight', 'plot', 'plots', 'plus', 'poem', 'poetic', 'poetry', 'poignant', 'point', 'pointed', 'pointless', 'points', 'pokemon', 'polanski', 'police', 'polished', 'political', 'politically', 'politics', 'pool', 'poor', 'poorly', 'pop', 'popcorn', 'pops', 'popular', 'popularity', 'population', 'porn', 'porno', 'portion', 'portrait', 'portray', 'portrayal', 'portrayed', 'portraying', 'portrays', 'position', 'positive', 'possessed', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'poster', 'pot', 'potential', 'potentially', 'poverty', 'powell', 'power', 'powerful', 'powers', 'practically', 'practice', 'praise', 'pre', 'precious', 'predictable', 'prefer', 'pregnant', 'premise', 'prepared', 'prequel', 'presence', 'present', 'presentation', 'presented', 'presents', 'president', 'press', 'presumably', 'pretend', 'pretending', 'pretentious', 'pretty', 'prevent', 'preview', 'previous', 'previously', 'price', 'priceless', 'pride', 'priest', 'primarily', 'primary', 'prime', 'prince', 'princess', 'principal', 'print', 'prior', 'prison', 'prisoner', 'prisoners', 'private', 'prize', 'pro', 'probably', 'problem', 'problems', 'proceedings', 'proceeds', 'process', 'produce', 'produced', 'producer', 'producers', 'producing', 'product', 'production', 'productions', 'professional', 'professor', 'profound', 'program', 'progress', 'progresses', 'project', 'projects', 'prom', 'promise', 'promised', 'promises', 'promising', 'proof', 'propaganda', 'proper', 'properly', 'property', 'props', 'prostitute', 'protagonist', 'protagonists', 'protect', 'proud', 'prove', 'proved', 'proves', 'provide', 'provided', 'provides', 'providing', 'provoking', 'pseudo', 'psychiatrist', 'psychic', 'psycho', 'psychological', 'psychotic', 'public', 'pull', 'pulled', 'pulling', 'pulls', 'pulp', 'punch', 'punishment', 'punk', 'purchase', 'purchased', 'pure', 'purely', 'purple', 'purpose', 'purposes', 'pursuit', 'push', 'pushed', 'pushing', 'put', 'puts', 'putting', 'q', 'qualities', 'quality', 'queen', 'quest', 'question', 'questionable', 'questions', 'quick', 'quickly', 'quiet', 'quirky', 'quite', 'quote', 'quotes', 'r', 'rabbit', 'race', 'rachel', 'racial', 'racism', 'racist', 'radio', 'rage', 'rain', 'raise', 'raised', 'raising', 'ralph', 'rambo', 'ran', 'random', 'randomly', 'randy', 'range', 'rangers', 'rank', 'ranks', 'rap', 'rape', 'raped', 'rare', 'rarely', 'rat', 'rate', 'rated', 'rather', 'rating', 'ratings', 'rats', 'raw', 'ray', 'raymond', 're', 'reach', 'reached', 'reaches', 'reaching', 'react', 'reaction', 'reactions', 'read', 'reading', 'reads', 'ready', 'real', 'realise', 'realism', 'realistic', 'reality', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reason', 'reasonable', 'reasonably', 'reasons', 'rebel', 'recall', 'receive', 'received', 'recent', 'recently', 'recognition', 'recognize', 'recognized', 'recommend', 'recommended', 'record', 'recorded', 'recording', 'red', 'redeeming', 'redemption', 'reduced', 'reed', 'reel', 'reference', 'references', 'reflect', 'refreshing', 'refuses', 'regard', 'regarding', 'regardless', 'regret', 'regular', 'reid', 'relate', 'related', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relatives', 'release', 'released', 'relevant', 'relief', 'relies', 'religion', 'religious', 'remain', 'remaining', 'remains', 'remake', 'remarkable', 'remarkably', 'remarks', 'remember', 'remembered', 'remind', 'reminded', 'reminds', 'reminiscent', 'remote', 'remotely', 'removed', 'rendition', 'rent', 'rental', 'rented', 'renting', 'repeat', 'repeated', 'repeatedly', 'repetitive', 'replaced', 'report', 'reporter', 'represent', 'represented', 'represents', 'reputation', 'required', 'requires', 'rescue', 'research', 'resemblance', 'resembles', 'resident', 'resist', 'resolution', 'resort', 'resources', 'respect', 'respected', 'response', 'responsibility', 'responsible', 'rest', 'restaurant', 'restored', 'result', 'results', 'retarded', 'return', 'returned', 'returning', 'returns', 'reunion', 'reveal', 'revealed', 'revealing', 'reveals', 'revelation', 'revenge', 'review', 'reviewer', 'reviewers', 'reviews', 'revolution', 'revolutionary', 'revolves', 'rex', 'reynolds', 'rich', 'richard', 'richards', 'richardson', 'rick', 'rid', 'ridden', 'ride', 'ridiculous', 'ridiculously', 'riding', 'right', 'rights', 'ring', 'rings', 'rip', 'ripped', 'rise', 'rising', 'risk', 'ritter', 'rival', 'river', 'riveting', 'road', 'rob', 'robbery', 'robert', 'roberts', 'robin', 'robinson', 'robot', 'robots', 'rochester', 'rock', 'rocket', 'rocks', 'rocky', 'roger', 'rogers', 'role', 'roles', 'roll', 'rolling', 'roman', 'romance', 'romantic', 'romero', 'ron', 'room', 'rooms', 'rooney', 'root', 'rose', 'roth', 'rotten', 'rough', 'round', 'routine', 'row', 'roy', 'royal', 'rubber', 'rubbish', 'ruby', 'ruin', 'ruined', 'ruins', 'rukh', 'rule', 'rules', 'run', 'running', 'runs', 'rural', 'rush', 'rushed', 'russell', 'russian', 'ruth', 'ruthless', 'ryan', 's', 'sabrina', 'sacrifice', 'sad', 'sadistic', 'sadly', 'sadness', 'safe', 'safety', 'saga', 'said', 'sake', 'sally', 'sam', 'same', 'samurai', 'san', 'sandler', 'sandra', 'santa', 'sappy', 'sarah', 'sat', 'satan', 'satire', 'satisfied', 'satisfy', 'satisfying', 'saturday', 'savage', 'save', 'saved', 'saves', 'saving', 'saw', 'say', 'saying', 'says', 'scale', 'scare', 'scarecrow', 'scared', 'scares', 'scary', 'scenario', 'scene', 'scenery', 'scenes', 'scheme', 'school', 'sci', 'science', 'scientific', 'scientist', 'scientists', 'scooby', 'scope', 'score', 'scores', 'scotland', 'scott', 'scottish', 'scream', 'screaming', 'screams', 'screen', 'screening', 'screenplay', 'screenwriter', 'script', 'scripted', 'scripts', 'scrooge', 'sea', 'seagal', 'sean', 'search', 'searching', 'season', 'seasons', 'seat', 'second', 'secondly', 'seconds', 'secret', 'secretary', 'secretly', 'secrets', 'section', 'security', 'see', 'seed', 'seeing', 'seek', 'seeking', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'sees', 'segment', 'segments', 'self', 'selfish', 'sell', 'sellers', 'selling', 'semi', 'send', 'sends', 'sense', 'senseless', 'sensitive', 'sent', 'sentence', 'sentimental', 'separate', 'september', 'sequel', 'sequels', 'sequence', 'sequences', 'serial', 'series', 'serious', 'seriously', 'serve', 'served', 'serves', 'service', 'serving', 'set', 'sets', 'setting', 'settings', 'settle', 'seven', 'seventies', 'several', 'severe', 'sex', 'sexual', 'sexuality', 'sexually', 'sexy', 'sh', 'shadow', 'shadows', 'shake', 'shakespeare', 'shall', 'shallow', 'shame', 'shape', 'share', 'shark', 'sharp', 'shaw', 'she', 'sheer', 'shelf', 'shelley', 'sheriff', 'shine', 'shines', 'shining', 'ship', 'shirley', 'shirt', 'shock', 'shocked', 'shocking', 'shoes', 'shoot', 'shooting', 'shoots', 'shop', 'short', 'shortly', 'shorts', 'shot', 'shots', 'should', 'shouldn', 'show', 'showcase', 'showdown', 'showed', 'shower', 'showing', 'shown', 'shows', 'shut', 'shy', 'sick', 'side', 'sidekick', 'sides', 'sidney', 'sight', 'sign', 'signed', 'significant', 'signs', 'silence', 'silent', 'silly', 'silver', 'similar', 'similarities', 'similarly', 'simmons', 'simon', 'simple', 'simplicity', 'simplistic', 'simply', 'simpson', 'sin', 'sinatra', 'since', 'sincere', 'sing', 'singer', 'singing', 'single', 'sings', 'sinister', 'sink', 'sir', 'sirk', 'sister', 'sisters', 'sit', 'sitcom', 'site', 'sits', 'sitting', 'situation', 'situations', 'six', 'size', 'skill', 'skills', 'skin', 'skip', 'sky', 'slap', 'slapstick', 'slasher', 'slaughter', 'slave', 'sleazy', 'sleep', 'sleeping', 'slick', 'slight', 'slightest', 'slightly', 'sloppy', 'slow', 'slowly', 'small', 'smaller', 'smart', 'smile', 'smiling', 'smith', 'smoke', 'smoking', 'smooth', 'snake', 'sneak', 'snl', 'snow', 'so', 'soap', 'soccer', 'social', 'society', 'soft', 'sold', 'soldier', 'soldiers', 'sole', 'solely', 'solid', 'solo', 'solution', 'solve', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'sons', 'soon', 'sophisticated', 'sorry', 'sort', 'sorts', 'soul', 'souls', 'sound', 'sounded', 'sounding', 'sounds', 'soundtrack', 'source', 'south', 'southern', 'soviet', 'space', 'spain', 'spanish', 'spare', 'speak', 'speaking', 'speaks', 'special', 'specially', 'species', 'specific', 'specifically', 'spectacular', 'speech', 'speed', 'spell', 'spend', 'spending', 'spends', 'spent', 'spider', 'spielberg', 'spike', 'spin', 'spirit', 'spirited', 'spirits', 'spiritual', 'spite', 'splatter', 'splendid', 'split', 'spock', 'spoil', 'spoiled', 'spoiler', 'spoilers', 'spoke', 'spoken', 'spoof', 'spooky', 'sport', 'sports', 'spot', 'spots', 'spread', 'spring', 'spy', 'square', 'st', 'staff', 'stage', 'staged', 'stale', 'stallone', 'stan', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanley', 'stanwyck', 'star', 'staring', 'starred', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'station', 'status', 'stay', 'stayed', 'staying', 'stays', 'steal', 'stealing', 'steals', 'steel', 'stellar', 'step', 'stephen', 'steps', 'stereotype', 'stereotypes', 'stereotypical', 'steve', 'steven', 'stevens', 'stewart', 'stick', 'sticks', 'stiff', 'still', 'stiller', 'stilted', 'stinker', 'stinks', 'stock', 'stole', 'stolen', 'stomach', 'stone', 'stood', 'stooges', 'stop', 'stopped', 'stops', 'store', 'stories', 'storm', 'story', 'storyline', 'storytelling', 'straight', 'strange', 'strangely', 'stranger', 'streep', 'street', 'streets', 'streisand', 'strength', 'stress', 'stretch', 'strictly', 'strike', 'strikes', 'striking', 'string', 'strip', 'strong', 'stronger', 'strongly', 'struck', 'structure', 'struggle', 'struggles', 'struggling', 'stuck', 'student', 'students', 'studio', 'studios', 'study', 'stuff', 'stunning', 'stunt', 'stunts', 'stupid', 'stupidity', 'style', 'styles', 'stylish', 'sub', 'subject', 'subjects', 'subplot', 'subplots', 'subsequent', 'substance', 'subtitles', 'subtle', 'subtlety', 'succeed', 'succeeded', 'succeeds', 'success', 'successful', 'successfully', 'such', 'suck', 'sucked', 'sucks', 'sudden', 'suddenly', 'sue', 'suffer', 'suffered', 'suffering', 'suffers', 'suffice', 'suggest', 'suggests', 'suicide', 'suit', 'suitable', 'suited', 'suits', 'sullivan', 'sum', 'summary', 'summer', 'sun', 'sunday', 'sunshine', 'super', 'superb', 'superbly', 'superficial', 'superhero', 'superior', 'superman', 'supernatural', 'support', 'supporting', 'suppose', 'supposed', 'supposedly', 'sure', 'surely', 'surface', 'surfing', 'surprise', 'surprised', 'surprises', 'surprising', 'surprisingly', 'surreal', 'surrounded', 'surrounding', 'survival', 'survive', 'survived', 'surviving', 'survivor', 'survivors', 'susan', 'suspect', 'suspects', 'suspend', 'suspense', 'suspenseful', 'suspicious', 'sutherland', 'swear', 'swedish', 'sweet', 'swimming', 'switch', 'sword', 'symbolism', 'sympathetic', 'sympathy', 'synopsis', 'system', 't', 'table', 'tad', 'tag', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talents', 'tales', 'talk', 'talked', 'talking', 'talks', 'tall', 'tame', 'tap', 'tape', 'target', 'tarzan', 'task', 'taste', 'taught', 'taxi', 'taylor', 'tea', 'teach', 'teacher', 'team', 'tear', 'tears', 'technical', 'technically', 'technique', 'techniques', 'technology', 'ted', 'tedious', 'teen', 'teenage', 'teenager', 'teenagers', 'teens', 'teeth', 'television', 'tell', 'telling', 'tells', 'temple', 'ten', 'tend', 'tender', 'tends', 'tense', 'tension', 'term', 'terms', 'terrible', 'terribly', 'terrific', 'terrifying', 'territory', 'terror', 'terrorist', 'terrorists', 'terry', 'test', 'texas', 'text', 'than', 'thank', 'thankfully', 'thanks', 'that', 'thats', 'the', 'theater', 'theaters', 'theatre', 'theatrical', 'their', 'them', 'theme', 'themes', 'themselves', 'then', 'theory', 'there', 'therefore', 'these', 'they', 'thick', 'thief', 'thin', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'thirty', 'this', 'thomas', 'thompson', 'thoroughly', 'those', 'though', 'thought', 'thoughtful', 'thoughts', 'thousand', 'thousands', 'threat', 'threatening', 'three', 'threw', 'thrill', 'thriller', 'thrillers', 'thrilling', 'thrills', 'throat', 'through', 'throughout', 'throw', 'throwing', 'thrown', 'throws', 'thru', 'thugs', 'thumbs', 'thus', 'ticket', 'tie', 'tied', 'ties', 'tiger', 'tight', 'till', 'tim', 'time', 'timeless', 'times', 'timing', 'timon', 'timothy', 'tiny', 'tired', 'tiresome', 'titanic', 'title', 'titled', 'titles', 'to', 'today', 'todd', 'together', 'toilet', 'told', 'tom', 'tomatoes', 'tommy', 'tone', 'tongue', 'tonight', 'tons', 'tony', 'too', 'took', 'tooth', 'top', 'topic', 'topless', 'torn', 'torture', 'tortured', 'total', 'totally', 'touch', 'touched', 'touches', 'touching', 'tough', 'tour', 'toward', 'towards', 'town', 'toy', 'toys', 'track', 'tracks', 'tracy', 'trade', 'tradition', 'traditional', 'tragedy', 'tragic', 'trailer', 'trailers', 'train', 'trained', 'training', 'transfer', 'transformation', 'transition', 'translation', 'trap', 'trapped', 'trash', 'trashy', 'travel', 'traveling', 'travels', 'travesty', 'treasure', 'treat', 'treated', 'treatment', 'treats', 'tree', 'trees', 'trek', 'tremendous', 'trial', 'tribe', 'tribute', 'trick', 'tricks', 'tried', 'tries', 'trilogy', 'trio', 'trip', 'trite', 'triumph', 'troops', 'trouble', 'troubled', 'troubles', 'truck', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'tune', 'tunes', 'turkey', 'turn', 'turned', 'turner', 'turning', 'turns', 'tv', 'twelve', 'twenty', 'twice', 'twilight', 'twin', 'twist', 'twisted', 'twists', 'two', 'type', 'types', 'typical', 'typically', 'u', 'ugly', 'uk', 'ultimate', 'ultimately', 'ultra', 'un', 'unable', 'unaware', 'unbearable', 'unbelievable', 'unbelievably', 'uncle', 'uncomfortable', 'unconvincing', 'under', 'underground', 'underrated', 'understand', 'understandable', 'understanding', 'understated', 'understood', 'undoubtedly', 'uneven', 'unexpected', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unhappy', 'uninspired', 'unintentional', 'unintentionally', 'uninteresting', 'union', 'unique', 'unit', 'united', 'universal', 'universe', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unnecessary', 'unpleasant', 'unreal', 'unrealistic', 'unsettling', 'until', 'unusual', 'unwatchable', 'up', 'upon', 'upper', 'ups', 'upset', 'urban', 'urge', 'us', 'usa', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'using', 'ustinov', 'usual', 'usually', 'utter', 'utterly', 'uwe', 'v', 'vacation', 'vague', 'vaguely', 'valuable', 'value', 'values', 'vampire', 'vampires', 'van', 'variety', 'various', 'vast', 've', 'vegas', 'vehicle', 'vengeance', 'verhoeven', 'version', 'versions', 'versus', 'very', 'veteran', 'vhs', 'via', 'vice', 'vicious', 'victim', 'victims', 'victor', 'victoria', 'video', 'videos', 'vietnam', 'view', 'viewed', 'viewer', 'viewers', 'viewing', 'viewings', 'views', 'village', 'villain', 'villains', 'vincent', 'violence', 'violent', 'virgin', 'virginia', 'virtually', 'virus', 'visible', 'vision', 'visit', 'visits', 'visual', 'visually', 'visuals', 'vivid', 'voice', 'voiced', 'voices', 'voight', 'von', 'vote', 'vs', 'vulnerable', 'w', 'wacky', 'wait', 'waited', 'waiting', 'waitress', 'wake', 'walk', 'walked', 'walken', 'walker', 'walking', 'walks', 'wall', 'wallace', 'walls', 'walsh', 'walter', 'wandering', 'wanna', 'wannabe', 'want', 'wanted', 'wanting', 'wants', 'war', 'ward', 'warm', 'warming', 'warmth', 'warn', 'warned', 'warner', 'warning', 'warren', 'warrior', 'wars', 'was', 'washington', 'wasn', 'waste', 'wasted', 'wasting', 'watch', 'watchable', 'watched', 'watches', 'watching', 'water', 'waters', 'watson', 'wave', 'waves', 'way', 'wayne', 'ways', 'we', 'weak', 'weakest', 'wealth', 'wealthy', 'weapon', 'weapons', 'wear', 'wearing', 'wears', 'web', 'website', 'wedding', 'week', 'weekend', 'weeks', 'weight', 'weird', 'welcome', 'well', 'welles', 'wells', 'wendy', 'went', 'were', 'weren', 'werewolf', 'wes', 'west', 'western', 'westerns', 'wet', 'whale', 'what', 'whatever', 'whats', 'whatsoever', 'when', 'whenever', 'where', 'whereas', 'whether', 'which', 'while', 'whilst', 'white', 'who', 'whoever', 'whole', 'whom', 'whoopi', 'whose', 'why', 'wicked', 'wide', 'widely', 'widmark', 'widow', 'wife', 'wild', 'will', 'william', 'williams', 'willie', 'willing', 'willis', 'wilson', 'win', 'wind', 'window', 'winds', 'wing', 'winner', 'winning', 'wins', 'winter', 'winters', 'wisdom', 'wise', 'wish', 'wished', 'wishes', 'wishing', 'wit', 'witch', 'witches', 'with', 'within', 'without', 'witness', 'witnessed', 'witnesses', 'witty', 'wives', 'wizard', 'wolf', 'woman', 'women', 'won', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wondering', 'wonders', 'wont', 'wood', 'wooden', 'woods', 'woody', 'word', 'words', 'wore', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'world', 'worlds', 'worn', 'worried', 'worry', 'worse', 'worst', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wouldn', 'wound', 'wounded', 'wow', 'wrap', 'wrapped', 'wreck', 'wrestling', 'write', 'writer', 'writers', 'writes', 'writing', 'written', 'wrong', 'wrote', 'wwii', 'x', 'y', 'ya', 'yeah', 'year', 'years', 'yelling', 'yellow', 'yes', 'yesterday', 'yet', 'york', 'you', 'young', 'younger', 'your', 'yourself', 'youth', 'z', 'zero', 'zizek', 'zombie', 'zombies', 'zone']\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "X = vectorizer.fit_transform(x_train)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】TF-IDFを用いた学習\n",
    ">問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    ">\n",
    ">ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750, 5000)\n",
      "(18750,)\n",
      "(6250, 5000)\n",
      "(6250,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.copy(y_train)\n",
    "\n",
    "# 学習データとテストデータを７５：２５で分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ロジスティック回帰による予測値\n",
      "[0 0 1 ... 0 0 0]\n",
      "ロジスティック回帰の正解率： 0.88448\n"
     ]
    }
   ],
   "source": [
    "# stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b', max_features = 5000\n",
    "\n",
    "logistic = LogisticRegression(random_state=0)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic.predict(X_val)\n",
    "print(\"ロジスティック回帰による予測値\\n{}\".format(y_pred))\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"ロジスティック回帰の正解率： {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features = 10000　の予測値\n",
      "[0 0 1 ... 0 0 0]\n",
      "max_features = 10000　の正解率： 0.88848\n"
     ]
    }
   ],
   "source": [
    "# max_features = 10000 に変更\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b', max_features = 10000)\n",
    "X = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# 学習データとテストデータを７５：２５で分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "logistic = LogisticRegression(random_state=0)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic.predict(X_val)\n",
    "print(\"max_features = 10000　の予測値\\n{}\".format(y_pred))\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"max_features = 10000　の正解率： {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words の予測値\n",
      "[0 0 1 ... 0 0 0]\n",
      "stop_words の正解率： 0.88416\n"
     ]
    }
   ],
   "source": [
    "# stop_words=[\"is\", \"was\", \"the\", \"this\"] に変更\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=[\"is\", \"was\", \"the\", \"this\"], token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "X = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# 学習データとテストデータを７５：２５で分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "logistic = LogisticRegression(random_state=0)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic.predict(X_val)\n",
    "print(\"stop_words の予測値\\n{}\".format(y_pred))\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"stop_words の正解率： {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range=(2, 4) の予測値\n",
      "[0 0 1 ... 1 0 0]\n",
      "ngram_range=(2, 4) の正解率： 0.84176\n"
     ]
    }
   ],
   "source": [
    "# ngram_range=(2, 4) を追加\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 4), stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "X = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# 学習データとテストデータを７５：２５で分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "logistic = LogisticRegression(random_state=0)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic.predict(X_val)\n",
    "print(\"ngram_range=(2, 4) の予測値\\n{}\".format(y_pred))\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"ngram_range=(2, 4) の正解率： {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】TF-IDFのスクラッチ実装\n",
    ">以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>movie</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>movie</th>\n",
       "      <th>ever</th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.245401</td>\n",
       "      <td>0.227538</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245401</td>\n",
       "      <td>0.227538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283564</td>\n",
       "      <td>0.283564</td>\n",
       "      <td>0.255980</td>\n",
       "      <td>0.283564</td>\n",
       "      <td>0.283564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184051</td>\n",
       "      <td>0.341307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192188</td>\n",
       "      <td>0.341307</td>\n",
       "      <td>0.192188</td>\n",
       "      <td>0.184051</td>\n",
       "      <td>0.341307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       this     movie        is     soooo     funny      what         a  \\\n",
       "0  0.245401  0.227538  0.253437  0.253437  0.253437  0.000000  0.000000   \n",
       "1  0.000000  0.255980  0.000000  0.000000  0.000000  0.283564  0.283564   \n",
       "2  0.184051  0.341307  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      movie         i     never      best     movie      ever      this  \\\n",
       "0  0.227538  0.000000  0.000000  0.000000  0.227538  0.000000  0.245401   \n",
       "1  0.255980  0.283564  0.283564  0.000000  0.255980  0.000000  0.000000   \n",
       "2  0.341307  0.000000  0.000000  0.192188  0.341307  0.192188  0.184051   \n",
       "\n",
       "      movie  \n",
       "0  0.227538  \n",
       "1  0.255980  \n",
       "2  0.341307  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is soooo</th>\n",
       "      <th>soooo funny</th>\n",
       "      <th>what a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie i</th>\n",
       "      <th>i never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.409964</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.409964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.440795</td>\n",
       "      <td>0.409964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this movie  movie is  is soooo  soooo funny    what a   a movie   movie i  \\\n",
       "0    0.409964  0.440795  0.440795     0.440795  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000     0.000000  0.540788  0.540788  0.540788   \n",
       "2    0.409964  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    i never  best movie  movie ever  ever this  this movie  \n",
       "0  0.000000    0.000000    0.000000   0.000000    0.409964  \n",
       "1  0.540788    0.000000    0.000000   0.000000    0.000000  \n",
       "2  0.000000    0.440795    0.440795   0.440795    0.409964  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scratch_TF_IDF(sentence, n):\n",
    "    \"\"\"\n",
    "    Bowのスクラッチ\n",
    "    \n",
    "    Parameters\n",
    "    ----------------\n",
    "    sentence: list（string）\n",
    "      Bowの対象となる文章\n",
    "    n: int\n",
    "      n-gramのn\n",
    "\n",
    "    Returns\n",
    "    ----------------\n",
    "    \n",
    "    \"\"\"\n",
    "    # 正規表現の準備\n",
    "    kigo = string.punctuation# >>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    table = str.maketrans( \"\", \"\", kigo)\n",
    "\n",
    "    # 全部小文字にする\n",
    "    sentence = list(map(str.lower,sentence))\n",
    "    \n",
    "    #文ごとに単語単位に分ける\n",
    "    sentence_words = []\n",
    "    for s in sentence:\n",
    "        result = s.translate(table)#正規表現\n",
    "        sentence_words.append(result.split(' '))\n",
    "        \n",
    "    ############ n-gram ############\n",
    "    sentence_words_n_gram = []\n",
    "    for s in sentence_words:\n",
    "        n_gram_temp = []\n",
    "        for i in range(len(s)-n+1):\n",
    "            n_gram_temp.append(' '.join(s[i:i+n]))\n",
    "        sentence_words_n_gram.append(n_gram_temp)\n",
    "\n",
    "    # 特徴量用に平坦化する\n",
    "    feature_list = sum(sentence_words_n_gram, [])\n",
    "    \n",
    "    # BoWの表の作成\n",
    "    n_appearances = []\n",
    "    tf = []\n",
    "    for s in sentence_words_n_gram:\n",
    "        n_app_sentence = []\n",
    "        n_td = []\n",
    "        for f in feature_list:\n",
    "            n_app_sentence.append(s.count(f))\n",
    "        n_appearances.append(n_app_sentence)\n",
    "        ########## TF の　計算　##########\n",
    "        n_td = np.array(n_app_sentence)\n",
    "        tf.append(n_td/np.sum(n_td))\n",
    "\n",
    "    tf = np.array(tf)\n",
    "    n_appearances= np.array(n_appearances)\n",
    "    \n",
    "    ########## IDF の　計算　##########\n",
    "    idf = np.log((1+len(sentence_words))/(1+np.sum(tf, axis=0)))+1\n",
    "    \n",
    "    ########## TF-IDF の　実装　##########\n",
    "    tf_idf = tf*idf\n",
    "        \n",
    "    df = pd.DataFrame(tf_idf, columns=feature_list)\n",
    "    display(df)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# 分類したい文字列\n",
    "sentence = ['This movie is SOOOO funny!!!',\n",
    "                        'What a movie! I never',\n",
    "                        'best movie ever!!!!! this movie'\n",
    "                       ]\n",
    "\n",
    "n=1\n",
    "scratch_TF_IDF(sentence, n)\n",
    "\n",
    "n=2\n",
    "scratch_TF_IDF(sentence, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>movie</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>movie</th>\n",
       "      <th>ever</th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.265144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.198858</td>\n",
       "      <td>0.397716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198858</td>\n",
       "      <td>0.397716</td>\n",
       "      <td>0.198858</td>\n",
       "      <td>0.198858</td>\n",
       "      <td>0.397716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       this     movie        is     soooo     funny      what         a  \\\n",
       "0  0.265144  0.265144  0.265144  0.265144  0.265144  0.000000  0.000000   \n",
       "1  0.000000  0.298287  0.000000  0.000000  0.000000  0.298287  0.298287   \n",
       "2  0.198858  0.397716  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      movie         i     never      best     movie      ever      this  \\\n",
       "0  0.265144  0.000000  0.000000  0.000000  0.265144  0.000000  0.265144   \n",
       "1  0.298287  0.298287  0.298287  0.000000  0.298287  0.000000  0.000000   \n",
       "2  0.397716  0.000000  0.000000  0.198858  0.397716  0.198858  0.198858   \n",
       "\n",
       "      movie  \n",
       "0  0.265144  \n",
       "1  0.298287  \n",
       "2  0.397716  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is soooo</th>\n",
       "      <th>soooo funny</th>\n",
       "      <th>what a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie i</th>\n",
       "      <th>i never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596574</td>\n",
       "      <td>0.596574</td>\n",
       "      <td>0.596574</td>\n",
       "      <td>0.596574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.477259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this movie  movie is  is soooo  soooo funny    what a   a movie   movie i  \\\n",
       "0    0.477259  0.477259  0.477259     0.477259  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000     0.000000  0.596574  0.596574  0.596574   \n",
       "2    0.477259  0.000000  0.000000     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    i never  best movie  movie ever  ever this  this movie  \n",
       "0  0.000000    0.000000    0.000000   0.000000    0.477259  \n",
       "1  0.596574    0.000000    0.000000   0.000000    0.000000  \n",
       "2  0.000000    0.477259    0.477259   0.477259    0.477259  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scikt-learn\n",
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scratch_TF_IDF(sentence, n):\n",
    "    \"\"\"\n",
    "    Bowのスクラッチ\n",
    "    \n",
    "    Parameters\n",
    "    ----------------\n",
    "    sentence: list（string）\n",
    "      Bowの対象となる文章\n",
    "    n: int\n",
    "      n-gramのn\n",
    "\n",
    "    Returns\n",
    "    ----------------\n",
    "    \n",
    "    \"\"\"\n",
    "    # 正規表現の準備\n",
    "    kigo = string.punctuation# >>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    table = str.maketrans( \"\", \"\", kigo)\n",
    "\n",
    "    # 全部小文字にする\n",
    "    sentence = list(map(str.lower,sentence))\n",
    "    \n",
    "    #文ごとに単語単位に分ける\n",
    "    sentence_words = []\n",
    "    for s in sentence:\n",
    "        result = s.translate(table)#正規表現\n",
    "        sentence_words.append(result.split(' '))\n",
    "        \n",
    "    ############ n-gram ############\n",
    "    sentence_words_n_gram = []\n",
    "    for s in sentence_words:\n",
    "        n_gram_temp = []\n",
    "        for i in range(len(s)-n+1):\n",
    "            n_gram_temp.append(' '.join(s[i:i+n]))\n",
    "        sentence_words_n_gram.append(n_gram_temp)\n",
    "\n",
    "    # 特徴量用に平坦化する\n",
    "    feature_list = sum(sentence_words_n_gram, [])\n",
    "    \n",
    "    # BoWの表の作成\n",
    "    n_appearances = []\n",
    "    tf = []\n",
    "    for s in sentence_words_n_gram:\n",
    "        n_app_sentence = []\n",
    "        n_td = []\n",
    "        for f in feature_list:\n",
    "            n_app_sentence.append(s.count(f))\n",
    "        n_appearances.append(n_app_sentence)\n",
    "        ########## TF の　計算　##########\n",
    "        n_td = np.array(n_app_sentence)\n",
    "        tf.append(n_td/np.sum(n_td))\n",
    "\n",
    "    tf = np.array(tf)\n",
    "    n_appearances= np.array(n_appearances)\n",
    "    \n",
    "    ########## IDF の　計算　##########\n",
    "    # 分母を削除\n",
    "    idf = np.log((1+len(sentence_words)))+1\n",
    "    \n",
    "    ########## TF-IDF の　実装　##########\n",
    "    tf_idf = tf*idf\n",
    "        \n",
    "    df = pd.DataFrame(tf_idf, columns=feature_list)\n",
    "    display(df)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# 分類したい文字列\n",
    "sentence = ['This movie is SOOOO funny!!!',\n",
    "                        'What a movie! I never',\n",
    "                        'best movie ever!!!!! this movie'\n",
    "                       ]\n",
    "\n",
    "n=1\n",
    "scratch_TF_IDF(sentence, n)\n",
    "\n",
    "n=2\n",
    "scratch_TF_IDF(sentence, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
