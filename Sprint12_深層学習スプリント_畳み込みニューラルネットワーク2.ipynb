{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2次元の畳み込みニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "\n",
    "`(n_samples, n_channels, height, width)`の`NCHW`または`(n_samples, height, width, n_channels)`の`NHWC`どちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_test.shape) # (12000, 784)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 軸を追加\n",
    "X_train=X_train[:, np.newaxis, :]\n",
    "X_test=X_test[:, np.newaxis, :]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 軸を追加\n",
    "y_train_one_hot=y_train_one_hot[:, np.newaxis, :]\n",
    "#y_test_one_hot=y_test_one_hot[:, np.newaxis, :]\n",
    "print(y_train_one_hot.shape)\n",
    "#print(y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 1, 28, 28)\n",
      "(12000, 1, 28, 28)\n",
      "(48000, 1, 10)\n",
      "(12000, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n",
      "[2 5 9 ... 1 9 5]\n"
     ]
    }
   ],
   "source": [
    "# テスト用にデコード\n",
    "y_test_decode = np.argmax(y_val, axis=2).reshape(-1,)\n",
    "print(y_test_decode.shape)\n",
    "print(y_test_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】2次元畳み込み層の作成  \n",
    ">1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    ">\n",
    ">フォワードプロパゲーションの数式は以下のようになります。\n",
    ">\n",
    ">$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$\n",
    ">\n",
    ">ai,j,m : 出力される配列のi行j列、mチャンネルの値\n",
    ">\n",
    ">i : 配列の行方向のインデックス\n",
    ">\n",
    ">j : 配列の列方向のインデックス\n",
    ">\n",
    ">m : 出力チャンネルのインデックス\n",
    ">\n",
    ">K : 入力チャンネル数\n",
    ">\n",
    ">Fh,Fw : 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    ">\n",
    ">x(i+s),(j+t),k : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    ">\n",
    ">ws,t,k,m : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    ">\n",
    ">bm : mチャンネルへの出力のバイアス項\n",
    ">\n",
    ">全てスカラーです。\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    ">\n",
    ">$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$\n",
    ">\n",
    ">α : 学習率\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$: $w_{s,t,k,m}$に関する損失 L の勾配\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial b_{m}}$: bm に関する損失 L の勾配\n",
    ">\n",
    ">勾配$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$や$\\frac{\\partial L}{\\partial b_{m}}$を求めるためのバックプロパゲーションの数式が以下である。\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    "$\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi行j列、mチャンネルの値\n",
    ">\n",
    ">$N_{out,h},N_{out,w}$: 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    ">\n",
    ">前の層に流す誤差の数式は以下です。\n",
    ">\n",
    ">$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$\n",
    ">\n",
    ">$\\frac{\\partial L}{\\partial x_{i,j,k}}$: 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    ">\n",
    ">M  : 出力チャンネル数\n",
    ">\n",
    ">ただし、  $i-s<0$または$i-s>N_{out,h}-1$または$j-t<0$または$j-t>N_{out,w}-1$のとき$\\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}} =0$です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter, n_channel, Fs, Ft, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_filter = n_filter\n",
    "        self.n_channel = n_channel\n",
    "        self.Fs = Fs\n",
    "        self.Ft = Ft\n",
    "        \n",
    "        self.W = initializer.W(self.n_filter, self.n_channel, self.Fs, self.Ft)\n",
    "        self.B = initializer.B(self.n_filter)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        self.n_input=self.X.shape[1]#.shape[1]\n",
    "        \n",
    "        self.n_input_h=self.X.shape[2]#.shape[1]\n",
    "        self.n_input_w=self.X.shape[3]#.shape[1]\n",
    "        \n",
    "        #F=3\n",
    "        stride=1\n",
    "        padding=0\n",
    "        #self.n_output1 = self.X_i - self.Fs\n",
    "        #self.n_output2 = self.X_j - self.Ft\n",
    "        self.n_output_h, self.n_output_w = self.output_size(self.n_input_h, self.n_input_w, self.Fs, self.Ft, P=padding, S=stride)\n",
    "        \n",
    "        A = np.empty((self.n_filter, self.n_channel, self.n_output_h, self.n_output_w))\n",
    "        for f in range(self.n_filter):#出力チャンネル\n",
    "            for c in range(self.n_channel):#入力チャンネル\n",
    "                for i in range(self.n_input_h - self.Fs):#self.n_output　　タテ\n",
    "                    for j in range(self.n_input_w - self.Ft):#self.n_output　ヨコ\n",
    "                        A[f, c, i, j] += np.sum(X[f, c, i:i+self.Fs:, j:j+self.Ft]*self.W[f, c])+self.B[f]\n",
    "   \n",
    "        self.A = A\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 実際の処理においてはラベルは不要\n",
    "        \n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, self.X)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def output_size(self, n_input_h, n_input_w, Fs, Ft, P=0, S=1):\n",
    "        \"\"\"\n",
    "        出力数数える\n",
    "        \"\"\"\n",
    "        n_output_h = int((n_input_h + 2*P - Fs)/S + 1)\n",
    "        n_output_w = int((n_input_w + 2*P - Ft)/S + 1)\n",
    "\n",
    "        return (n_output_h, n_output_w)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer():\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_filter, n_channel, Fs, Ft):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_filter : int\n",
    "          フィルタのサイズ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_filter, n_channel, Fs, Ft)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, n_filter):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_filter, )\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenewalFormula():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, X):#dA, Z\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        print(f\"dA.shape：　{dA.shape}\")\n",
    "            \n",
    "        dA_num = layer.n_output_h*layer.n_output_w\n",
    "        dW_temp = np.zeros((layer.n_filter, dA_num, layer.n_channel, layer.Fs, layer.Ft))\n",
    "        print(f\"layer.n_output_h：　{layer.n_output_h}\")\n",
    "        print(f\"layer.n_output_w：　{layer.n_output_w}\")\n",
    "        \n",
    "        for f in range(layer.n_filter):\n",
    "            for c in range(layer.n_channel):\n",
    "                a = 0\n",
    "                for i in range(layer.n_output_h):\n",
    "                    for j in range(layer.n_output_w):\n",
    "                        dW_temp[f, a, c] += layer.W[f, c]*dA[f, c, i, j]\n",
    "                        a += 1\n",
    "\n",
    "        print(f\"dW_temp.shape：　{dW_temp.shape}\")\n",
    "        \n",
    "        \n",
    "        dW = np.sum(dW_temp, axis=1)\n",
    "        print(f\"dW.shape：　{dW.shape}\")\n",
    "        \n",
    "        # (48000, 1, 28, 28)\n",
    "        print(f\"X.shape：　{X.shape}\")\n",
    "        dX=np.zeros(X.shape)\n",
    "        \n",
    "        for f in range(layer.n_filter):\n",
    "            for c in range(layer.n_channel):\n",
    "                a = 0\n",
    "                for i in range(layer.n_output_h):\n",
    "                    for j in range(layer.n_output_w):\n",
    "                        dX[f, c, i:i+layer.Fs, j:j+layer.Ft] += dW[f, c]\n",
    "                        a += 1\n",
    "        \n",
    "        #　バイアスの更新\n",
    "        dB = np.sum(dA, axis=(1,2, 3))#, axis=0 , axis=1\n",
    "        print(f\"dB.shape：　{dB.shape}\")\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        return (layer, dX)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.tanh(X)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA = dZ*(1 - np.tanh(X)**2)\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### インスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: (4, 1, 3, 3)\n",
      "B.shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "initializer=SimpleInitializer(0.01)\n",
    "optimizer=RenewalFormula(1)\n",
    "n_filter=4#写真の枚数と同じ\n",
    "n_channel=1#白黒、RGB\n",
    "Fs=3\n",
    "Ft=3\n",
    "# インスタンス化\n",
    "CNN_conv2d = Conv2d(n_filter, n_channel, Fs, Ft, initializer, optimizer)\n",
    "\n",
    "print(\"W.shape: {}\".format(CNN_conv2d.W.shape))\n",
    "print(\"B.shape: {}\".format(CNN_conv2d.B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "# CNN　順伝播\n",
    "CNN_conv2d.forward(X_train[0:4])\n",
    "A=CNN_conv2d.A\n",
    "print(A.shape)\n",
    "#print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "# 活性化関数　順伝播\n",
    "tanh = Tanh()\n",
    "Z = tanh.forward(A)\n",
    "print(Z.shape)\n",
    "#print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "# 活性化関数　逆伝播\n",
    "dA = tanh.backward(A, Z)#本当はdZ\n",
    "print(dA.shape)\n",
    "#print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA.shape：　(4, 1, 26, 26)\n",
      "layer.n_output_h：　26\n",
      "layer.n_output_w：　26\n",
      "dW_temp.shape：　(4, 676, 1, 3, 3)\n",
      "dW.shape：　(4, 1, 3, 3)\n",
      "X.shape：　(4, 1, 28, 28)\n",
      "dB.shape：　(4,)\n",
      "dX.shape：　(4, 1, 28, 28)\n",
      "dX：　[[[[ 2.10938214e-02  4.57211941e-02  2.13442303e-02 ...  2.13442303e-02\n",
      "     2.50408919e-04 -2.43769638e-02]\n",
      "   [ 4.50580140e-02  1.28203962e-01  8.56484823e-02 ...  8.56484823e-02\n",
      "     4.05904683e-02 -4.25554794e-02]\n",
      "   [ 8.95490109e-02  1.74158397e-01  7.35834498e-02 ...  7.35834498e-02\n",
      "    -1.59655611e-02 -1.00574947e-01]\n",
      "   ...\n",
      "   [ 8.95490109e-02  1.74158397e-01  7.35834498e-02 ...  7.35834498e-02\n",
      "    -1.59655611e-02 -1.00574947e-01]\n",
      "   [ 6.84551895e-02  1.28437203e-01  5.22392195e-02 ...  5.22392195e-02\n",
      "    -1.62159701e-02 -7.61979836e-02]\n",
      "   [ 4.44909970e-02  4.59544354e-02 -1.20650325e-02 ... -1.20650325e-02\n",
      "    -5.65560294e-02 -5.80194679e-02]]]\n",
      "\n",
      "\n",
      " [[[ 2.72084677e-02  4.33542980e-02  6.23693391e-02 ...  6.23693391e-02\n",
      "     3.51608714e-02  1.90150411e-02]\n",
      "   [ 6.91727630e-02  1.34872568e-01  1.82400864e-01 ...  1.82400864e-01\n",
      "     1.13228101e-01  4.75282963e-02]\n",
      "   [ 4.25911421e-02  1.06789211e-01  1.52043860e-01 ...  1.52043860e-01\n",
      "     1.09452718e-01  4.52546490e-02]\n",
      "   ...\n",
      "   [ 4.25911421e-02  1.06789211e-01  1.52043860e-01 ...  1.52043860e-01\n",
      "     1.09452718e-01  4.52546490e-02]\n",
      "   [ 1.53826743e-02  6.34349132e-02  8.96745211e-02 ...  8.96745211e-02\n",
      "     7.42918468e-02  2.62396079e-02]\n",
      "   [-2.65816210e-02 -2.80833567e-02 -3.03570040e-02 ... -3.03570040e-02\n",
      "    -3.77538307e-03 -2.27364729e-03]]]\n",
      "\n",
      "\n",
      " [[[ 1.43815724e-01 -4.92606521e-02  2.30826983e-01 ...  2.30826983e-01\n",
      "     8.70112585e-02  2.80087635e-01]\n",
      "   [ 1.89752090e-01  1.85143639e-01  2.67843991e-01 ...  2.67843991e-01\n",
      "     7.80919002e-02  8.27003514e-02]\n",
      "   [ 7.36692196e-02  6.19237672e-02  5.33630693e-01 ...  5.33630693e-01\n",
      "     4.59961473e-01  4.71706926e-01]\n",
      "   ...\n",
      "   [ 7.36692196e-02  6.19237672e-02  5.33630693e-01 ...  5.33630693e-01\n",
      "     4.59961473e-01  4.71706926e-01]\n",
      "   [-7.01465045e-02  1.11184419e-01  3.02803710e-01 ...  3.02803710e-01\n",
      "     3.72950215e-01  1.91619291e-01]\n",
      "   [-1.16082871e-01 -1.23219872e-01  2.65786702e-01 ...  2.65786702e-01\n",
      "     3.81869573e-01  3.89006574e-01]]]\n",
      "\n",
      "\n",
      " [[[ 5.20057189e-03 -8.31261943e-02 -2.38055949e-02 ... -2.38055949e-02\n",
      "    -2.90061668e-02  5.93205994e-02]\n",
      "   [-9.89776175e-03 -2.09485470e-02  2.52175118e-02 ...  2.52175118e-02\n",
      "     3.51152735e-02  4.61660588e-02]\n",
      "   [-5.92583863e-02 -2.01448218e-02  1.23824927e-01 ...  1.23824927e-01\n",
      "     1.83083313e-01  1.43969749e-01]\n",
      "   ...\n",
      "   [-5.92583863e-02 -2.01448218e-02  1.23824927e-01 ...  1.23824927e-01\n",
      "     1.83083313e-01  1.43969749e-01]\n",
      "   [-6.44589582e-02  6.29813724e-02  1.47630522e-01 ...  1.47630522e-01\n",
      "     2.12089480e-01  8.46491495e-02]\n",
      "   [-4.93606245e-02  8.03725205e-04  9.86074153e-02 ...  9.86074153e-02\n",
      "     1.47968040e-01  9.78036901e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "# CNN　逆伝播\n",
    "dX = CNN_conv2d.backward(dA, X_train[0:4])\n",
    "print(f\"dX.shape：　{dX.shape}\")\n",
    "print(f\"dX：　{dX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】2次元畳み込み後の出力サイズ  \n",
    ">畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラス内にoutput_sizeメソッドを作成。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高さのアウトプット数：　26\n",
      "幅のアウトプット数：　26\n"
     ]
    }
   ],
   "source": [
    "n_input_h = 28\n",
    "n_input_w = 28\n",
    "Fs = 3\n",
    "Ft = 3\n",
    "P=0\n",
    "S=1\n",
    "\n",
    "n_output_h, n_output_w = CNN_conv2d.output_size(n_input_h, \n",
    "                                                                                                n_input_w, \n",
    "                                                                                                Fs, \n",
    "                                                                                                Ft, \n",
    "                                                                                                P=P,\n",
    "                                                                                                S=S\n",
    "                                                                                               )\n",
    "print(f\"高さのアウトプット数：　{n_output_h}\")\n",
    "print(f\"幅のアウトプット数：　{n_output_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最大プーリング層の作成  \n",
    ">最大プーリング層のクラスMaxPool2Dを作成してください。  \n",
    ">プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    ">\n",
    ">$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$\n",
    ">\n",
    ">Pi,j : i行j列への出力する場合の入力配列のインデックスの集合。 \n",
    ">\n",
    ">Sh×Sw の範囲内の行（p）と列（q）\n",
    ">\n",
    ">Sh,Sw : 高さ方向（h）と幅方向（w）のストライドのサイズ\n",
    ">\n",
    ">(p,q)∈Pi,j : Pi,j に含まれる行（p）と列（q）のインデックス\n",
    ">\n",
    ">ai,j,m : 出力される配列のi行j列、kチャンネルの値\n",
    ">\n",
    ">xp,q,k : 入力の配列のp行q列、kチャンネルの値\n",
    ">\n",
    ">ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    ">バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス \n",
    "(p,q) を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filter, n_channel, Sh, Sw):\n",
    "        #self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_filter = n_filter\n",
    "        self.n_channel = n_channel\n",
    "        self.Sh = Sh\n",
    "        self.Sw = Sw\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        print(X.shape)\n",
    "        self.n_input=self.X.shape[1]#.shape[1]\n",
    "        \n",
    "        self.n_input_h=self.X.shape[2]#.shape[1]\n",
    "        self.n_input_w=self.X.shape[3]#.shape[1]\n",
    "        \n",
    "        self.n_output_h=int(self.n_input_h/self.Sh)\n",
    "        self.n_output_w=int(self.n_input_w/self.Sw)\n",
    "        \n",
    "        A = np.empty((self.n_filter, self.n_channel, self.n_output_h, self.n_output_w))\n",
    "        A_index = []\n",
    "        \n",
    "        for f in range(self.n_filter):#出力チャンネル\n",
    "            for c in range(self.n_channel):#入力チャンネル\n",
    "                for i in range(self.n_output_h):#self.n_output　　タテ 【割り切れる数】\n",
    "                    for j in range(self.n_output_w):#self.n_output　ヨコ 【割り切れる数】\n",
    "                        A[f, c, i, j] = np.max(X[f, c, i*self.Sh:i*self.Sh+self.Sh:, j*self.Sw:j*self.Sw+self.Sw])\n",
    "\n",
    "                    \n",
    "        self.A = A\n",
    "        print(f\"self.A.shape：　{self.A.shape}\")\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self, A):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 実際の処理においてはラベルは不要\n",
    "        \n",
    "        # 更新\n",
    "        X_max = np.zeros(self.X.shape)\n",
    "        self.A\n",
    "        a = 0\n",
    "        for f in range(self.n_filter):#出力チャンネル\n",
    "            for c in range(self.n_channel):#入力チャンネル\n",
    "                for i in range(self.n_output_h):#self.n_output　　タテ\n",
    "                    for j in range(self.n_output_w):#self.n_output　ヨコ \n",
    "                        for h in range(self.Sw):#self.Sw　　タテ\n",
    "                            for w in range(self.Sw):#self.Sw　ヨコ\n",
    "                                # マックスプーリングの値と一致するXはそのままXの値を入力\n",
    "                                if self.A[f, c, i, j]==self.X[f, c, i*self.Sh+h, j*self.Sw+w]:\n",
    "                                    X_max[f, c, i, j] = self.X[f, c, i*self.Sh+h, j*self.Sw+w]\n",
    "\n",
    "        self.X_max = X_max\n",
    "        \n",
    "        return self.X_max\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filter=4#写真の枚数と同じ\n",
    "n_channel=1#白黒、RGB\n",
    "Sh=4#【割り切れる数】\n",
    "Sw=4#【割り切れる数】\n",
    "\n",
    "# インスタンス化\n",
    "max_pool_2d = MaxPool2D(n_filter, n_channel, Sh, Sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 28, 28)\n",
      "self.A.shape：　(4, 1, 7, 7)\n",
      "A_max.shape：　(4, 1, 7, 7)\n",
      "A_max：　[[[[0.1741584  0.08564848 0.08564848 0.08564848 0.08564848 0.08564848\n",
      "    0.08564848]\n",
      "   [0.1741584  0.07358345 0.07358345 0.07358345 0.07358345 0.07358345\n",
      "    0.07358345]\n",
      "   [0.1741584  0.07358345 0.07358345 0.07358345 0.07358345 0.07358345\n",
      "    0.07358345]\n",
      "   [0.1741584  0.07358345 0.07358345 0.07358345 0.07358345 0.07358345\n",
      "    0.07358345]\n",
      "   [0.1741584  0.07358345 0.07358345 0.07358345 0.07358345 0.07358345\n",
      "    0.07358345]\n",
      "   [0.1741584  0.07358345 0.07358345 0.07358345 0.07358345 0.07358345\n",
      "    0.07358345]\n",
      "   [0.1741584  0.07358345 0.07358345 0.07358345 0.07358345 0.07358345\n",
      "    0.07358345]]]\n",
      "\n",
      "\n",
      " [[[0.18240086 0.18240086 0.18240086 0.18240086 0.18240086 0.18240086\n",
      "    0.18240086]\n",
      "   [0.15204386 0.15204386 0.15204386 0.15204386 0.15204386 0.15204386\n",
      "    0.15204386]\n",
      "   [0.15204386 0.15204386 0.15204386 0.15204386 0.15204386 0.15204386\n",
      "    0.15204386]\n",
      "   [0.15204386 0.15204386 0.15204386 0.15204386 0.15204386 0.15204386\n",
      "    0.15204386]\n",
      "   [0.15204386 0.15204386 0.15204386 0.15204386 0.15204386 0.15204386\n",
      "    0.15204386]\n",
      "   [0.15204386 0.15204386 0.15204386 0.15204386 0.15204386 0.15204386\n",
      "    0.15204386]\n",
      "   [0.15204386 0.15204386 0.15204386 0.15204386 0.15204386 0.15204386\n",
      "    0.15204386]]]\n",
      "\n",
      "\n",
      " [[[0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]\n",
      "   [0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]\n",
      "   [0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]\n",
      "   [0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]\n",
      "   [0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]\n",
      "   [0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]\n",
      "   [0.53363069 0.53363069 0.53363069 0.53363069 0.53363069 0.53363069\n",
      "    0.53363069]]]\n",
      "\n",
      "\n",
      " [[[0.12382493 0.12382493 0.12382493 0.12382493 0.12382493 0.12382493\n",
      "    0.18308331]\n",
      "   [0.12382493 0.12382493 0.12382493 0.12382493 0.12382493 0.12382493\n",
      "    0.18308331]\n",
      "   [0.12382493 0.12382493 0.12382493 0.12382493 0.12382493 0.12382493\n",
      "    0.18308331]\n",
      "   [0.12382493 0.12382493 0.12382493 0.12382493 0.12382493 0.12382493\n",
      "    0.18308331]\n",
      "   [0.12382493 0.12382493 0.12382493 0.12382493 0.12382493 0.12382493\n",
      "    0.18308331]\n",
      "   [0.12382493 0.12382493 0.12382493 0.12382493 0.12382493 0.12382493\n",
      "    0.18308331]\n",
      "   [0.14763052 0.14763052 0.14763052 0.14763052 0.14763052 0.14763052\n",
      "    0.21208948]]]]\n"
     ]
    }
   ],
   "source": [
    "A_max = max_pool_2d.forward(dX)\n",
    "print(f\"A_max.shape：　{A_max.shape}\")\n",
    "print(f\"A_max：　{A_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_max.shape：　(4, 1, 28, 28)\n",
      "X_max：　[[[[0.1741584  0.08564848 0.08564848 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.1741584  0.07358345 0.07358345 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.1741584  0.07358345 0.07358345 ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.18240086 0.18240086 0.18240086 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.15204386 0.15204386 0.15204386 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.15204386 0.15204386 0.15204386 ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.53363069 0.53363069 0.53363069 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.53363069 0.53363069 0.53363069 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.53363069 0.53363069 0.53363069 ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.12382493 0.12382493 0.12382493 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.12382493 0.12382493 0.12382493 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.12382493 0.12382493 0.12382493 ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "X_max = max_pool_2d.backward(A_max)\n",
    "print(f\"X_max.shape：　{X_max.shape}\")\n",
    "print(f\"X_max：　{X_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】平滑化  \n",
    ">平滑化するためのFlattenクラスを作成してください。\n",
    ">\n",
    ">フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    ">\n",
    ">この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \"\"\"\n",
    "    Flatten平滑化\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        # チャンネル、高さ、幅の3次元を1次元にreshape\n",
    "        self.X = X\n",
    "        X_flat = X.reshape(X.shape[0],-1,1)\n",
    "        \n",
    "        return X_flat \n",
    "        \n",
    "    def backward(self, X):\n",
    "        # バックワードのときに再びreshapeによって形を戻します\n",
    "        X = X.reshape(self.X.shape)\n",
    "        \n",
    "        return X \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_max.shape：　(4, 784, 1)\n"
     ]
    }
   ],
   "source": [
    "flat = Flatten()\n",
    "X_flat = flat.forward(X_max)\n",
    "print(f\"A_max.shape：　{X_flat.shape}\")\n",
    "#print(f\"X_flat：　{X_flat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_max.shape：　(4, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_restore = flat.backward(X_flat)\n",
    "print(f\"A_max.shape：　{X_restore.shape}\")\n",
    "#print(f\"_restore：　{_restore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定  \n",
    ">作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    ">\n",
    ">精度は低くともまずは動くことを目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        A=[]\n",
    "        for f in range(self.X.shape[0]):\n",
    "            A_temp = self.X[f,:].T@self.W[f,:]+self.B.reshape(-1,1).T\n",
    "            A.append(A_temp)\n",
    "        \n",
    "        return np.array(A)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA, Z):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        self, dZ = self.optimizer.update(self, dA, Z)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializerFC:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(4, n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dA, Z):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        dB = np.sum(dA, axis=0)#バッチサイズ分の合計\n",
    "        \n",
    "        dW=[]\n",
    "        for f in range(dA.shape[0]):\n",
    "            dW.append(Z[f,:]*dA[f,:])\n",
    "        dW = np.array(dW)\n",
    "        \n",
    "        \n",
    "        dZ =[]\n",
    "        for f in range(layer.W.shape[0]):\n",
    "            dZ.append(layer.W[f,:]@dA[f,:].T)\n",
    "        dZ=np.array(dZ)\n",
    "        \n",
    "        #【問題3】\n",
    "        layer.W = layer.W - self.lr*(dW)\n",
    "        layer.B = layer.B - self.lr*(dB)\n",
    "        \n",
    "        \n",
    "        return (layer, np.array(dZ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.exp(X)/(np.sum(np.exp(X)))#, axis=()\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA =X - y\n",
    "        \n",
    "        # 目的関数（損失関数）　交差エントロピー誤差\n",
    "        nb = y.shape[0]#バッチサイズ\n",
    "        L = -(1/nb)*(np.sum(y*np.log(X)))\n",
    "        \n",
    "        return (dA, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### インスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC インスタンス化\n",
    "FC_Q6 = FC(36,10, SimpleInitializerFC(0.01), SGD(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape: (4, 1, 3, 3)\n",
      "B.shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Conv2d インスタンス化\n",
    "\n",
    "initializer=SimpleInitializer(0.01)\n",
    "optimizer=RenewalFormula(1)\n",
    "n_filter=4#写真の枚数と同じ\n",
    "n_channel=1#白黒、RGB\n",
    "Fs=3\n",
    "Ft=3\n",
    "# インスタンス化\n",
    "CNN_conv2d_Q6 = Conv2d(n_filter, n_channel, Fs, Ft, initializer, optimizer)\n",
    "\n",
    "print(\"W.shape: {}\".format(CNN_conv2d_Q6.W.shape))\n",
    "print(\"B.shape: {}\".format(CNN_conv2d_Q6.B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool2D　インスタンス化\n",
    "\n",
    "n_filter=4#写真の枚数と同じ\n",
    "n_channel=1#白黒、RGB\n",
    "Sh=4#【割り切れる数】\n",
    "Sw=4#【割り切れる数】\n",
    "\n",
    "# インスタンス化\n",
    "max_pool_2d_Q6 = MaxPool2D(n_filter, n_channel, Sh, Sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス化\n",
    "tanh_Q6 = Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス化\n",
    "softmax_Q6 = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス化\n",
    "flat_Q6 = Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN　順伝播：　(4, 1, 26, 26)\n",
      "活性化関数　順伝播：　(4, 1, 26, 26)\n",
      "(4, 1, 26, 26)\n",
      "self.A.shape：　(4, 1, 6, 6)\n",
      "Max Poolinb　順伝播：　(4, 1, 6, 6)\n",
      "Flatten　順伝播：　(4, 36, 1)\n",
      "全結合層　順伝播：　(4, 1, 10)\n",
      "softmax関数　順伝播：　(4, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# CNN　順伝播\n",
    "A1_q6=CNN_conv2d_Q6.forward(X_train[0:4])#0:4\n",
    "print(f\"CNN　順伝播：　{A1_q6.shape}\")\n",
    "# 活性化関数　順伝播\n",
    "Z1_q6 = tanh_Q6.forward(A1_q6)\n",
    "print(f\"活性化関数　順伝播：　{Z1_q6.shape}\")\n",
    "# Max Poolinb　順伝播\n",
    "X_max_q6 = max_pool_2d_Q6.forward(Z1_q6)\n",
    "print(f\"Max Poolinb　順伝播：　{X_max_q6.shape}\")\n",
    "# Flatten　順伝播\n",
    "X_flat_q6 = flat.forward(X_max_q6)\n",
    "print(f\"Flatten　順伝播：　{X_flat_q6.shape}\")\n",
    "# 全結合層　順伝播\n",
    "A3_q6 = FC_Q6.forward(X_flat_q6)\n",
    "print(f\"全結合層　順伝播：　{A3_q6.shape}\")\n",
    "# 活性化関数　順伝播\n",
    "Z3_q6 = softmax_Q6.forward(A3_q6)\n",
    "print(f\"softmax関数　順伝播：　{Z3_q6.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "活性化関数　逆伝播：　(4, 1, 10)\n",
      "全結合層　逆伝播：　(4, 36, 1)\n",
      "Flatten　逆伝播：　(4, 1, 6, 6)\n",
      "Max Poolinb　逆伝播：　(4, 1, 26, 26)\n",
      "活性化関数　逆伝播：　(4, 1, 26, 26)\n",
      "dA.shape：　(4, 1, 26, 26)\n",
      "layer.n_output_h：　26\n",
      "layer.n_output_w：　26\n",
      "dW_temp.shape：　(4, 676, 1, 3, 3)\n",
      "dW.shape：　(4, 1, 3, 3)\n",
      "X.shape：　(4, 1, 28, 28)\n",
      "dB.shape：　(4,)\n",
      "CNN　逆伝播：　(4, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishiitomoaki/.pyenv/versions/anaconda3-2020.02/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "# 活性化関数　逆伝播\n",
    "dA3_q6, _ = softmax_Q6.backward(A3_q6, y_train[0:4])#  A2,dZ2\n",
    "print(f\"活性化関数　逆伝播：　{dA3_q6.shape}\")\n",
    "# 全結合層　逆伝播\n",
    "dZ2_q6 = FC_Q6.backward(dA3_q6, X_flat_q6)#dA2, Z1\n",
    "print(f\"全結合層　逆伝播：　{dZ2_q6.shape}\")\n",
    "# Flatten　逆伝播\n",
    "X_restore_q6 = flat.backward(dZ2_q6)\n",
    "print(f\"Flatten　逆伝播：　{X_restore_q6.shape}\")\n",
    "# Max Poolinb　逆伝播\n",
    "X_max_q6 = max_pool_2d_Q6.backward(X_restore_q6)\n",
    "print(f\"Max Poolinb　逆伝播：　{X_max_q6.shape}\")\n",
    "# 活性化関数　逆伝播\n",
    "dA_q6 = tanh_Q6.backward(A1_q6, X_max_q6)\n",
    "print(f\"活性化関数　逆伝播：　{dA_q6.shape}\")\n",
    "# CNN　逆伝播\n",
    "dX_q6 = CNN_conv2d_Q6.backward(dA_q6, X_train[0:4])\n",
    "print(f\"CNN　逆伝播：　{dX_q6.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN　順伝播：　(4, 1, 26, 26)\n",
      "活性化関数　順伝播：　(4, 1, 26, 26)\n",
      "(4, 1, 26, 26)\n",
      "self.A.shape：　(4, 1, 6, 6)\n",
      "Max Poolinb　順伝播：　(4, 1, 6, 6)\n",
      "Flatten　順伝播：　(4, 36, 1)\n",
      "全結合層　順伝播：　(4, 1, 10)\n",
      "活性化関数　順伝播：　(4, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# CNN　順伝播\n",
    "A1_pred=CNN_conv2d_Q6.forward(X_val[0:4])#0:4\n",
    "print(f\"CNN　順伝播：　{A1_pred.shape}\")\n",
    "# 活性化関数　順伝播\n",
    "Z1_pred = tanh_Q6.forward(A1_pred)\n",
    "print(f\"活性化関数　順伝播：　{Z1_pred.shape}\")\n",
    "# Max Poolinb　順伝播\n",
    "X_max_pred = max_pool_2d_Q6.forward(Z1_pred)\n",
    "print(f\"Max Poolinb　順伝播：　{X_max_pred.shape}\")\n",
    "# Flatten　順伝播\n",
    "X_flat_pred = flat.forward(X_max_pred)\n",
    "print(f\"Flatten　順伝播：　{X_flat_pred.shape}\")\n",
    "# 全結合層　順伝播\n",
    "A3_pred = FC_Q6.forward(X_flat_pred)\n",
    "print(f\"全結合層　順伝播：　{A3_pred.shape}\")\n",
    "# 活性化関数　順伝播\n",
    "Z3_pred = tanh_Q6.forward(A3_pred)\n",
    "print(f\"活性化関数　順伝播：　{Z3_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00066119, -0.06155702, -0.03385686, -0.01386086,\n",
       "         -0.02155221,  0.04196057, -0.01887374,  0.05493433,\n",
       "          0.00739246,  0.02929838]],\n",
       "\n",
       "       [[-0.04229882,  0.06375575,  0.0877241 ,  0.12828851,\n",
       "         -0.02843156, -0.05266303,  0.05252657,  0.00870839,\n",
       "         -0.01946171,  0.04936045]],\n",
       "\n",
       "       [[ 0.02806418, -0.07584328, -0.12057347,  0.00175743,\n",
       "         -0.00555056, -0.04010421, -0.04662781, -0.038095  ,\n",
       "         -0.09551056,  0.04698329]],\n",
       "\n",
       "       [[ 0.09028674,  0.02103523, -0.08092963,  0.00969304,\n",
       "         -0.04235279,  0.02050761, -0.1265246 ,  0.10695408,\n",
       "          0.03906774, -0.03141681]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推定結果： [7 3 9 7]\n",
      "正解： [2 5 9 9]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(Z3_pred, axis=2).reshape(-1,)\n",
    "print(\"推定結果： {}\".format(y_pred))\n",
    "print(\"正解： {}\".format(y_test_decode[0:4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_decode[0:4], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】出力サイズとパラメータ数の計算  \n",
    ">CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    ">\n",
    ">また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    ">\n",
    ">以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    ">\n",
    ">1  \n",
    ">\n",
    ">- 入力サイズ : 144×144, 3チャンネル\n",
    ">- フィルタサイズ : 3×3, 6チャンネル\n",
    ">- ストライド : 1\n",
    ">- パディング : なし\n",
    ">\n",
    ">2 \n",
    ">\n",
    ">- 入力サイズ : 60×60, 24チャンネル\n",
    ">- フィルタサイズ : 3×3, 48チャンネル\n",
    ">- ストライド　: 1\n",
    ">- パディング : なし\n",
    ">\n",
    ">3\n",
    ">\n",
    ">- 入力サイズ : 20×20, 10チャンネル\n",
    ">- フィルタサイズ: 3×3, 20チャンネル\n",
    ">- ストライド : 2\n",
    ">- パディング : なし\n",
    ">\n",
    ">＊最後の例は丁度良く畳み込みをすることができない場合です。  \n",
    ">フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。  \n",
    ">端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  \n",
    "- 出力サイズ : 142×6\n",
    "- パラメータ数 : 124416\n",
    "- バイアス項 : 6\n",
    "\n",
    "2  \n",
    "- 出力サイズ : 37×48\n",
    "- パラメータ数 : 172800\n",
    "- バイアス項 : 48\n",
    "\n",
    "3  \n",
    "- 出力サイズ : 9×20\n",
    "- パラメータ数 : 8000\n",
    "- バイアス項 : 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
