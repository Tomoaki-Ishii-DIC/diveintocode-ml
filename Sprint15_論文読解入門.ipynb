{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下の論文を読み問題に答えてください。  \n",
    ">CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    ">\n",
    ">[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    ">\n",
    ">https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題\n",
    ">それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    ">\n",
    ">(1) 物体検出の分野にはどういった手法が存在したか。\n",
    ">\n",
    ">(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    ">\n",
    ">(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    ">\n",
    ">(4) RPNとは何か。\n",
    ">\n",
    ">(5) RoIプーリングとは何か。\n",
    ">\n",
    ">(6) Anchorのサイズはどうするのが適切か。\n",
    ">\n",
    ">(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    ">\n",
    ">(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    ">\n",
    ">\n",
    "### 条件\n",
    "- 答える際は論文のどの部分からそれが分かるかを書く。  \n",
    "- 必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "- 論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks \n",
    "Faster R-CNN: Region Proposal Networksによるリアルタイムオブジェクト検出に向けて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">【お勧めの入りかた】  \n",
    ">まずは概要（Abstract）と結論（CONCLUSION）をざっと読む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract  \n",
    "—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.\n",
    "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
    "proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
    "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
    "generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN\n",
    "into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with\n",
    "“attention” mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3],\n",
    "our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection\n",
    "accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO\n",
    "2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been\n",
    "made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概要\n",
    "-最先端のオブジェクト検出ネットワークは、オブジェクトの場所を仮定するために 領域提案（region proposal）アルゴリズムに依存しています。  \n",
    "SPPnet [1]やFastR-CNN [2]のような進歩により、これらの検出ネットワークの実行時間が短縮され、領域提案の計算がボトルネックになっています。  \n",
    "この論文では、フルイメージを共有する領域提案（region proposal）ネットワーク（RPN）を紹介します。  \n",
    "検出ネットワークを備えた畳み込み機能により、ほぼ計算コストのかからない領域提案が可能になります。   \n",
    "RPNは各位置でオブジェクトの境界とオブジェクト性スコアを同時に予測する完全な畳み込みネットワークです。  \n",
    "RPNは、検出のためにFastR-CNNによって使用される高品質の領域提案を生成するようにエンドツーエンドでトレーニングされます。  \n",
    "さらに、RPNとFast R-CNNの畳み込み機能を共有することで、RPNとFast R-CNNを単一のネットワークに統合します。  \n",
    "最近人気のある「attention」メカニズムを備えたニューラルネットワークの用語を使用して、RPNコンポーネントは統合ネットワークにどこを見ればよいかを指示します。  \n",
    "非常に深いVGG-16モデル[3]の場合、検出システムはGPUで5fps（すべてのステップを含む）のフレームレートを持ち、PASCAL VOC 2007、2012、および画像あたり300件の領域候補を含むMSCOCOデータセットで最先端のオブジェクト検出精度を実現します。   \n",
    "ILSVRCとCOCO2015のコンテストでは、Faster R-CNNとRPNが、いくつかのトラックで1位を獲得したエントリーの基盤となっています。コードは公開されています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 CONCLUSION  \n",
    "We have presented RPNs for efficient and accurate region proposal generation. By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free. Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates. The learned RPN also improves region proposal quality and thus the overall object detection accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結論  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "効率的かつ正確な領域候補生成のためのRPNを紹介しました。  \n",
    "畳み込み機能をダウンストリーム検出ネットワークと共有することにより、領域提案はほぼコストがかかりません。  \n",
    "私たちの方法は、統合された深層学習ベースのオブジェクト検出システムをほぼリアルタイムのフレームレートで実行できるようにします。  \n",
    "学習されたRPNは、領域提案の品質を向上させ、したがって全体的なオブジェクト検出の精度も向上させます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">【お勧めの入りかた】  \n",
    ">図と表と数式を眺めてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1  \n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.30.51.png)\n",
    "Figure 1: Different schemes for addressing multiple scales and sizes.  \n",
    "(a) Pyramids of images and feature maps are built, and the classifier is run at all scales.   \n",
    "(b) Pyramids of filters with multiple scales/sizes are run on the feature map.   \n",
    "(c) We use pyramids of reference boxes in the regression functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図1：複数のスケールとサイズに対処するためのさまざまなスキーム。  \n",
    "（a）画像と特徴マップのピラミッドが構築され、分類器がすべてのスケールで実行されます。  \n",
    "（b）複数のスケール/サイズのフィルターのピラミッドがフィーチャマップ上で実行されます。  \n",
    "（c）回帰関数で参照ボックスのピラミッドを使用します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2  \n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.37.21.png)\n",
    "Figure 2: Faster R-CNN is a single, unified network\n",
    "for object detection. The RPN module serves as the\n",
    "‘attention’ of this unified network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2：Faster R-CNNは、オブジェクト検出のための単一の統合ネットワークです。 RPNモジュールは、この統合ネットワークの「attention」として機能します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3  \n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.40.17.png)\n",
    "Figure 3: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals on PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3：  \n",
    "左：領域提案ネットワーク（RPN）。   \n",
    "右：PASCAL VOC2007テストでRPN提案を使用した検出例。 私たちの方法は、さまざまなスケールとアスペクト比でオブジェクトを検出します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4  \n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.48.34.png)\n",
    "Figure 4: Recall vs. IoU overlap ratio on the PASCAL VOC 2007 test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図4：PASCAL VOC2007テストセットでのリコールとIoUのオーバーラップ率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 5  \n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.51.36.png)\n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.51.54.png)\n",
    "![caption](./picture/スクリーンショット_2020-12-10_13.52.12.png)\n",
    "Figure 5: Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster R-CNN system. The model is VGG-16 and the training data is 07+12 trainval (73.2% mAP on the 2007 test set).   \n",
    "Our method detects objects of a wide range of scales and aspect ratios. Each output box is associated with a category label and a softmax score in [0, 1]. A score threshold of 0.6 is used to display these images.  \n",
    "The running time for obtaining these results is 198ms per image, including all steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5：FasterR-CNNシステムを使用したPASCALVOC2007テストセットでのオブジェクト検出結果の選択例。   \n",
    "モデルはVGG-16であり、トレーニングデータは07 + 12 trainval（2007テストセットで73.2％mAP）です。  \n",
    "私たちの方法は、さまざまなスケールとアスペクト比のオブジェクトを検出します。 各出力ボックスは、[0、1]のカテゴリラベルとソフトマックススコアに関連付けられています。   \n",
    "これらの画像を表示するには、スコアのしきい値0.6が使用されます。  \n",
    "これらの結果を取得するための実行時間は、すべてのステップを含めて、画像あたり198msです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 6  \n",
    "![caption](./picture/スクリーンショット_2020-12-11_11.54.45.png)\n",
    "Figure 6: Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN system. \n",
    "The model is VGG-16 and the training data is COCO trainval (42.7% mAP@0.5 on the test-dev set).\n",
    "Each output box is associated with a category label and a softmax score in [0, 1]. A score threshold of 0.6 is used to display these images. \n",
    "For each image, one color represents one object category in that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図6：FasterR-CNNシステムを使用したMSCOCOtest-devセットでのオブジェクト検出結果の選択された例。\n",
    "モデルはVGG-16であり、トレーニングデータはCOCO trainval（test-devセットで42.7％mAP@0.5）です。\n",
    "各出力ボックスは、[0、1]のカテゴリラベルとソフトマックススコアに関連付けられています。 これらの画像を表示するには、スコアのしきい値0.6が使用されます。\n",
    "画像ごとに、1つの色がその画像の1つのオブジェクトカテゴリを表します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.25.29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表1：ZFネットを使用した各アンカーの学習された平均領域候補のサイズ（sの数値= 600）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 2\n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.25.47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表2：PASCAL VOC 2007テストセット（VOC 2007 trainvalでトレーニング済み）での検出結果。   \n",
    "検出器はZFを使用したFastR-CNNですが、トレーニングとテストにさまざまな提案方法を使用しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 3 - 5   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.26.56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表3：PASCAL VOC2007テストセットでの検出結果。 検出器はFastR-CNNとVGG-16です。 トレーニングデータ：「07」：VOC 2007トレインバル、「07 + 12」：VOC2007トレインバルとVOC2012トレインバルの和集合。RPNの場合、FastR-CNNの列車時間の提案は2000です。\n",
    "†：この数は[2]で報告されました。 このホワイトペーパーで提供されているリポジトリを使用すると、この結果は高くなります（68.1）。\n",
    "\n",
    "表4：PASCAL VOC2012テストセットでの検出結果。 検出器はFastR-CNNとVGG-16です。 トレーニングデータ：「07」：VOC 2007 trainval、「07 ++ 12」：VOC 2007 trainval + testとVOC2012trainvalの和集合。 RPNの場合、FastR-CNNの列車時間の提案は2000です。\n",
    "\n",
    "表5：SSプロポーザルがCPUで評価される場合を除き、K40 GPUでのタイミング（ミリ秒）。 「領域ごと」には、NMS、プーリング、完全接続、およびソフトマックス層が含まれます。 実行時間のプロファイリングについては、リリースされたコードを参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 6 - 7   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.27.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表6：高速R-CNN検出器とVGG-16を使用したPASCAL VOC2007テストセットの結果。 RPNの場合、Fast R-CNNのトレーニング時間の提案は2000です。RPN∗は、非共有機能のバージョンを示します。\n",
    "\n",
    "表7：高速R-CNN検出器とVGG-16を使用したPASCAL VOC2012テストセットの結果。 RPNの場合、FastR-CNNの列車時間の提案は2000です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 8 - 9   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.27.23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表8：さまざまなアンカー設定を使用したPASCAL VOC2007テストセットでのFasterR-CNNの検出結果。 ネットワークはVGG-16です。 トレーニングデータはVOC2007trainvalです。 3つのスケールと3つのアスペクト比（69.9％）を使用するデフォルト設定は、表3の設定と同じです。\n",
    "\n",
    "表9：式（1）のさまざまなλの値を使用したPASCAL VOC2007テストセットでのFasterR-CNNの検出結果。 ネットワークはVGG-16です。 トレーニングデータはVOC2007trainvalです。 λ= 10（69.9％）を使用するデフォルト設定は、表3の設定と同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 10   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.27.35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表10：1段階の検出と2段階の提案+検出。 検出結果は、ZFモデルとFastR-CNNを使用したPASCALVOC2007テストセットにあります。 RPNは非共有機能を使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 11   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.27.45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表11：MS COCOデータセットでのオブジェクト検出結果（％）。 モデルはVGG-16です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 12   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.27.54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表12：より高速なR-CNNの検出mAP（％）\n",
    "異なるトレーニングデータを使用したPASCALVOC2007テストセットと2012テストセット。 モデルはVGG-16です。\n",
    "「COCO」は、COCOtrainvalセットがトレーニングに使用されることを示します。 表6および表7も参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数式 1   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_16.16.54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数式 2   \n",
    "![caption](./picture/スクリーンショット_2020-12-10_16.17.37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SPPnet\n",
    "- Fast R-CNN\n",
    "\n",
    "（論文 p.1 「Abstract」に記載あり。）  \n",
    "Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "従来のモデルでは画像のピラミッドまたはフィルターのピラミッドを使用する方法が用いられているが、Faster-R-CNNにおいては複数のスケールとアスペクト比での参照として新しい「アンカー」ボックスを導入したことで、回帰参照のピラミッドと考えることができ、複数のスケールまたはアスペクト比の画像またはフィルターを列挙することを回避した。\n",
    "\n",
    "（論文 p.１〜２ 「Introduction」に記載あり。）  \n",
    "In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel “anchor” boxes\n",
    "that serve as references at multiple scales and aspect ratios. \n",
    "Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Stageの手法はOverFeatという1段階のクラス固有の検出パイプラインであるが、Two-Stageのパイプラインは、クラスにとらわれない領域提案とクラス固有の検出で構成される2段階のカスケードである点が異なる。  \n",
    "なお、「OverFeat」は高速で正確な2つのモデルを持つ特徴抽出器ことで、Pierre Sermanetらの論文\"OverFeat:Integrated Recognition, Localization and Detection using Convolutional Networks\"にて紹介されている。\n",
    "\n",
    "\n",
    "\n",
    "OverFeatis a one-stage, class-specific detection pipeline, and ours　is a two-stage cascade consisting of class-agnostic proposals and class-specific detections.　  \n",
    "(論文 p.１0 「4 EXPERIMENTS 4.1 Experiments on PASCAL VOC」に記載あり。）\n",
    "\n",
    "追加情報  \n",
    "OverFeatでは、領域ごとの機能は、スケールピラミッド上の1つのアスペクト比のスライディングウィンドウから取得されます。これらの機能は、オブジェクトの場所とカテゴリを同時に決定するために使用されます。 RPNでは、機能は正方形（3×3）のスライディングウィンドウと、さまざまなスケールとアスペクト比のアンカーに関連する提案を予測します。どちらの方法もスライディングウィンドウを使用しますが、領域提案タスクはFaster RCNNの最初の段階にすぎません。つまり、ダウンストリームのFastR-CNN検出器が提案に対応してそれらを改良します。の第2段階で私たちのカスケードでは、地域ごとの特徴は、地域の特徴をより忠実にカバーする提案ボックスから適応的にプールされます[1]、[2]。これらの機能がより正確な検出につながると信じています。\n",
    "\n",
    "\n",
    "#### 参考論文  \n",
    "Pierre Sermanet David Eigen Xiang Zhang Michael Mathieu Rob Fergus Yann LeCun  \n",
    "\"OverFeat:Integrated Recognition, Localization and Detection using Convolutional Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN（region proposal network）とは画像の各位置でオブジェクトの境界とオブジェクト性スコアを同時に予測する領域検出ネットワークのことで、検出ネットワークを備えた畳み込み機能により、ほぼ計算コストのかからない領域提案が可能。\n",
    "入力として（任意のサイズの）画像を取得し、それぞれがオブジェクトネススコアを持つ長方形のオブジェクト提案のセットを出力します。このプロセスを完全畳み込みネットワークでモデル化します。\n",
    "\n",
    "In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional　network that simultaneously predicts object bounds and objectness scores at each position. 　　\n",
    "\n",
    "(論文 p.１ 「Abstract」に記載あり。）\n",
    "\n",
    "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.\n",
    "\n",
    "(論文 p.3 「3.1 Region Proposal Networks」に記載あり。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) RoIプーリングとは何か。\n",
    "RoIプーリングは、maxプーリングを使用して有効な関心領域内のフィーチャをH×W（7×7など）の固定空間範囲を持つ小さなフィーチャマップに変換するレイヤー。RoI プーリングは、h×wRoIウィンドウをおおよそのサイズh / H×w / WのサブウィンドウのH×Wグリッドに分割し、各サブウィンドウの値を対応する出力グリッドセルにmax-poolingすることで機能する。 \n",
    "\n",
    "(R. Girshick, “Fast R-CNN,” p.2「2.1. The RoI pooling layer」に記載あり。）\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本論文中に直接的な説明がないため、参考文献に挙げているR. Girshickの“Fast R-CNN,”を参照。\n",
    "\n",
    "The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features\n",
    "and also the predicted bounding boxes as input, so\n",
    "a theoretically valid backpropagation solver should\n",
    "also involve gradients w.r.t. the box coordinates. \n",
    "\n",
    "(論文 p.６ 「3 FASTER R-CNN」に下記論文を参照して記載した内容あり。）  \n",
    "[2]　R. Girshick, “Fast R-CNN,” in IEEE International Conference on\n",
    "Computer Vision (ICCV), 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参照論文を読む\n",
    "\n",
    "Abstract  \n",
    "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. \n",
    "Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. \n",
    "\n",
    "この論文は、物体検出のための高速領域ベースの畳み込みネットワーク法（高速R-CNN）を提案します。 Fast R-CNNは、以前の作業に基づいて、深い畳み込みネットワークを使用してオブジェクト提案を効率的に分類します。 以前の作業と比較して、Fast R-CNNは、検出精度を向上させながら、トレーニングとテストの速度を向上させるためにいくつかの革新を採用しています。 高速R-CNNは、非常に深いVGG16ネットワークをR-CNNより9倍高速にトレーニングし、テスト時に213倍高速で、PASCAL VOC 2012でより高いmAPを実現します。SPPnetと比較して、高速R-CNNはVGG163倍高速にトレーニングします。 テストが10倍速く、より正確です。\n",
    "\n",
    "#### RoI pooling に関する記載\n",
    "\n",
    "2.1. The RoI pooling layer\n",
    "The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI.\n",
    "\n",
    "RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. \n",
    "Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. \n",
    "We use the pooling sub-window calculation given in [11].\n",
    "\n",
    "2.1 RoIプーリングレイヤー  \n",
    "RoIプーリングレイヤーは、最大プーリングを使用して、有効な関心領域内のフィーチャを、H×W（7×7など）の固定空間範囲を持つ小さなフィーチャマップに変換します。\n",
    "\n",
    "RoI maxプーリングは、h×wRoIウィンドウをおおよそのサイズh / H×w / WのサブウィンドウのH×Wグリッドに分割し、各サブウィンドウの値を対応する出力グリッドセルにmax-poolingすることで機能します。 \n",
    "プーリングは、標準の最大プーリングと同様に、各フィーチャマップチャネルに個別に適用されます。 RoIレイヤーは、SPPnet [11]で使用される空間ピラミッドプーリングレイヤーの特殊なケースであり、ピラミッドレベルは1つだけです。\n",
    "[11]で与えられたプーリングサブウィンドウ計算を使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Anchorのサイズはどうするのが適切か。\n",
    "デフォルトでは、3つのスケールと3つのアスペクト比を使用し、各スライド位置でk = 9のアンカーを生成します。 サイズW×H（通常は約2,400）の畳み込み特徴マップの場合、合計でWHkアンカーがあります。\n",
    "\n",
    "複数のスケールとアスペクト比のアンカーボックスを参照して、境界ボックスを分類および回帰します。 単一の縮尺の画像とフィーチャマップのみに依存し、単一のサイズのフィルター（フィーチャマップ上のスライドウィンドウ）を使用します。 複数のスケールとサイズに対処するためのこのスキームの効果を実験によって示します（表8）。\n",
    "![caption](./picture/スクリーンショット_2020-12-11_11.33.09.png)\n",
    "\n",
    "By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W Hk anchors in total.\n",
    "\n",
    "As a comparison, our anchor-based method is built on a pyramid of anchors, which is more cost-efficient.\n",
    "Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios. It only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size. We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table 8).\n",
    "\n",
    "(論文 p.4 「3 FASTER R-CNN」に記載あり。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS COCOデータセットを用いてテストを行い、Fast R-CNNのmAP@0.5が39.3％（mAP@[.5, .95]も同等）であったのに対し、Faster R-CNNのmAP@0.5は42.1%、mAP@[.5, .95]は21.5%で、Fast RCNNよりもmAP @ 0.5で2.8％、mAP@[.5, .95]で2.2％高くなった。（表11）。\n",
    "\n",
    "![caption](./picture/スクリーンショット_2020-12-10_15.27.45.png)\n",
    "\n",
    "In Table 11 we first report the results of the Fast　R-CNN system [2] using the　implementation in this　paper. Our Fast R-CNN baseline has 39.3% mAP@0.5　on the test-dev set, higher than that reported in [2].　\n",
    "We conjecture that the reason for this gap is mainly　due to the definition of the negative samples and also　the changes of the mini-batch sizes. We also note that\n",
    "the mAP@[.5, .95] is just comparable.  \n",
    "Next we evaluate our Faster R-CNN system. Using　the COCO training set to train, Faster R-CNN has　42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the　COCO test-dev set. This is 2.8% higher for mAP@0.5　and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11).\n",
    "This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on　the COCO test-dev set. Figure 6 shows some results　on the MS COCO test-dev set.\n",
    "\n",
    "(論文 p.4 「4.2 Experiments on MS COCO」に記載あり。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
